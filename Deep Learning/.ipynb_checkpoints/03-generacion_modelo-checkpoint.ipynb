{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda y construcción del modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv',sep=';', decimal='.')\n",
    "val = pd.read_csv('./val.csv',sep=';', decimal='.')\n",
    "test = pd.read_csv('./test.csv',sep=';', decimal='.')\n",
    "\n",
    "images = np.load('images.npy')\n",
    "train_imgs = images[train['Unnamed: 0']]\n",
    "val_imgs = images[val['Unnamed: 0']]\n",
    "test_imgs = images[test['Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraigo la variable objetivo y dropeo los índices para empezar a trabajar con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train.Price\n",
    "Y_val = val.Price\n",
    "Y_test = test.Price\n",
    "\n",
    "X_train = train.drop(columns=['Unnamed: 0','Price'])\n",
    "X_val = val.drop(columns=['Unnamed: 0','Price'])\n",
    "X_test = test.drop(columns=['Unnamed: 0','Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, concatenate\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función me permite simplificar la creación de un modelo denso. Se puede especificar el parámetro linear=True para que devuelva una arquitectura completa con una salida lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(dim, linear=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, activation='relu'))\n",
    "    while dim > 18:\n",
    "        dim //= 2\n",
    "        model.add(Dense(dim, activation='relu'))\n",
    "    \n",
    "    if linear:\n",
    "        model.add(Dense(3, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echo un primer vistazo al modelo con los hiperparámetros que mejor suelen funcionar a nivel general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/200\n",
      "6634/6634 [==============================] - 0s 28us/step - loss: 6420.9283 - mse: 6420.9287 - val_loss: 206409.2500 - val_mse: 206409.2500\n",
      "Epoch 2/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 5395.5341 - mse: 5395.5337 - val_loss: 781480.7500 - val_mse: 781480.7500\n",
      "Epoch 3/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 4433.7544 - mse: 4433.7539 - val_loss: 800081.6250 - val_mse: 800081.6250\n",
      "Epoch 4/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 3529.2403 - mse: 3529.2405 - val_loss: 335655.1562 - val_mse: 335655.1562\n",
      "Epoch 5/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 3034.0165 - mse: 3034.0164 - val_loss: 130231.8672 - val_mse: 130231.8672\n",
      "Epoch 6/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2852.5383 - mse: 2852.5381 - val_loss: 29764.6152 - val_mse: 29764.6152\n",
      "Epoch 7/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2766.9045 - mse: 2766.9048 - val_loss: 10381.7598 - val_mse: 10381.7598\n",
      "Epoch 8/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2706.2495 - mse: 2706.2495 - val_loss: 47639.8711 - val_mse: 47639.8711\n",
      "Epoch 9/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2725.4083 - mse: 2725.4084 - val_loss: 6021.6245 - val_mse: 6021.6245\n",
      "Epoch 10/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2604.2394 - mse: 2604.2393 - val_loss: 6587.7100 - val_mse: 6587.7100\n",
      "Epoch 11/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2561.6275 - mse: 2561.6274 - val_loss: 7652.1523 - val_mse: 7652.1523\n",
      "Epoch 12/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2514.6301 - mse: 2514.6299 - val_loss: 7560.9038 - val_mse: 7560.9038\n",
      "Epoch 13/200\n",
      "6634/6634 [==============================] - ETA: 0s - loss: 2418.5864 - mse: 2418.586 - 0s 6us/step - loss: 2477.8946 - mse: 2477.8945 - val_loss: 7109.2173 - val_mse: 7109.2173\n",
      "Epoch 14/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2437.3024 - mse: 2437.3025 - val_loss: 6286.9829 - val_mse: 6286.9829\n",
      "Epoch 15/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2392.8444 - mse: 2392.8445 - val_loss: 5735.7954 - val_mse: 5735.7954\n",
      "Epoch 16/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2353.6157 - mse: 2353.6157 - val_loss: 5273.1592 - val_mse: 5273.1592\n",
      "Epoch 17/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2302.8131 - mse: 2302.8130 - val_loss: 5049.4199 - val_mse: 5049.4199\n",
      "Epoch 18/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2253.3540 - mse: 2253.3540 - val_loss: 4696.0586 - val_mse: 4696.0586\n",
      "Epoch 19/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2194.8736 - mse: 2194.8738 - val_loss: 4443.6333 - val_mse: 4443.6333\n",
      "Epoch 20/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2146.5352 - mse: 2146.5352 - val_loss: 4123.4341 - val_mse: 4123.4341\n",
      "Epoch 21/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2095.5267 - mse: 2095.5266 - val_loss: 3832.9292 - val_mse: 3832.9292\n",
      "Epoch 22/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2013.1841 - mse: 2013.1841 - val_loss: 3600.8892 - val_mse: 3600.8892\n",
      "Epoch 23/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1957.0676 - mse: 1957.0675 - val_loss: 3508.2354 - val_mse: 3508.2354\n",
      "Epoch 24/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1925.7796 - mse: 1925.7797 - val_loss: 3316.7778 - val_mse: 3316.7778\n",
      "Epoch 25/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1861.0861 - mse: 1861.0861 - val_loss: 3211.0054 - val_mse: 3211.0054\n",
      "Epoch 26/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1823.8973 - mse: 1823.8972 - val_loss: 3069.9871 - val_mse: 3069.9871\n",
      "Epoch 27/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1773.2348 - mse: 1773.2349 - val_loss: 2947.0754 - val_mse: 2947.0754\n",
      "Epoch 28/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1755.1651 - mse: 1755.1650 - val_loss: 3023.4441 - val_mse: 3023.4441\n",
      "Epoch 29/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1713.3690 - mse: 1713.3689 - val_loss: 2786.6311 - val_mse: 2786.6311\n",
      "Epoch 30/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1677.9222 - mse: 1677.9222 - val_loss: 2678.4475 - val_mse: 2678.4475\n",
      "Epoch 31/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1639.3533 - mse: 1639.3533 - val_loss: 2664.4861 - val_mse: 2664.4861\n",
      "Epoch 32/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1613.6988 - mse: 1613.6987 - val_loss: 2595.6838 - val_mse: 2595.6838\n",
      "Epoch 33/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1589.1371 - mse: 1589.1370 - val_loss: 2570.6956 - val_mse: 2570.6956\n",
      "Epoch 34/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1582.0348 - mse: 1582.0348 - val_loss: 2862.1006 - val_mse: 2862.1006\n",
      "Epoch 35/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1558.4803 - mse: 1558.4803 - val_loss: 2386.7751 - val_mse: 2386.7751\n",
      "Epoch 36/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1543.4412 - mse: 1543.4412 - val_loss: 2436.6455 - val_mse: 2436.6455\n",
      "Epoch 37/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1531.2883 - mse: 1531.2883 - val_loss: 2441.9097 - val_mse: 2441.9097\n",
      "Epoch 38/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1502.8184 - mse: 1502.8185 - val_loss: 2281.3582 - val_mse: 2281.3582\n",
      "Epoch 39/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1478.3329 - mse: 1478.3329 - val_loss: 2357.3362 - val_mse: 2357.3362\n",
      "Epoch 40/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1455.6272 - mse: 1455.6272 - val_loss: 2207.8472 - val_mse: 2207.8472\n",
      "Epoch 41/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1467.0102 - mse: 1467.0101 - val_loss: 2733.5305 - val_mse: 2733.5305\n",
      "Epoch 42/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1486.1088 - mse: 1486.1089 - val_loss: 2063.7446 - val_mse: 2063.7446\n",
      "Epoch 43/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1442.4596 - mse: 1442.4595 - val_loss: 2111.8831 - val_mse: 2111.8831\n",
      "Epoch 44/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1415.0513 - mse: 1415.0514 - val_loss: 2258.5266 - val_mse: 2258.5266\n",
      "Epoch 45/200\n",
      "6634/6634 [==============================] - ETA: 0s - loss: 1151.9198 - mse: 1151.919 - 0s 6us/step - loss: 1363.2041 - mse: 1363.2041 - val_loss: 1993.0425 - val_mse: 1993.0425\n",
      "Epoch 46/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1353.6745 - mse: 1353.6746 - val_loss: 1859.7456 - val_mse: 1859.7456\n",
      "Epoch 47/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1341.9316 - mse: 1341.9315 - val_loss: 2207.9138 - val_mse: 2207.9138\n",
      "Epoch 48/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1409.7671 - mse: 1409.7671 - val_loss: 2072.2834 - val_mse: 2072.2834\n",
      "Epoch 49/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1341.4925 - mse: 1341.4924 - val_loss: 1887.0712 - val_mse: 1887.0712\n",
      "Epoch 50/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1311.6371 - mse: 1311.6370 - val_loss: 1778.7072 - val_mse: 1778.7072\n",
      "Epoch 51/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1298.7504 - mse: 1298.7504 - val_loss: 1739.2402 - val_mse: 1739.2402\n",
      "Epoch 52/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1288.5887 - mse: 1288.5886 - val_loss: 1718.3903 - val_mse: 1718.3903\n",
      "Epoch 53/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1283.0239 - mse: 1283.0238 - val_loss: 1740.7552 - val_mse: 1740.7552\n",
      "Epoch 54/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1273.7520 - mse: 1273.7522 - val_loss: 1706.1406 - val_mse: 1706.1406\n",
      "Epoch 55/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1260.8936 - mse: 1260.8936 - val_loss: 1606.9558 - val_mse: 1606.9558\n",
      "Epoch 56/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1290.1510 - mse: 1290.1510 - val_loss: 2175.0544 - val_mse: 2175.0544\n",
      "Epoch 57/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1261.0400 - mse: 1261.0399 - val_loss: 1595.0551 - val_mse: 1595.0551\n",
      "Epoch 58/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1254.1968 - mse: 1254.1969 - val_loss: 1616.1677 - val_mse: 1616.1677\n",
      "Epoch 59/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1264.2325 - mse: 1264.2324 - val_loss: 1604.7009 - val_mse: 1604.7009\n",
      "Epoch 60/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1275.2190 - mse: 1275.2189 - val_loss: 1532.9550 - val_mse: 1532.9550\n",
      "Epoch 61/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1234.9360 - mse: 1234.9359 - val_loss: 1549.3994 - val_mse: 1549.3994\n",
      "Epoch 62/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1250.9509 - mse: 1250.9508 - val_loss: 1570.1053 - val_mse: 1570.1053\n",
      "Epoch 63/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1226.5916 - mse: 1226.5916 - val_loss: 1525.5211 - val_mse: 1525.5211\n",
      "Epoch 64/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1217.5024 - mse: 1217.5024 - val_loss: 1506.5088 - val_mse: 1506.5088\n",
      "Epoch 65/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1207.6024 - mse: 1207.6024 - val_loss: 1482.1937 - val_mse: 1482.1937\n",
      "Epoch 66/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1219.4973 - mse: 1219.4973 - val_loss: 1501.2440 - val_mse: 1501.2440\n",
      "Epoch 67/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1218.8108 - mse: 1218.8108 - val_loss: 1835.9335 - val_mse: 1835.9335\n",
      "Epoch 68/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1202.6520 - mse: 1202.6520 - val_loss: 1516.6722 - val_mse: 1516.6722\n",
      "Epoch 69/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1221.2988 - mse: 1221.2987 - val_loss: 1547.1097 - val_mse: 1547.1097\n",
      "Epoch 70/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1216.5422 - mse: 1216.5421 - val_loss: 1487.3514 - val_mse: 1487.3514\n",
      "Epoch 71/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1190.2186 - mse: 1190.2186 - val_loss: 1522.1538 - val_mse: 1522.1538\n",
      "Epoch 72/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1172.8912 - mse: 1172.8912 - val_loss: 1568.4590 - val_mse: 1568.4590\n",
      "Epoch 73/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1174.1630 - mse: 1174.1630 - val_loss: 2140.5420 - val_mse: 2140.5420\n",
      "Epoch 74/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1215.4685 - mse: 1215.4686 - val_loss: 1546.1140 - val_mse: 1546.1140\n",
      "Epoch 75/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1221.4464 - mse: 1221.4463 - val_loss: 1511.9479 - val_mse: 1511.9479\n",
      "Epoch 76/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1168.4188 - mse: 1168.4188 - val_loss: 1471.4653 - val_mse: 1471.4653\n",
      "Epoch 77/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1152.8796 - mse: 1152.8795 - val_loss: 1533.3757 - val_mse: 1533.3757\n",
      "Epoch 78/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1140.7881 - mse: 1140.7881 - val_loss: 1506.2551 - val_mse: 1506.2551\n",
      "Epoch 79/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1185.3613 - mse: 1185.3613 - val_loss: 1476.5889 - val_mse: 1476.5889\n",
      "Epoch 80/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1147.8739 - mse: 1147.8739 - val_loss: 1485.7208 - val_mse: 1485.7208\n",
      "Epoch 81/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1156.7284 - mse: 1156.7283 - val_loss: 1677.3148 - val_mse: 1677.3148\n",
      "Epoch 82/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1171.6909 - mse: 1171.6910 - val_loss: 1591.3915 - val_mse: 1591.3915\n",
      "Epoch 83/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1151.0556 - mse: 1151.0557 - val_loss: 1572.8110 - val_mse: 1572.8110\n",
      "Epoch 84/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1144.0066 - mse: 1144.0067 - val_loss: 1530.3994 - val_mse: 1530.3994\n",
      "Epoch 85/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1155.0646 - mse: 1155.0647 - val_loss: 1867.1272 - val_mse: 1867.1272\n",
      "Epoch 86/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1152.9474 - mse: 1152.9474 - val_loss: 1584.1862 - val_mse: 1584.1862\n",
      "Epoch 87/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1132.0809 - mse: 1132.0809 - val_loss: 1651.8811 - val_mse: 1651.8811\n",
      "Epoch 88/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1106.8062 - mse: 1106.8062 - val_loss: 1590.7960 - val_mse: 1590.7960\n",
      "Epoch 89/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1092.4291 - mse: 1092.4291 - val_loss: 1575.0569 - val_mse: 1575.0569\n",
      "Epoch 90/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1096.5467 - mse: 1096.5466 - val_loss: 1616.5425 - val_mse: 1616.5425\n",
      "Epoch 91/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1098.0268 - mse: 1098.0269 - val_loss: 1475.2772 - val_mse: 1475.2772\n",
      "Epoch 92/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1094.0238 - mse: 1094.0237 - val_loss: 1429.9365 - val_mse: 1429.9365\n",
      "Epoch 93/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1084.0416 - mse: 1084.0416 - val_loss: 1440.2583 - val_mse: 1440.2583\n",
      "Epoch 94/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1088.5562 - mse: 1088.5563 - val_loss: 1717.9265 - val_mse: 1717.9265\n",
      "Epoch 95/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1115.4444 - mse: 1115.4443 - val_loss: 1578.8046 - val_mse: 1578.8046\n",
      "Epoch 96/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1081.2076 - mse: 1081.2075 - val_loss: 2226.3857 - val_mse: 2226.3857\n",
      "Epoch 97/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1091.9257 - mse: 1091.9257 - val_loss: 1543.2606 - val_mse: 1543.2606\n",
      "Epoch 98/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1093.9373 - mse: 1093.9373 - val_loss: 1609.5078 - val_mse: 1609.5078\n",
      "Epoch 99/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1139.4552 - mse: 1139.4552 - val_loss: 8288.9150 - val_mse: 8288.9150\n",
      "Epoch 100/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1172.3409 - mse: 1172.3409 - val_loss: 1522.6973 - val_mse: 1522.6973\n",
      "Epoch 101/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1121.2925 - mse: 1121.2925 - val_loss: 1529.5554 - val_mse: 1529.5554\n",
      "Epoch 102/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1102.9959 - mse: 1102.9958 - val_loss: 1461.2678 - val_mse: 1461.2678\n",
      "Epoch 103/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1076.4719 - mse: 1076.4719 - val_loss: 1455.3638 - val_mse: 1455.3638\n",
      "Epoch 104/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1077.4043 - mse: 1077.4042 - val_loss: 1484.4441 - val_mse: 1484.4441\n",
      "Epoch 105/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1086.4328 - mse: 1086.4329 - val_loss: 1465.9900 - val_mse: 1465.9900\n",
      "Epoch 106/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1059.4542 - mse: 1059.4542 - val_loss: 1564.4631 - val_mse: 1564.4631\n",
      "Epoch 107/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1093.2747 - mse: 1093.2747 - val_loss: 1453.8403 - val_mse: 1453.8403\n",
      "Epoch 108/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1053.7533 - mse: 1053.7533 - val_loss: 1432.5464 - val_mse: 1432.5464\n",
      "Epoch 109/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1033.4498 - mse: 1033.4498 - val_loss: 1467.9047 - val_mse: 1467.9047\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 0s 6us/step - loss: 1041.2217 - mse: 1041.2217 - val_loss: 1419.1627 - val_mse: 1419.1627\n",
      "Epoch 111/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1052.7743 - mse: 1052.7743 - val_loss: 1453.8441 - val_mse: 1453.8441\n",
      "Epoch 112/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1030.7021 - mse: 1030.7020 - val_loss: 1464.2418 - val_mse: 1464.2418\n",
      "Epoch 113/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1030.7375 - mse: 1030.7375 - val_loss: 1420.2151 - val_mse: 1420.2151\n",
      "Epoch 114/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1016.6117 - mse: 1016.6118 - val_loss: 1442.1294 - val_mse: 1442.1294\n",
      "Epoch 115/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1028.3671 - mse: 1028.3671 - val_loss: 1433.6019 - val_mse: 1433.6019\n",
      "Epoch 116/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1032.7574 - mse: 1032.7573 - val_loss: 1719.0503 - val_mse: 1719.0503\n",
      "Epoch 117/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 998.5076 - mse: 998.5076 - val_loss: 1516.2177 - val_mse: 1516.2177\n",
      "Epoch 118/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 998.1141 - mse: 998.1141 - val_loss: 1432.2365 - val_mse: 1432.2365\n",
      "Epoch 119/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1005.3326 - mse: 1005.3326 - val_loss: 1784.8336 - val_mse: 1784.8336\n",
      "Epoch 120/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 991.9725 - mse: 991.9725 - val_loss: 1436.9155 - val_mse: 1436.9155\n",
      "Epoch 121/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 997.7181 - mse: 997.7181 - val_loss: 1505.0979 - val_mse: 1505.0979\n",
      "Epoch 122/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1032.3752 - mse: 1032.3752 - val_loss: 1442.3765 - val_mse: 1442.3765\n",
      "Epoch 123/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1005.2141 - mse: 1005.2141 - val_loss: 1488.7003 - val_mse: 1488.7003\n",
      "Epoch 124/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 993.1395 - mse: 993.1394 - val_loss: 1637.4585 - val_mse: 1637.4585\n",
      "Epoch 125/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 974.5554 - mse: 974.5554 - val_loss: 1467.2382 - val_mse: 1467.2382\n",
      "Epoch 126/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1001.7214 - mse: 1001.7214 - val_loss: 1474.8163 - val_mse: 1474.8163\n",
      "Epoch 127/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1015.3464 - mse: 1015.3463 - val_loss: 1421.6705 - val_mse: 1421.6705\n",
      "Epoch 128/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 973.1326 - mse: 973.1326 - val_loss: 1459.4072 - val_mse: 1459.4072\n",
      "Epoch 129/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1001.4622 - mse: 1001.4622 - val_loss: 1546.6962 - val_mse: 1546.6962\n",
      "Epoch 130/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1013.0594 - mse: 1013.0596 - val_loss: 1722.5397 - val_mse: 1722.5397\n",
      "Epoch 131/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1041.3660 - mse: 1041.3660 - val_loss: 1715.4042 - val_mse: 1715.4042\n",
      "Epoch 132/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 955.3640 - mse: 955.3640 - val_loss: 1835.0336 - val_mse: 1835.0336\n",
      "Epoch 133/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 954.8939 - mse: 954.8939 - val_loss: 1466.2041 - val_mse: 1466.2041\n",
      "Epoch 134/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 945.3476 - mse: 945.3477 - val_loss: 1556.3535 - val_mse: 1556.3535\n",
      "Epoch 135/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 995.7855 - mse: 995.7854 - val_loss: 1438.5602 - val_mse: 1438.5602\n",
      "Epoch 136/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 941.6847 - mse: 941.6846 - val_loss: 1516.0673 - val_mse: 1516.0673\n",
      "Epoch 137/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 938.7729 - mse: 938.7729 - val_loss: 1485.2662 - val_mse: 1485.2662\n",
      "Epoch 138/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 928.8679 - mse: 928.8680 - val_loss: 1509.1779 - val_mse: 1509.1779\n",
      "Epoch 139/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 980.6316 - mse: 980.6315 - val_loss: 2343.1606 - val_mse: 2343.1606\n",
      "Epoch 140/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1040.0516 - mse: 1040.0515 - val_loss: 1713.9597 - val_mse: 1713.9597\n",
      "Epoch 141/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1076.2546 - mse: 1076.2546 - val_loss: 1472.5009 - val_mse: 1472.5009\n",
      "Epoch 142/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 993.2812 - mse: 993.2812 - val_loss: 1492.7676 - val_mse: 1492.7676\n",
      "Epoch 143/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 960.7604 - mse: 960.7603 - val_loss: 1434.0967 - val_mse: 1434.0967\n",
      "Epoch 144/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 917.7116 - mse: 917.7115 - val_loss: 1423.1174 - val_mse: 1423.1174\n",
      "Epoch 145/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 915.5352 - mse: 915.5353 - val_loss: 1432.7609 - val_mse: 1432.7609\n",
      "Epoch 146/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 955.2494 - mse: 955.2494 - val_loss: 1430.1555 - val_mse: 1430.1555\n",
      "Epoch 147/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 942.6789 - mse: 942.6790 - val_loss: 1476.6599 - val_mse: 1476.6599\n",
      "Epoch 148/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 924.0966 - mse: 924.0966 - val_loss: 2743.7212 - val_mse: 2743.7212\n",
      "Epoch 149/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 994.2547 - mse: 994.2548 - val_loss: 1795.3456 - val_mse: 1795.3456\n",
      "Epoch 150/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 932.5028 - mse: 932.5029 - val_loss: 1413.3130 - val_mse: 1413.3130\n",
      "Epoch 151/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 900.4213 - mse: 900.4213 - val_loss: 1416.0369 - val_mse: 1416.0369\n",
      "Epoch 152/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 887.3541 - mse: 887.3542 - val_loss: 1482.7692 - val_mse: 1482.7692\n",
      "Epoch 153/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 889.2729 - mse: 889.2728 - val_loss: 1549.5145 - val_mse: 1549.5145\n",
      "Epoch 154/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 904.8298 - mse: 904.8298 - val_loss: 1833.8230 - val_mse: 1833.8230\n",
      "Epoch 155/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 914.4243 - mse: 914.4243 - val_loss: 1617.6692 - val_mse: 1617.6692\n",
      "Epoch 156/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 907.1115 - mse: 907.1115 - val_loss: 1500.6237 - val_mse: 1500.6237\n",
      "Epoch 157/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 889.7125 - mse: 889.7125 - val_loss: 1541.7002 - val_mse: 1541.7002\n",
      "Epoch 158/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 871.0718 - mse: 871.0718 - val_loss: 1476.4020 - val_mse: 1476.4020\n",
      "Epoch 159/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 873.9878 - mse: 873.9878 - val_loss: 1440.3690 - val_mse: 1440.3690\n",
      "Epoch 160/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 868.3068 - mse: 868.3068 - val_loss: 1403.2075 - val_mse: 1403.2075\n",
      "Epoch 161/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 882.2691 - mse: 882.2690 - val_loss: 1676.0171 - val_mse: 1676.0171\n",
      "Epoch 162/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 896.8722 - mse: 896.8722 - val_loss: 1415.7153 - val_mse: 1415.7153\n",
      "Epoch 163/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 856.7140 - mse: 856.7141 - val_loss: 1413.3011 - val_mse: 1413.3011\n",
      "Epoch 164/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 866.1547 - mse: 866.1547 - val_loss: 1421.5380 - val_mse: 1421.5380\n",
      "Epoch 165/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 864.1892 - mse: 864.1893 - val_loss: 1509.2351 - val_mse: 1509.2351\n",
      "Epoch 166/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 866.7086 - mse: 866.7086 - val_loss: 1415.0930 - val_mse: 1415.0930\n",
      "Epoch 167/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 916.1546 - mse: 916.1545 - val_loss: 1581.2679 - val_mse: 1581.2679\n",
      "Epoch 168/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 892.0425 - mse: 892.0426 - val_loss: 1429.7711 - val_mse: 1429.7711\n",
      "Epoch 169/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 888.8347 - mse: 888.8347 - val_loss: 1484.6201 - val_mse: 1484.6201\n",
      "Epoch 170/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 912.9484 - mse: 912.9484 - val_loss: 2945.4390 - val_mse: 2945.4390\n",
      "Epoch 171/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 902.2430 - mse: 902.2430 - val_loss: 1443.3035 - val_mse: 1443.3035\n",
      "Epoch 172/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 865.6267 - mse: 865.6267 - val_loss: 1488.3942 - val_mse: 1488.3942\n",
      "Epoch 173/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 880.9586 - mse: 880.9587 - val_loss: 1636.6797 - val_mse: 1636.6797\n",
      "Epoch 174/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 854.7307 - mse: 854.7305 - val_loss: 1589.6289 - val_mse: 1589.6289\n",
      "Epoch 175/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 843.5624 - mse: 843.5624 - val_loss: 1446.3956 - val_mse: 1446.3956\n",
      "Epoch 176/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 861.7080 - mse: 861.7080 - val_loss: 1522.4658 - val_mse: 1522.4658\n",
      "Epoch 177/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 839.2878 - mse: 839.2878 - val_loss: 1431.4789 - val_mse: 1431.4789\n",
      "Epoch 178/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 830.3167 - mse: 830.3167 - val_loss: 1449.4519 - val_mse: 1449.4519\n",
      "Epoch 179/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 818.4688 - mse: 818.4688 - val_loss: 1424.3588 - val_mse: 1424.3588\n",
      "Epoch 180/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 829.2169 - mse: 829.2170 - val_loss: 1473.7728 - val_mse: 1473.7728\n",
      "Epoch 181/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 816.1126 - mse: 816.1126 - val_loss: 1592.1193 - val_mse: 1592.1193\n",
      "Epoch 182/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 829.7338 - mse: 829.7338 - val_loss: 1530.3496 - val_mse: 1530.3496\n",
      "Epoch 183/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 855.6129 - mse: 855.6130 - val_loss: 1412.5618 - val_mse: 1412.5618\n",
      "Epoch 184/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 843.5584 - mse: 843.5584 - val_loss: 1386.2689 - val_mse: 1386.2689\n",
      "Epoch 185/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 845.6636 - mse: 845.6636 - val_loss: 1438.6039 - val_mse: 1438.6039\n",
      "Epoch 186/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 913.8415 - mse: 913.8414 - val_loss: 6548.3896 - val_mse: 6548.3896\n",
      "Epoch 187/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 895.8422 - mse: 895.8422 - val_loss: 1388.3080 - val_mse: 1388.3080\n",
      "Epoch 188/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 835.8366 - mse: 835.8366 - val_loss: 1455.7014 - val_mse: 1455.7014\n",
      "Epoch 189/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 832.8098 - mse: 832.8098 - val_loss: 1396.0764 - val_mse: 1396.0764\n",
      "Epoch 190/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 840.8411 - mse: 840.8411 - val_loss: 1576.0853 - val_mse: 1576.0853\n",
      "Epoch 191/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 803.6989 - mse: 803.6989 - val_loss: 1453.8295 - val_mse: 1453.8295\n",
      "Epoch 192/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 805.8490 - mse: 805.8491 - val_loss: 1389.4656 - val_mse: 1389.4656\n",
      "Epoch 193/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 804.7659 - mse: 804.7659 - val_loss: 1417.6953 - val_mse: 1417.6953\n",
      "Epoch 194/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 808.2180 - mse: 808.2180 - val_loss: 1458.4468 - val_mse: 1458.4468\n",
      "Epoch 195/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 847.7775 - mse: 847.7775 - val_loss: 1994.1184 - val_mse: 1994.1184\n",
      "Epoch 196/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 806.7397 - mse: 806.7397 - val_loss: 1562.9767 - val_mse: 1562.9767\n",
      "Epoch 197/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 827.1740 - mse: 827.1740 - val_loss: 1396.8812 - val_mse: 1396.8812\n",
      "Epoch 198/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 873.0433 - mse: 873.0433 - val_loss: 1531.6769 - val_mse: 1531.6769\n",
      "Epoch 199/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 916.7678 - mse: 916.7678 - val_loss: 1561.5446 - val_mse: 1561.5446\n",
      "Epoch 200/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 833.0961 - mse: 833.0961 - val_loss: 1415.5189 - val_mse: 1415.5189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d944141a48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atts_model = create_dense(X_train.shape[1], linear=True)\n",
    "\n",
    "atts_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "atts_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=200, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1183.4383611515207\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((atts_model.predict(X_test) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es excesivamente malo para ser un modelo tan simple. Ahora voy a definir un modelo para trabajar con las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función simplifica la creación de una arquitectura CNN. Su utilidad es parecida a la anterior, pero ésta me da la opción de utilizar una arquitectura base y hacer transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(shape, base=None, linear=False, density=144, mode='trainable', dropout=None):\n",
    "    \n",
    "    if base:\n",
    "        \n",
    "        if mode == 'transfer_learning':\n",
    "            for layer in base.layers: \n",
    "                layer.trainable = False\n",
    "                \n",
    "        if mode == 'keep_first':\n",
    "            base.layers[1].trainable = False\n",
    "        \n",
    "        model = base.layers[-1].output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(density, activation='relu')(model)\n",
    "        \n",
    "        if linear:\n",
    "            model = Dense(9, activation='relu')(model)\n",
    "            model = Dense(3, activation='relu')(model)\n",
    "            model = Dense(1, activation='linear')(model)\n",
    "            \n",
    "        model = Model(base.input, model)\n",
    "    \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(18, kernel_size=(3, 3), input_shape=shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "        model.add(Conv2D(36, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(density, activation='relu'))\n",
    "    \n",
    "        if linear:\n",
    "            model.add(Dense(9, activation='relu'))\n",
    "            model.add(Dense(3, activation='relu'))\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescalo las imágenes a la mitad de su resolución original por cuestiones de memoria RAM de mi GPU y después estandarizo los datos para que el modelo trabaje con números entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],shape[1]//2,shape[2]//2,shape[3]))\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se comporta esta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/5\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 3373.5612 - mse: 3373.5618 - val_loss: 5006.2853 - val_mse: 5006.2856\n",
      "Epoch 2/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 3028.9497 - mse: 3028.9500 - val_loss: 4488.7453 - val_mse: 4488.7451\n",
      "Epoch 3/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 2996.3681 - mse: 2996.3684 - val_loss: 3798.2024 - val_mse: 3798.2021\n",
      "Epoch 4/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 2968.4253 - mse: 2968.4253 - val_loss: 3668.8245 - val_mse: 3668.8245\n",
      "Epoch 5/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 2945.8788 - mse: 2945.8789 - val_loss: 3577.7160 - val_mse: 3577.7166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d983c2a7c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_model = create_cnn(train_imgs_res.shape[1:], linear=True, dropout=0.2, density=512)\n",
    "\n",
    "imgs_model.compile(loss='mse', optimizer='adam' , metrics=['mse'])\n",
    "\n",
    "imgs_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3116.577766626536\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((imgs_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora voy a probar con VGG16 cargando los pesos de imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],48,48,3)) # Reescalo a 48, 48, 3 para poder usar VGG16\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/5\n",
      "6634/6634 [==============================] - 7s 1ms/step - loss: 4287.6449 - mse: 4287.6450 - val_loss: 3664.3528 - val_mse: 3664.3530\n",
      "Epoch 2/5\n",
      "6634/6634 [==============================] - 5s 792us/step - loss: 3035.5118 - mse: 3035.5120 - val_loss: 3636.4545 - val_mse: 3636.4546\n",
      "Epoch 3/5\n",
      "6634/6634 [==============================] - 5s 792us/step - loss: 3012.9005 - mse: 3012.8999 - val_loss: 3634.9739 - val_mse: 3634.9739\n",
      "Epoch 4/5\n",
      "6634/6634 [==============================] - 5s 794us/step - loss: 3008.0693 - mse: 3008.0691 - val_loss: 3615.7102 - val_mse: 3615.7100\n",
      "Epoch 5/5\n",
      "6634/6634 [==============================] - 5s 795us/step - loss: 2992.9559 - mse: 2992.9556 - val_loss: 3636.4518 - val_mse: 3636.4517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d9810116c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "\n",
    "vgg16_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=True, density=1024, mode='transfer_learning')\n",
    "\n",
    "vgg16_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "vgg16_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3092.6069059289584\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((vgg16_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí he definido dos funciones que permiten generar mis modelos de una forma sencilla. Ahora toca unificarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero voy a pasar la variable objetivo a categórica para trabajar con las categorías \"barato\", \"asequible\" y \"caro\", ya que seguramente serán más fáciles de predecir que un precio aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_train_cat = []\n",
    "for y in Y_train:\n",
    "    if y < 35: Y_train_cat.append(0)\n",
    "    elif y < 85: Y_train_cat.append(1)\n",
    "    else: Y_train_cat.append(2)\n",
    "        \n",
    "plt.hist(Y_train_cat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_cat = []\n",
    "for y in Y_val:\n",
    "    if y < 35: Y_val_cat.append(0)\n",
    "    elif y < 85: Y_val_cat.append(1)\n",
    "    else: Y_val_cat.append(2)\n",
    "        \n",
    "Y_test_cat = []\n",
    "for y in Y_test:\n",
    "    if y < 35: Y_test_cat.append(0)\n",
    "    elif y < 85: Y_test_cat.append(1)\n",
    "    else: Y_test_cat.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un buen reparto de los valores en categorías, veamos como se comportan nuestros modelos si los unificamos. Comienzo creando mis dos modelos (atributos e imágenes) con las funciones que definí al comienzo del notebook, pero esta vez le indicaré en los parámetros que no quiero un final lineal. Así podré combinarlas y agregar las capas finales que crea convenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(he dejado comentados algunos de los modelos que he estado probando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.3015 - accuracy: 0.4806 - val_loss: 1.1531 - val_accuracy: 0.5447\n",
      "Epoch 2/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.0077 - accuracy: 0.5232 - val_loss: 2.4443 - val_accuracy: 0.5569\n",
      "Epoch 3/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.9512 - accuracy: 0.5505 - val_loss: 1.0223 - val_accuracy: 0.5786\n",
      "Epoch 4/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.9013 - accuracy: 0.5787 - val_loss: 1.4915 - val_accuracy: 0.5786\n",
      "Epoch 5/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.9147 - accuracy: 0.5725 - val_loss: 3.1949 - val_accuracy: 0.5515\n",
      "Epoch 6/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.8720 - accuracy: 0.5927 - val_loss: 2.5732 - val_accuracy: 0.5949\n",
      "Epoch 7/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.8181 - accuracy: 0.6099 - val_loss: 2.1859 - val_accuracy: 0.5583\n",
      "Epoch 8/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7956 - accuracy: 0.6244 - val_loss: 1.8073 - val_accuracy: 0.5827\n",
      "Epoch 9/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7623 - accuracy: 0.6427 - val_loss: 2.5484 - val_accuracy: 0.6179\n",
      "Epoch 10/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7526 - accuracy: 0.6516 - val_loss: 1.0805 - val_accuracy: 0.6382\n",
      "Epoch 11/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7156 - accuracy: 0.6737 - val_loss: 1.1131 - val_accuracy: 0.6721\n",
      "Epoch 12/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7074 - accuracy: 0.6777 - val_loss: 1.4695 - val_accuracy: 0.6314\n",
      "Epoch 13/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6807 - accuracy: 0.6854 - val_loss: 1.0885 - val_accuracy: 0.6653\n",
      "Epoch 14/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6914 - accuracy: 0.6845 - val_loss: 1.0788 - val_accuracy: 0.6301\n",
      "Epoch 15/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6575 - accuracy: 0.7076 - val_loss: 2.8772 - val_accuracy: 0.6301\n",
      "Epoch 16/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6625 - accuracy: 0.7074 - val_loss: 1.0486 - val_accuracy: 0.6680\n",
      "Epoch 17/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6453 - accuracy: 0.7134 - val_loss: 1.2334 - val_accuracy: 0.6301\n",
      "Epoch 18/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6873 - accuracy: 0.6895 - val_loss: 3.0857 - val_accuracy: 0.6450\n",
      "Epoch 19/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6651 - accuracy: 0.7122 - val_loss: 1.8321 - val_accuracy: 0.6789\n",
      "Epoch 20/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6281 - accuracy: 0.7207 - val_loss: 1.8525 - val_accuracy: 0.6748\n",
      "Epoch 21/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6347 - accuracy: 0.7109 - val_loss: 1.1680 - val_accuracy: 0.6612\n",
      "Epoch 22/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6248 - accuracy: 0.7190 - val_loss: 0.6904 - val_accuracy: 0.6680\n",
      "Epoch 23/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6205 - accuracy: 0.7193 - val_loss: 0.6584 - val_accuracy: 0.6789\n",
      "Epoch 24/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6064 - accuracy: 0.7302 - val_loss: 1.3217 - val_accuracy: 0.6789\n",
      "Epoch 25/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5857 - accuracy: 0.7410 - val_loss: 0.6991 - val_accuracy: 0.7005\n",
      "Epoch 26/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5932 - accuracy: 0.7413 - val_loss: 2.6148 - val_accuracy: 0.6924\n",
      "Epoch 27/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5800 - accuracy: 0.7451 - val_loss: 2.5943 - val_accuracy: 0.6883\n",
      "Epoch 28/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5840 - accuracy: 0.7419 - val_loss: 2.3456 - val_accuracy: 0.6951\n",
      "Epoch 29/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5744 - accuracy: 0.7412 - val_loss: 1.8986 - val_accuracy: 0.6978\n",
      "Epoch 30/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5661 - accuracy: 0.7499 - val_loss: 2.2054 - val_accuracy: 0.6856\n",
      "Epoch 31/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5597 - accuracy: 0.7511 - val_loss: 1.3603 - val_accuracy: 0.6883\n",
      "Epoch 32/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5845 - accuracy: 0.7386 - val_loss: 1.9857 - val_accuracy: 0.6653\n",
      "Epoch 33/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5665 - accuracy: 0.7454 - val_loss: 1.1776 - val_accuracy: 0.7005\n",
      "Epoch 34/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5623 - accuracy: 0.7445 - val_loss: 1.6239 - val_accuracy: 0.7033\n",
      "Epoch 35/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5621 - accuracy: 0.7543 - val_loss: 4.8265 - val_accuracy: 0.6992\n",
      "Epoch 36/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5698 - accuracy: 0.7538 - val_loss: 2.4913 - val_accuracy: 0.6762\n",
      "Epoch 37/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5855 - accuracy: 0.7557 - val_loss: 3.2827 - val_accuracy: 0.6355\n",
      "Epoch 38/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5482 - accuracy: 0.7614 - val_loss: 2.4687 - val_accuracy: 0.6734\n",
      "Epoch 39/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5397 - accuracy: 0.7645 - val_loss: 1.9743 - val_accuracy: 0.7019\n",
      "Epoch 40/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5205 - accuracy: 0.7765 - val_loss: 1.8258 - val_accuracy: 0.7005\n",
      "Epoch 41/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5201 - accuracy: 0.7759 - val_loss: 1.7684 - val_accuracy: 0.6951\n",
      "Epoch 42/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5190 - accuracy: 0.7754 - val_loss: 1.4509 - val_accuracy: 0.7005\n",
      "Epoch 43/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5230 - accuracy: 0.7683 - val_loss: 1.4439 - val_accuracy: 0.7046\n",
      "Epoch 44/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5122 - accuracy: 0.7751 - val_loss: 3.7787 - val_accuracy: 0.6165\n",
      "Epoch 45/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5994 - accuracy: 0.7394 - val_loss: 2.2471 - val_accuracy: 0.7127\n",
      "Epoch 46/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5342 - accuracy: 0.7661 - val_loss: 1.7497 - val_accuracy: 0.7060\n",
      "Epoch 47/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5064 - accuracy: 0.7856 - val_loss: 1.3520 - val_accuracy: 0.7114\n",
      "Epoch 48/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5448 - accuracy: 0.7688 - val_loss: 4.0551 - val_accuracy: 0.6965\n",
      "Epoch 49/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5024 - accuracy: 0.7801 - val_loss: 1.7309 - val_accuracy: 0.7060\n",
      "Epoch 50/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5054 - accuracy: 0.7743 - val_loss: 1.9306 - val_accuracy: 0.6924\n",
      "Epoch 51/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5067 - accuracy: 0.7765 - val_loss: 1.5664 - val_accuracy: 0.7182\n",
      "Epoch 52/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4828 - accuracy: 0.7852 - val_loss: 1.3393 - val_accuracy: 0.7114\n",
      "Epoch 53/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5063 - accuracy: 0.7810 - val_loss: 1.2911 - val_accuracy: 0.7127\n",
      "Epoch 54/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5106 - accuracy: 0.7781 - val_loss: 1.4002 - val_accuracy: 0.6396\n",
      "Epoch 55/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4871 - accuracy: 0.7823 - val_loss: 1.1548 - val_accuracy: 0.6924\n",
      "Epoch 56/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5020 - accuracy: 0.7769 - val_loss: 1.0355 - val_accuracy: 0.7100\n",
      "Epoch 57/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4873 - accuracy: 0.7820 - val_loss: 1.3016 - val_accuracy: 0.7073\n",
      "Epoch 58/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4851 - accuracy: 0.7849 - val_loss: 7.2013 - val_accuracy: 0.7154\n",
      "Epoch 59/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4798 - accuracy: 0.7894 - val_loss: 2.0479 - val_accuracy: 0.7100\n",
      "Epoch 60/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4780 - accuracy: 0.7968 - val_loss: 2.1962 - val_accuracy: 0.7304\n",
      "Epoch 61/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4848 - accuracy: 0.7918 - val_loss: 1.9649 - val_accuracy: 0.7005\n",
      "Epoch 62/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4649 - accuracy: 0.7974 - val_loss: 2.0153 - val_accuracy: 0.7127\n",
      "Epoch 63/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4619 - accuracy: 0.7994 - val_loss: 1.8788 - val_accuracy: 0.7019\n",
      "Epoch 64/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4961 - accuracy: 0.7902 - val_loss: 2.8828 - val_accuracy: 0.7114\n",
      "Epoch 65/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4599 - accuracy: 0.8031 - val_loss: 2.6057 - val_accuracy: 0.6951\n",
      "Epoch 66/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4610 - accuracy: 0.7997 - val_loss: 1.8265 - val_accuracy: 0.7100\n",
      "Epoch 67/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4827 - accuracy: 0.7872 - val_loss: 1.9479 - val_accuracy: 0.7046\n",
      "Epoch 68/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4544 - accuracy: 0.7979 - val_loss: 1.6442 - val_accuracy: 0.6789\n",
      "Epoch 69/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4918 - accuracy: 0.7850 - val_loss: 1.4368 - val_accuracy: 0.6978\n",
      "Epoch 70/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4479 - accuracy: 0.8054 - val_loss: 1.6944 - val_accuracy: 0.7168\n",
      "Epoch 71/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4414 - accuracy: 0.8069 - val_loss: 1.3786 - val_accuracy: 0.7141\n",
      "Epoch 72/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4315 - accuracy: 0.8084 - val_loss: 1.3331 - val_accuracy: 0.6667\n",
      "Epoch 73/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4689 - accuracy: 0.7926 - val_loss: 1.2970 - val_accuracy: 0.7127\n",
      "Epoch 74/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4512 - accuracy: 0.8080 - val_loss: 1.5797 - val_accuracy: 0.6924\n",
      "Epoch 75/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4532 - accuracy: 0.8025 - val_loss: 1.3978 - val_accuracy: 0.7114\n",
      "Epoch 76/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4239 - accuracy: 0.8123 - val_loss: 1.2932 - val_accuracy: 0.7100\n",
      "Epoch 77/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4527 - accuracy: 0.8037 - val_loss: 1.4168 - val_accuracy: 0.7046\n",
      "Epoch 78/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4232 - accuracy: 0.8170 - val_loss: 1.2329 - val_accuracy: 0.7195\n",
      "Epoch 79/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4291 - accuracy: 0.8146 - val_loss: 1.3727 - val_accuracy: 0.7209\n",
      "Epoch 80/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4283 - accuracy: 0.8138 - val_loss: 1.3514 - val_accuracy: 0.7154\n",
      "Epoch 81/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4231 - accuracy: 0.8166 - val_loss: 1.3071 - val_accuracy: 0.7236\n",
      "Epoch 82/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4333 - accuracy: 0.8095 - val_loss: 1.5872 - val_accuracy: 0.7222\n",
      "Epoch 83/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4069 - accuracy: 0.8236 - val_loss: 1.5116 - val_accuracy: 0.7290\n",
      "Epoch 84/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4060 - accuracy: 0.8238 - val_loss: 1.3939 - val_accuracy: 0.6951\n",
      "Epoch 85/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4086 - accuracy: 0.8238 - val_loss: 1.3679 - val_accuracy: 0.7019\n",
      "Epoch 86/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4204 - accuracy: 0.8138 - val_loss: 1.2894 - val_accuracy: 0.7154\n",
      "Epoch 87/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4072 - accuracy: 0.8262 - val_loss: 2.2552 - val_accuracy: 0.6924\n",
      "Epoch 88/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4112 - accuracy: 0.8187 - val_loss: 1.5443 - val_accuracy: 0.7127\n",
      "Epoch 89/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3962 - accuracy: 0.8316 - val_loss: 1.3760 - val_accuracy: 0.6748\n",
      "Epoch 90/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4052 - accuracy: 0.8211 - val_loss: 1.2813 - val_accuracy: 0.7114\n",
      "Epoch 91/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3898 - accuracy: 0.8333 - val_loss: 2.2709 - val_accuracy: 0.7033\n",
      "Epoch 92/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4747 - accuracy: 0.7941 - val_loss: 1.9988 - val_accuracy: 0.6707\n",
      "Epoch 93/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.4318 - accuracy: 0.8149 - val_loss: 1.9430 - val_accuracy: 0.6802\n",
      "Epoch 94/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3934 - accuracy: 0.8267 - val_loss: 1.4845 - val_accuracy: 0.6992\n",
      "Epoch 95/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3737 - accuracy: 0.8389 - val_loss: 1.4915 - val_accuracy: 0.7304\n",
      "Epoch 96/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3930 - accuracy: 0.8265 - val_loss: 1.0198 - val_accuracy: 0.7005\n",
      "Epoch 97/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3743 - accuracy: 0.8339 - val_loss: 1.3459 - val_accuracy: 0.6870\n",
      "Epoch 98/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3866 - accuracy: 0.8306 - val_loss: 1.3554 - val_accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3997 - accuracy: 0.8277 - val_loss: 1.2253 - val_accuracy: 0.7141\n",
      "Epoch 100/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.3760 - accuracy: 0.8380 - val_loss: 1.3614 - val_accuracy: 0.7046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2991857a8c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='transfer_learning')\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], linear=False, density=512)\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512)\n",
    "imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='keep_first')\n",
    "\n",
    "atts_model = create_dense(X_train.shape[1], linear=False)\n",
    "\n",
    "combined = concatenate([atts_model.output, imgs_model.output])\n",
    "\n",
    "# Añado una nueva capa de procesado y una última salida que me haga la clasificación.\n",
    "z = Dense(9, activation='relu')(combined)\n",
    "z = Dense(3, activation='softmax')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res], Y_train_cat, validation_data=([X_val, val_imgs_res], Y_val_cat),\n",
    "             epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.7434924078091106\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy in test: {sum(np.array([np.argmax(comb) for comb in combined_model.predict([X_test,test_imgs_res])])==np.array(Y_test_cat))/len(Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ver ahora cómo se comportaría esta misma arquitectura frente a un problema de regresión añadiendo una capa más de complejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1472.8067 - mse: 1472.8064 - val_loss: 1705.0753 - val_mse: 1705.0752\n",
      "Epoch 2/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 900.1619 - mse: 900.1620 - val_loss: 1356.5290 - val_mse: 1356.5289\n",
      "Epoch 3/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 826.6339 - mse: 826.6337 - val_loss: 1361.7367 - val_mse: 1361.7366\n",
      "Epoch 4/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 833.5047 - mse: 833.5048 - val_loss: 1397.0136 - val_mse: 1397.0135\n",
      "Epoch 5/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 758.1603 - mse: 758.1604 - val_loss: 1350.3175 - val_mse: 1350.3175\n",
      "Epoch 6/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 804.8815 - mse: 804.8813 - val_loss: 1369.9769 - val_mse: 1369.9768\n",
      "Epoch 7/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 764.2962 - mse: 764.2963 - val_loss: 1420.2640 - val_mse: 1420.2639\n",
      "Epoch 8/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 776.2350 - mse: 776.2350 - val_loss: 1319.8637 - val_mse: 1319.8635\n",
      "Epoch 9/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 797.4855 - mse: 797.4856 - val_loss: 1489.5111 - val_mse: 1489.5112\n",
      "Epoch 10/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 786.3590 - mse: 786.3590 - val_loss: 2159.1187 - val_mse: 2159.1187\n",
      "Epoch 11/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 805.6361 - mse: 805.6362 - val_loss: 1358.0444 - val_mse: 1358.0444\n",
      "Epoch 12/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 744.0731 - mse: 744.0732 - val_loss: 1921.3650 - val_mse: 1921.3650\n",
      "Epoch 13/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 812.5948 - mse: 812.5949 - val_loss: 1403.8115 - val_mse: 1403.8114\n",
      "Epoch 14/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 811.1882 - mse: 811.1883 - val_loss: 1507.5919 - val_mse: 1507.5919\n",
      "Epoch 15/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 734.8648 - mse: 734.8649 - val_loss: 1410.6972 - val_mse: 1410.6973\n",
      "Epoch 16/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 786.3139 - mse: 786.3138 - val_loss: 1387.6293 - val_mse: 1387.6293\n",
      "Epoch 17/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 807.3192 - mse: 807.3193 - val_loss: 24793.4413 - val_mse: 24793.4414\n",
      "Epoch 18/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 711.6240 - mse: 711.6240 - val_loss: 2052.4163 - val_mse: 2052.4163\n",
      "Epoch 19/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 920.4561 - mse: 920.4558 - val_loss: 1393.7992 - val_mse: 1393.7992\n",
      "Epoch 20/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1185.3182 - mse: 1185.3182 - val_loss: 28829.4438 - val_mse: 28829.4453\n",
      "Epoch 21/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 845.1229 - mse: 845.1229 - val_loss: 1585.3186 - val_mse: 1585.3185\n",
      "Epoch 22/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 756.4796 - mse: 756.4797 - val_loss: 1461.2070 - val_mse: 1461.2070\n",
      "Epoch 23/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 740.1127 - mse: 740.1129 - val_loss: 1460.2717 - val_mse: 1460.2719\n",
      "Epoch 24/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 693.1761 - mse: 693.1761 - val_loss: 8528.5189 - val_mse: 8528.5186\n",
      "Epoch 25/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 754.6488 - mse: 754.6488 - val_loss: 1453.3916 - val_mse: 1453.3916\n",
      "Epoch 26/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 745.8517 - mse: 745.8517 - val_loss: 1323.3949 - val_mse: 1323.3949\n",
      "Epoch 27/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 700.6532 - mse: 700.6532 - val_loss: 1547.6355 - val_mse: 1547.6355\n",
      "Epoch 28/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 713.4324 - mse: 713.4326 - val_loss: 1325.6238 - val_mse: 1325.6238\n",
      "Epoch 29/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 676.5873 - mse: 676.5875 - val_loss: 1381.7382 - val_mse: 1381.7380\n",
      "Epoch 30/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 727.8851 - mse: 727.8851 - val_loss: 1365.4140 - val_mse: 1365.4142\n",
      "Epoch 31/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 725.9313 - mse: 725.9313 - val_loss: 1551.8432 - val_mse: 1551.8434\n",
      "Epoch 32/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 768.9353 - mse: 768.9354 - val_loss: 1416.7160 - val_mse: 1416.7161\n",
      "Epoch 33/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 757.6284 - mse: 757.6284 - val_loss: 1377.6432 - val_mse: 1377.6432\n",
      "Epoch 34/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 698.0043 - mse: 698.0044 - val_loss: 1373.0422 - val_mse: 1373.0421\n",
      "Epoch 35/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 796.1406 - mse: 796.1406 - val_loss: 1372.3805 - val_mse: 1372.3806\n",
      "Epoch 36/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 730.2532 - mse: 730.2530 - val_loss: 1391.9510 - val_mse: 1391.9509\n",
      "Epoch 37/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 676.5277 - mse: 676.5276 - val_loss: 1325.9595 - val_mse: 1325.9595\n",
      "Epoch 38/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 654.3879 - mse: 654.3881 - val_loss: 1593.7959 - val_mse: 1593.7959\n",
      "Epoch 39/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 832.2923 - mse: 832.2924 - val_loss: 1383.4226 - val_mse: 1383.4225\n",
      "Epoch 40/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 743.5594 - mse: 743.5594 - val_loss: 1391.6968 - val_mse: 1391.6970\n",
      "Epoch 41/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 667.4886 - mse: 667.4885 - val_loss: 1404.8030 - val_mse: 1404.8030\n",
      "Epoch 42/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 658.8516 - mse: 658.8517 - val_loss: 1469.0525 - val_mse: 1469.0525\n",
      "Epoch 43/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 749.9014 - mse: 749.9015 - val_loss: 1307.4502 - val_mse: 1307.4502\n",
      "Epoch 44/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 645.1616 - mse: 645.1614 - val_loss: 1331.8865 - val_mse: 1331.8866\n",
      "Epoch 45/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 757.6088 - mse: 757.6086 - val_loss: 1577.4097 - val_mse: 1577.4095\n",
      "Epoch 46/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 624.9472 - mse: 624.9473 - val_loss: 1769.5233 - val_mse: 1769.5231\n",
      "Epoch 47/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 712.1133 - mse: 712.1133 - val_loss: 1785.2379 - val_mse: 1785.2379\n",
      "Epoch 48/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 683.1001 - mse: 683.1002 - val_loss: 1339.8539 - val_mse: 1339.8539\n",
      "Epoch 49/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 660.9309 - mse: 660.9308 - val_loss: 1452.6891 - val_mse: 1452.6890\n",
      "Epoch 50/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 710.5459 - mse: 710.5460 - val_loss: 1870.2932 - val_mse: 1870.2933\n",
      "Epoch 51/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 708.0057 - mse: 708.0055 - val_loss: 1380.8348 - val_mse: 1380.8350\n",
      "Epoch 52/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 690.1967 - mse: 690.1970 - val_loss: 1340.5272 - val_mse: 1340.5271\n",
      "Epoch 53/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 618.5458 - mse: 618.5460 - val_loss: 1487.4755 - val_mse: 1487.4755\n",
      "Epoch 54/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 636.4721 - mse: 636.4721 - val_loss: 1381.9619 - val_mse: 1381.9619\n",
      "Epoch 55/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 659.9920 - mse: 659.9919 - val_loss: 1326.6604 - val_mse: 1326.6604\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 15s 2ms/step - loss: 640.4487 - mse: 640.4485 - val_loss: 1443.2459 - val_mse: 1443.2460\n",
      "Epoch 57/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 629.9518 - mse: 629.9519 - val_loss: 1374.6974 - val_mse: 1374.6975\n",
      "Epoch 58/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 622.7304 - mse: 622.7305 - val_loss: 1396.4719 - val_mse: 1396.4719\n",
      "Epoch 59/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 680.5748 - mse: 680.5748 - val_loss: 1359.9280 - val_mse: 1359.9280\n",
      "Epoch 60/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 654.7211 - mse: 654.7212 - val_loss: 1345.9263 - val_mse: 1345.9263\n",
      "Epoch 61/100\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 618.0529 - mse: 618.0529 - val_loss: 1468.2623 - val_mse: 1468.2623\n",
      "Epoch 62/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 772.3636 - mse: 772.3635 - val_loss: 1341.7193 - val_mse: 1341.7192\n",
      "Epoch 63/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 600.4599 - mse: 600.4599 - val_loss: 1383.7422 - val_mse: 1383.7422\n",
      "Epoch 64/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 625.4686 - mse: 625.4685 - val_loss: 1533.2021 - val_mse: 1533.2023\n",
      "Epoch 65/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 690.1697 - mse: 690.1696 - val_loss: 1564.9754 - val_mse: 1564.9755\n",
      "Epoch 66/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 659.4896 - mse: 659.4897 - val_loss: 1413.8716 - val_mse: 1413.8716\n",
      "Epoch 67/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 649.4509 - mse: 649.4510 - val_loss: 1482.1197 - val_mse: 1482.1196\n",
      "Epoch 68/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 554.3607 - mse: 554.3606 - val_loss: 1362.3981 - val_mse: 1362.3981\n",
      "Epoch 69/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 607.8989 - mse: 607.8988 - val_loss: 1269.0526 - val_mse: 1269.0526\n",
      "Epoch 70/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 872.1842 - mse: 872.1842 - val_loss: 1281.8136 - val_mse: 1281.8136\n",
      "Epoch 71/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 652.3322 - mse: 652.3323 - val_loss: 1322.0211 - val_mse: 1322.0211\n",
      "Epoch 72/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 683.2024 - mse: 683.2025 - val_loss: 1355.6631 - val_mse: 1355.6631\n",
      "Epoch 73/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 604.0918 - mse: 604.0920 - val_loss: 1826.9732 - val_mse: 1826.9733\n",
      "Epoch 74/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 744.6814 - mse: 744.6812 - val_loss: 1486.2438 - val_mse: 1486.2439\n",
      "Epoch 75/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 624.8945 - mse: 624.8945 - val_loss: 1275.3550 - val_mse: 1275.3550\n",
      "Epoch 76/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 684.3459 - mse: 684.3459 - val_loss: 1208.6592 - val_mse: 1208.6592\n",
      "Epoch 77/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 612.6410 - mse: 612.6410 - val_loss: 1657.1045 - val_mse: 1657.1045\n",
      "Epoch 78/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 599.7447 - mse: 599.7447 - val_loss: 1206.9303 - val_mse: 1206.9304\n",
      "Epoch 79/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 645.3636 - mse: 645.3636 - val_loss: 1658.2579 - val_mse: 1658.2579\n",
      "Epoch 80/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 604.5822 - mse: 604.5822 - val_loss: 1377.3264 - val_mse: 1377.3264\n",
      "Epoch 81/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 563.1215 - mse: 563.1215 - val_loss: 1450.4614 - val_mse: 1450.4615\n",
      "Epoch 82/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 547.5832 - mse: 547.5833 - val_loss: 1386.8253 - val_mse: 1386.8253\n",
      "Epoch 83/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 577.1982 - mse: 577.1982 - val_loss: 1299.3562 - val_mse: 1299.3562\n",
      "Epoch 84/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 650.4731 - mse: 650.4731 - val_loss: 1468.7257 - val_mse: 1468.7258\n",
      "Epoch 85/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 602.5394 - mse: 602.5394 - val_loss: 1460.4929 - val_mse: 1460.4929\n",
      "Epoch 86/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 613.8506 - mse: 613.8507 - val_loss: 1294.7787 - val_mse: 1294.7786\n",
      "Epoch 87/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 556.5584 - mse: 556.5584 - val_loss: 1304.9205 - val_mse: 1304.9205\n",
      "Epoch 88/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 567.9000 - mse: 567.9000 - val_loss: 1421.0552 - val_mse: 1421.0552\n",
      "Epoch 89/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 569.8715 - mse: 569.8716 - val_loss: 1349.2810 - val_mse: 1349.2809\n",
      "Epoch 90/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 583.0605 - mse: 583.0605 - val_loss: 1586.7437 - val_mse: 1586.7439\n",
      "Epoch 91/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 614.0180 - mse: 614.0181 - val_loss: 1214.1897 - val_mse: 1214.1898\n",
      "Epoch 92/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 712.2915 - mse: 712.2914 - val_loss: 1850.9512 - val_mse: 1850.9512\n",
      "Epoch 93/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 621.8922 - mse: 621.8922 - val_loss: 1279.3416 - val_mse: 1279.3417\n",
      "Epoch 94/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 585.8308 - mse: 585.8308 - val_loss: 1512.5836 - val_mse: 1512.5836\n",
      "Epoch 95/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 619.3575 - mse: 619.3575 - val_loss: 1456.8959 - val_mse: 1456.8959\n",
      "Epoch 96/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 593.7016 - mse: 593.7015 - val_loss: 1657.6519 - val_mse: 1657.6520\n",
      "Epoch 97/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 696.6142 - mse: 696.6143 - val_loss: 1314.7798 - val_mse: 1314.7798\n",
      "Epoch 98/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 516.6195 - mse: 516.6195 - val_loss: 1616.7866 - val_mse: 1616.7866\n",
      "Epoch 99/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 556.1337 - mse: 556.1337 - val_loss: 1356.4947 - val_mse: 1356.4946\n",
      "Epoch 100/100\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 580.6883 - mse: 580.6884 - val_loss: 1377.0773 - val_mse: 1377.0773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24065422a48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añado una salida que me haga la regresión.\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "z = Dense(8, activation='relu')(z)\n",
    "z = Dense(1, activation='linear')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res], Y_train, validation_data=([X_val, val_imgs_res], Y_val),\n",
    "             epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1184.428657944656\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((combined_model.predict([X_test, test_imgs_res]) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
