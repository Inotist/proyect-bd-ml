{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda y construcción del modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv',sep=';', decimal='.')\n",
    "val = pd.read_csv('./val.csv',sep=';', decimal='.')\n",
    "test = pd.read_csv('./test.csv',sep=';', decimal='.')\n",
    "\n",
    "images = np.load('images.npy')\n",
    "train_imgs = images[train['Unnamed: 0']]\n",
    "val_imgs = images[val['Unnamed: 0']]\n",
    "test_imgs = images[test['Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraigo la variable objetivo y dropeo los índices para empezar a trabajar con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train.Price\n",
    "Y_val = val.Price\n",
    "Y_test = test.Price\n",
    "\n",
    "X_train = train.drop(columns=['Unnamed: 0','Price'])\n",
    "X_val = val.drop(columns=['Unnamed: 0','Price'])\n",
    "X_test = test.drop(columns=['Unnamed: 0','Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, concatenate\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función me permite simplificar la creación de un modelo denso. Se puede especificar el parámetro linear=True para que devuelva una arquitectura completa con una salida lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(dim, linear=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, activation='relu'))\n",
    "    while dim > 18:\n",
    "        dim //= 2\n",
    "        model.add(Dense(dim, activation='relu'))\n",
    "    \n",
    "    if linear:\n",
    "        model.add(Dense(3, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echo un primer vistazo al modelo con los hiperparámetros que mejor suelen funcionar a nivel general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/200\n",
      "6634/6634 [==============================] - 0s 52us/step - loss: 6207.0029 - mse: 6207.0029 - val_loss: 585853.3750 - val_mse: 585853.3750\n",
      "Epoch 2/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 3985.3408 - mse: 3985.3411 - val_loss: 9040.8213 - val_mse: 9040.8213\n",
      "Epoch 3/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2921.7329 - mse: 2921.7329 - val_loss: 3419.3713 - val_mse: 3419.3713\n",
      "Epoch 4/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2756.8468 - mse: 2756.8464 - val_loss: 3333.9221 - val_mse: 3333.9221\n",
      "Epoch 5/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2686.0220 - mse: 2686.0220 - val_loss: 3196.5515 - val_mse: 3196.5515\n",
      "Epoch 6/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2629.3552 - mse: 2629.3555 - val_loss: 3188.9590 - val_mse: 3188.9590\n",
      "Epoch 7/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2597.9130 - mse: 2597.9128 - val_loss: 3085.8740 - val_mse: 3085.8740\n",
      "Epoch 8/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2526.8608 - mse: 2526.8608 - val_loss: 3036.5212 - val_mse: 3036.5212\n",
      "Epoch 9/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2481.4935 - mse: 2481.4934 - val_loss: 2950.3689 - val_mse: 2950.3689\n",
      "Epoch 10/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 2426.1428 - mse: 2426.1428 - val_loss: 2884.1113 - val_mse: 2884.1113\n",
      "Epoch 11/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2358.2514 - mse: 2358.2515 - val_loss: 2807.4539 - val_mse: 2807.4539\n",
      "Epoch 12/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2298.1033 - mse: 2298.1033 - val_loss: 2699.5757 - val_mse: 2699.5757\n",
      "Epoch 13/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2196.2809 - mse: 2196.2810 - val_loss: 20477.1875 - val_mse: 20477.1875\n",
      "Epoch 14/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2133.8087 - mse: 2133.8086 - val_loss: 2511.8232 - val_mse: 2511.8232\n",
      "Epoch 15/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 2062.2827 - mse: 2062.2827 - val_loss: 2401.7024 - val_mse: 2401.7024\n",
      "Epoch 16/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1983.6619 - mse: 1983.6620 - val_loss: 2302.6479 - val_mse: 2302.6479\n",
      "Epoch 17/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1930.9463 - mse: 1930.9462 - val_loss: 2261.2415 - val_mse: 2261.2415\n",
      "Epoch 18/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1855.4367 - mse: 1855.4369 - val_loss: 2507.0039 - val_mse: 2507.0039\n",
      "Epoch 19/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1808.2760 - mse: 1808.2759 - val_loss: 2269.2368 - val_mse: 2269.2368\n",
      "Epoch 20/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1856.7284 - mse: 1856.7285 - val_loss: 2101.9502 - val_mse: 2101.9502\n",
      "Epoch 21/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1789.4496 - mse: 1789.4497 - val_loss: 3117.5986 - val_mse: 3117.5986\n",
      "Epoch 22/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1735.0023 - mse: 1735.0023 - val_loss: 1976.6866 - val_mse: 1976.6866\n",
      "Epoch 23/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1696.1600 - mse: 1696.1603 - val_loss: 1944.2637 - val_mse: 1944.2637\n",
      "Epoch 24/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1679.6848 - mse: 1679.6848 - val_loss: 1914.8427 - val_mse: 1914.8427\n",
      "Epoch 25/200\n",
      "6634/6634 [==============================] - 0s 12us/step - loss: 1648.7541 - mse: 1648.7542 - val_loss: 1884.8671 - val_mse: 1884.8671\n",
      "Epoch 26/200\n",
      "6634/6634 [==============================] - 0s 9us/step - loss: 1625.5087 - mse: 1625.5085 - val_loss: 2150.2563 - val_mse: 2150.2563\n",
      "Epoch 27/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1671.1831 - mse: 1671.1830 - val_loss: 1907.2365 - val_mse: 1907.2365\n",
      "Epoch 28/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1615.6535 - mse: 1615.6534 - val_loss: 1845.5054 - val_mse: 1845.5054\n",
      "Epoch 29/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1585.6584 - mse: 1585.6584 - val_loss: 1836.4866 - val_mse: 1836.4866\n",
      "Epoch 30/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1585.3002 - mse: 1585.3003 - val_loss: 1815.5454 - val_mse: 1815.5454\n",
      "Epoch 31/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1502.5727 - mse: 1502.5726 - val_loss: 1786.9031 - val_mse: 1786.9031\n",
      "Epoch 32/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1477.6813 - mse: 1477.6814 - val_loss: 1761.1320 - val_mse: 1761.1320\n",
      "Epoch 33/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1463.4731 - mse: 1463.4731 - val_loss: 1730.4790 - val_mse: 1730.4790\n",
      "Epoch 34/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1511.3645 - mse: 1511.3646 - val_loss: 1865.6241 - val_mse: 1865.6241\n",
      "Epoch 35/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1531.2578 - mse: 1531.2578 - val_loss: 14345.2158 - val_mse: 14345.2158\n",
      "Epoch 36/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1484.7008 - mse: 1484.7008 - val_loss: 1764.9352 - val_mse: 1764.9352\n",
      "Epoch 37/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1410.6317 - mse: 1410.6317 - val_loss: 1729.8104 - val_mse: 1729.8104\n",
      "Epoch 38/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1412.2910 - mse: 1412.2909 - val_loss: 4967.8872 - val_mse: 4967.8872\n",
      "Epoch 39/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1393.1297 - mse: 1393.1298 - val_loss: 1794.1835 - val_mse: 1794.1835\n",
      "Epoch 40/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1456.1637 - mse: 1456.1637 - val_loss: 1715.2070 - val_mse: 1715.2070\n",
      "Epoch 41/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1360.7519 - mse: 1360.7518 - val_loss: 1667.6559 - val_mse: 1667.6559\n",
      "Epoch 42/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1338.2206 - mse: 1338.2207 - val_loss: 1654.1984 - val_mse: 1654.1984\n",
      "Epoch 43/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1306.8783 - mse: 1306.8783 - val_loss: 1663.8300 - val_mse: 1663.8300\n",
      "Epoch 44/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1298.1025 - mse: 1298.1025 - val_loss: 1663.1107 - val_mse: 1663.1107\n",
      "Epoch 45/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1298.0073 - mse: 1298.0074 - val_loss: 1665.7634 - val_mse: 1665.7634\n",
      "Epoch 46/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1313.5537 - mse: 1313.5537 - val_loss: 1781.3159 - val_mse: 1781.3159\n",
      "Epoch 47/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1308.0646 - mse: 1308.0646 - val_loss: 1663.1996 - val_mse: 1663.1996\n",
      "Epoch 48/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1279.8407 - mse: 1279.8407 - val_loss: 1628.9855 - val_mse: 1628.9855\n",
      "Epoch 49/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1238.4035 - mse: 1238.4036 - val_loss: 1631.3715 - val_mse: 1631.3715\n",
      "Epoch 50/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1264.2002 - mse: 1264.2002 - val_loss: 1663.9579 - val_mse: 1663.9579\n",
      "Epoch 51/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1227.5918 - mse: 1227.5918 - val_loss: 1626.6617 - val_mse: 1626.6617\n",
      "Epoch 52/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1233.6750 - mse: 1233.6750 - val_loss: 1643.5270 - val_mse: 1643.5270\n",
      "Epoch 53/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1281.1810 - mse: 1281.1809 - val_loss: 5596.4009 - val_mse: 5596.4009\n",
      "Epoch 54/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1325.2621 - mse: 1325.2621 - val_loss: 1616.1700 - val_mse: 1616.1700\n",
      "Epoch 55/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1226.5211 - mse: 1226.5210 - val_loss: 1628.5039 - val_mse: 1628.5039\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 0s 7us/step - loss: 1196.0872 - mse: 1196.0872 - val_loss: 1608.2771 - val_mse: 1608.2771\n",
      "Epoch 57/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1189.6519 - mse: 1189.6520 - val_loss: 1594.4573 - val_mse: 1594.4573\n",
      "Epoch 58/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1184.1135 - mse: 1184.1135 - val_loss: 1611.0870 - val_mse: 1611.0870\n",
      "Epoch 59/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1180.5632 - mse: 1180.5632 - val_loss: 1615.0447 - val_mse: 1615.0447\n",
      "Epoch 60/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1166.1282 - mse: 1166.1282 - val_loss: 1684.2662 - val_mse: 1684.2662\n",
      "Epoch 61/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1223.6097 - mse: 1223.6097 - val_loss: 1840.0471 - val_mse: 1840.0471\n",
      "Epoch 62/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1256.7928 - mse: 1256.7927 - val_loss: 1604.5045 - val_mse: 1604.5045\n",
      "Epoch 63/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1254.6339 - mse: 1254.6339 - val_loss: 1688.0565 - val_mse: 1688.0565\n",
      "Epoch 64/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1205.0950 - mse: 1205.0950 - val_loss: 1681.8916 - val_mse: 1681.8916\n",
      "Epoch 65/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1150.8910 - mse: 1150.8911 - val_loss: 1606.7390 - val_mse: 1606.7390\n",
      "Epoch 66/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1139.0998 - mse: 1139.0997 - val_loss: 1594.3583 - val_mse: 1594.3583\n",
      "Epoch 67/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1116.4987 - mse: 1116.4988 - val_loss: 1564.5092 - val_mse: 1564.5092\n",
      "Epoch 68/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1132.6231 - mse: 1132.6230 - val_loss: 1586.9594 - val_mse: 1586.9594\n",
      "Epoch 69/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1153.6365 - mse: 1153.6365 - val_loss: 1709.3976 - val_mse: 1709.3976\n",
      "Epoch 70/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1146.3162 - mse: 1146.3162 - val_loss: 1571.9375 - val_mse: 1571.9375\n",
      "Epoch 71/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1108.4406 - mse: 1108.4406 - val_loss: 1622.5992 - val_mse: 1622.5992\n",
      "Epoch 72/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1105.4931 - mse: 1105.4930 - val_loss: 1640.9985 - val_mse: 1640.9985\n",
      "Epoch 73/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1122.3342 - mse: 1122.3342 - val_loss: 1637.7041 - val_mse: 1637.7041\n",
      "Epoch 74/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1096.2373 - mse: 1096.2373 - val_loss: 1545.6876 - val_mse: 1545.6876\n",
      "Epoch 75/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1075.9850 - mse: 1075.9850 - val_loss: 1574.8148 - val_mse: 1574.8148\n",
      "Epoch 76/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1086.4096 - mse: 1086.4095 - val_loss: 1578.9924 - val_mse: 1578.9924\n",
      "Epoch 77/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1101.7893 - mse: 1101.7893 - val_loss: 1641.4645 - val_mse: 1641.4645\n",
      "Epoch 78/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1083.0941 - mse: 1083.0941 - val_loss: 1579.1884 - val_mse: 1579.1884\n",
      "Epoch 79/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1055.5866 - mse: 1055.5865 - val_loss: 1592.0123 - val_mse: 1592.0123\n",
      "Epoch 80/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1054.2502 - mse: 1054.2502 - val_loss: 1566.4059 - val_mse: 1566.4059\n",
      "Epoch 81/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1063.2660 - mse: 1063.2660 - val_loss: 1559.0447 - val_mse: 1559.0447\n",
      "Epoch 82/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1118.6869 - mse: 1118.6869 - val_loss: 1544.8845 - val_mse: 1544.8845\n",
      "Epoch 83/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1085.7716 - mse: 1085.7716 - val_loss: 1576.2163 - val_mse: 1576.2163\n",
      "Epoch 84/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1074.7121 - mse: 1074.7120 - val_loss: 1594.8733 - val_mse: 1594.8733\n",
      "Epoch 85/200\n",
      "6634/6634 [==============================] - 0s 10us/step - loss: 1073.0695 - mse: 1073.0695 - val_loss: 1659.6116 - val_mse: 1659.6116\n",
      "Epoch 86/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1119.6746 - mse: 1119.6746 - val_loss: 1606.6769 - val_mse: 1606.6769\n",
      "Epoch 87/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 1102.3505 - mse: 1102.3505 - val_loss: 8733.6299 - val_mse: 8733.6299\n",
      "Epoch 88/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1399.7029 - mse: 1399.7030 - val_loss: 1815.4763 - val_mse: 1815.4763\n",
      "Epoch 89/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1320.5480 - mse: 1320.5481 - val_loss: 2121.7000 - val_mse: 2121.7000\n",
      "Epoch 90/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 1357.1476 - mse: 1357.1476 - val_loss: 1615.4551 - val_mse: 1615.4551\n",
      "Epoch 91/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1198.7608 - mse: 1198.7607 - val_loss: 1664.5731 - val_mse: 1664.5731\n",
      "Epoch 92/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1117.7006 - mse: 1117.7006 - val_loss: 1620.2771 - val_mse: 1620.2771\n",
      "Epoch 93/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1043.3329 - mse: 1043.3329 - val_loss: 1582.6736 - val_mse: 1582.6736\n",
      "Epoch 94/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1040.7153 - mse: 1040.7152 - val_loss: 1565.7985 - val_mse: 1565.7985\n",
      "Epoch 95/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1025.3204 - mse: 1025.3203 - val_loss: 1551.8405 - val_mse: 1551.8405\n",
      "Epoch 96/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1021.8577 - mse: 1021.8577 - val_loss: 1547.7744 - val_mse: 1547.7744\n",
      "Epoch 97/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1008.0174 - mse: 1008.0174 - val_loss: 1525.3712 - val_mse: 1525.3712\n",
      "Epoch 98/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 989.1070 - mse: 989.1069 - val_loss: 1529.6473 - val_mse: 1529.6473\n",
      "Epoch 99/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 987.7348 - mse: 987.7347 - val_loss: 1552.7994 - val_mse: 1552.7994\n",
      "Epoch 100/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1034.7322 - mse: 1034.7322 - val_loss: 1616.3900 - val_mse: 1616.3900\n",
      "Epoch 101/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1065.6745 - mse: 1065.6744 - val_loss: 1654.3671 - val_mse: 1654.3671\n",
      "Epoch 102/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1052.6979 - mse: 1052.6979 - val_loss: 1524.7131 - val_mse: 1524.7131\n",
      "Epoch 103/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1036.4693 - mse: 1036.4692 - val_loss: 1536.3838 - val_mse: 1536.3838\n",
      "Epoch 104/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 991.0172 - mse: 991.0173 - val_loss: 1544.8733 - val_mse: 1544.8733\n",
      "Epoch 105/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1005.9692 - mse: 1005.9692 - val_loss: 1550.7303 - val_mse: 1550.7303\n",
      "Epoch 106/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 985.0863 - mse: 985.0864 - val_loss: 1501.5073 - val_mse: 1501.5073\n",
      "Epoch 107/200\n",
      "6634/6634 [==============================] - 0s 5us/step - loss: 964.7604 - mse: 964.7604 - val_loss: 1585.5535 - val_mse: 1585.5535\n",
      "Epoch 108/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 977.3273 - mse: 977.3274 - val_loss: 1518.6783 - val_mse: 1518.6783\n",
      "Epoch 109/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 955.3382 - mse: 955.3383 - val_loss: 1559.4147 - val_mse: 1559.4147\n",
      "Epoch 110/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 964.7497 - mse: 964.7498 - val_loss: 1528.6774 - val_mse: 1528.6774\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 0s 6us/step - loss: 977.4752 - mse: 977.4753 - val_loss: 1561.8965 - val_mse: 1561.8965\n",
      "Epoch 112/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 997.1956 - mse: 997.1957 - val_loss: 1555.5366 - val_mse: 1555.5366\n",
      "Epoch 113/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1020.7199 - mse: 1020.7198 - val_loss: 1543.6853 - val_mse: 1543.6853\n",
      "Epoch 114/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 961.0771 - mse: 961.0771 - val_loss: 1484.9838 - val_mse: 1484.9838\n",
      "Epoch 115/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 944.1234 - mse: 944.1234 - val_loss: 1508.9685 - val_mse: 1508.9685\n",
      "Epoch 116/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 943.6322 - mse: 943.6322 - val_loss: 1530.2006 - val_mse: 1530.2006\n",
      "Epoch 117/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 951.3755 - mse: 951.3755 - val_loss: 1539.3707 - val_mse: 1539.3707\n",
      "Epoch 118/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1005.8198 - mse: 1005.8198 - val_loss: 1515.8890 - val_mse: 1515.8890\n",
      "Epoch 119/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1036.9353 - mse: 1036.9353 - val_loss: 1852.8182 - val_mse: 1852.8182\n",
      "Epoch 120/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1104.5418 - mse: 1104.5419 - val_loss: 1671.4816 - val_mse: 1671.4816\n",
      "Epoch 121/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1076.4927 - mse: 1076.4926 - val_loss: 1535.0625 - val_mse: 1535.0625\n",
      "Epoch 122/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1010.7344 - mse: 1010.7344 - val_loss: 1587.2659 - val_mse: 1587.2659\n",
      "Epoch 123/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1064.5332 - mse: 1064.5332 - val_loss: 1534.2561 - val_mse: 1534.2561\n",
      "Epoch 124/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 953.9363 - mse: 953.9363 - val_loss: 1689.2650 - val_mse: 1689.2650\n",
      "Epoch 125/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1018.8925 - mse: 1018.8925 - val_loss: 1632.8007 - val_mse: 1632.8007\n",
      "Epoch 126/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1026.3377 - mse: 1026.3378 - val_loss: 1544.6398 - val_mse: 1544.6398\n",
      "Epoch 127/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 966.3312 - mse: 966.3312 - val_loss: 1446.7129 - val_mse: 1446.7129\n",
      "Epoch 128/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 975.0516 - mse: 975.0517 - val_loss: 1594.2504 - val_mse: 1594.2504\n",
      "Epoch 129/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 946.0105 - mse: 946.0106 - val_loss: 1540.4454 - val_mse: 1540.4454\n",
      "Epoch 130/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 935.5436 - mse: 935.5436 - val_loss: 1598.1722 - val_mse: 1598.1722\n",
      "Epoch 131/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 923.2439 - mse: 923.2438 - val_loss: 1467.7606 - val_mse: 1467.7606\n",
      "Epoch 132/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 885.0413 - mse: 885.0413 - val_loss: 1490.0853 - val_mse: 1490.0853\n",
      "Epoch 133/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 926.6417 - mse: 926.6418 - val_loss: 1492.8638 - val_mse: 1492.8638\n",
      "Epoch 134/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 886.9854 - mse: 886.9854 - val_loss: 1476.2605 - val_mse: 1476.2605\n",
      "Epoch 135/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 945.0859 - mse: 945.0859 - val_loss: 1569.6898 - val_mse: 1569.6898\n",
      "Epoch 136/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 951.1173 - mse: 951.1174 - val_loss: 1529.3110 - val_mse: 1529.3110\n",
      "Epoch 137/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 936.7623 - mse: 936.7624 - val_loss: 1488.9808 - val_mse: 1488.9808\n",
      "Epoch 138/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 908.1637 - mse: 908.1637 - val_loss: 1448.4177 - val_mse: 1448.4177\n",
      "Epoch 139/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 936.7822 - mse: 936.7823 - val_loss: 1503.7771 - val_mse: 1503.7771\n",
      "Epoch 140/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 904.2032 - mse: 904.2032 - val_loss: 1466.1345 - val_mse: 1466.1345\n",
      "Epoch 141/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 872.6352 - mse: 872.6353 - val_loss: 1464.8007 - val_mse: 1464.8007\n",
      "Epoch 142/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 881.9632 - mse: 881.9631 - val_loss: 1486.4126 - val_mse: 1486.4126\n",
      "Epoch 143/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 857.0097 - mse: 857.0098 - val_loss: 1446.3059 - val_mse: 1446.3059\n",
      "Epoch 144/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 855.3289 - mse: 855.3289 - val_loss: 1425.0425 - val_mse: 1425.0425\n",
      "Epoch 145/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 851.7383 - mse: 851.7382 - val_loss: 1470.0847 - val_mse: 1470.0847\n",
      "Epoch 146/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 875.7399 - mse: 875.7398 - val_loss: 1522.2283 - val_mse: 1522.2283\n",
      "Epoch 147/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 879.6337 - mse: 879.6337 - val_loss: 1416.2626 - val_mse: 1416.2626\n",
      "Epoch 148/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 875.5471 - mse: 875.5471 - val_loss: 1437.8563 - val_mse: 1437.8563\n",
      "Epoch 149/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 873.1888 - mse: 873.1889 - val_loss: 1447.7988 - val_mse: 1447.7988\n",
      "Epoch 150/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 859.5542 - mse: 859.5542 - val_loss: 1461.8273 - val_mse: 1461.8273\n",
      "Epoch 151/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 925.3166 - mse: 925.3165 - val_loss: 1473.7921 - val_mse: 1473.7921\n",
      "Epoch 152/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 891.3563 - mse: 891.3563 - val_loss: 1478.7993 - val_mse: 1478.7993\n",
      "Epoch 153/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 886.7534 - mse: 886.7533 - val_loss: 1428.9313 - val_mse: 1428.9313\n",
      "Epoch 154/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 930.2710 - mse: 930.2711 - val_loss: 1531.9620 - val_mse: 1531.9620\n",
      "Epoch 155/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 927.8718 - mse: 927.8718 - val_loss: 1569.9302 - val_mse: 1569.9302\n",
      "Epoch 156/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 907.2139 - mse: 907.2139 - val_loss: 1446.1998 - val_mse: 1446.1998\n",
      "Epoch 157/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 853.0763 - mse: 853.0763 - val_loss: 1447.1451 - val_mse: 1447.1451\n",
      "Epoch 158/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 877.1739 - mse: 877.1740 - val_loss: 1491.8694 - val_mse: 1491.8694\n",
      "Epoch 159/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 909.2362 - mse: 909.2362 - val_loss: 1438.2114 - val_mse: 1438.2114\n",
      "Epoch 160/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 863.0220 - mse: 863.0220 - val_loss: 1460.0630 - val_mse: 1460.0630\n",
      "Epoch 161/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 827.9451 - mse: 827.9451 - val_loss: 1433.4714 - val_mse: 1433.4714\n",
      "Epoch 162/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 834.6385 - mse: 834.6385 - val_loss: 1441.1199 - val_mse: 1441.1199\n",
      "Epoch 163/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 809.4006 - mse: 809.4006 - val_loss: 1452.0210 - val_mse: 1452.0210\n",
      "Epoch 164/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 808.0591 - mse: 808.0591 - val_loss: 1429.1421 - val_mse: 1429.1421\n",
      "Epoch 165/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 805.4959 - mse: 805.4958 - val_loss: 1449.2388 - val_mse: 1449.2388\n",
      "Epoch 166/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 832.0321 - mse: 832.0320 - val_loss: 1487.8296 - val_mse: 1487.8296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 909.7989 - mse: 909.7990 - val_loss: 1457.0309 - val_mse: 1457.0309\n",
      "Epoch 168/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 909.7024 - mse: 909.7024 - val_loss: 1416.1782 - val_mse: 1416.1782\n",
      "Epoch 169/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 822.8063 - mse: 822.8063 - val_loss: 1418.8025 - val_mse: 1418.8025\n",
      "Epoch 170/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 801.1188 - mse: 801.1188 - val_loss: 1486.2854 - val_mse: 1486.2854\n",
      "Epoch 171/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 821.9968 - mse: 821.9968 - val_loss: 1442.1281 - val_mse: 1442.1281\n",
      "Epoch 172/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 810.9519 - mse: 810.9520 - val_loss: 1436.6730 - val_mse: 1436.6730\n",
      "Epoch 173/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 823.1643 - mse: 823.1644 - val_loss: 1432.5300 - val_mse: 1432.5300\n",
      "Epoch 174/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 801.5669 - mse: 801.5670 - val_loss: 1538.3923 - val_mse: 1538.3923\n",
      "Epoch 175/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 914.5076 - mse: 914.5076 - val_loss: 1363.4653 - val_mse: 1363.4653\n",
      "Epoch 176/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 886.6211 - mse: 886.6210 - val_loss: 1470.4017 - val_mse: 1470.4017\n",
      "Epoch 177/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 850.0214 - mse: 850.0214 - val_loss: 1442.5945 - val_mse: 1442.5945\n",
      "Epoch 178/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 806.1091 - mse: 806.1091 - val_loss: 1458.2324 - val_mse: 1458.2324\n",
      "Epoch 179/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 816.9712 - mse: 816.9711 - val_loss: 1392.9487 - val_mse: 1392.9487\n",
      "Epoch 180/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 823.7199 - mse: 823.7198 - val_loss: 1608.5436 - val_mse: 1608.5436\n",
      "Epoch 181/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 818.0003 - mse: 818.0003 - val_loss: 1468.3668 - val_mse: 1468.3668\n",
      "Epoch 182/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 840.5589 - mse: 840.5588 - val_loss: 1408.1426 - val_mse: 1408.1426\n",
      "Epoch 183/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 825.1109 - mse: 825.1110 - val_loss: 1588.7261 - val_mse: 1588.7261\n",
      "Epoch 184/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 888.0760 - mse: 888.0760 - val_loss: 1414.7301 - val_mse: 1414.7301\n",
      "Epoch 185/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 960.2616 - mse: 960.2616 - val_loss: 1432.6926 - val_mse: 1432.6926\n",
      "Epoch 186/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 1049.7767 - mse: 1049.7767 - val_loss: 1431.5792 - val_mse: 1431.5792\n",
      "Epoch 187/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 936.3920 - mse: 936.3920 - val_loss: 1478.4780 - val_mse: 1478.4780\n",
      "Epoch 188/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 875.0595 - mse: 875.0596 - val_loss: 1500.5361 - val_mse: 1500.5361\n",
      "Epoch 189/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 845.7558 - mse: 845.7558 - val_loss: 1663.5806 - val_mse: 1663.5806\n",
      "Epoch 190/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 918.0616 - mse: 918.0616 - val_loss: 1729.7355 - val_mse: 1729.7355\n",
      "Epoch 191/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 835.1041 - mse: 835.1041 - val_loss: 1452.2587 - val_mse: 1452.2587\n",
      "Epoch 192/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 775.2678 - mse: 775.2679 - val_loss: 1456.5814 - val_mse: 1456.5814\n",
      "Epoch 193/200\n",
      "6634/6634 [==============================] - 0s 8us/step - loss: 765.1007 - mse: 765.1007 - val_loss: 1430.0980 - val_mse: 1430.0980\n",
      "Epoch 194/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 741.3989 - mse: 741.3989 - val_loss: 1512.5137 - val_mse: 1512.5137\n",
      "Epoch 195/200\n",
      "6634/6634 [==============================] - 0s 7us/step - loss: 777.2277 - mse: 777.2276 - val_loss: 1407.2356 - val_mse: 1407.2356\n",
      "Epoch 196/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 750.1053 - mse: 750.1053 - val_loss: 1468.4135 - val_mse: 1468.4135\n",
      "Epoch 197/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 731.4066 - mse: 731.4066 - val_loss: 1487.9904 - val_mse: 1487.9904\n",
      "Epoch 198/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 735.0356 - mse: 735.0356 - val_loss: 1402.1505 - val_mse: 1402.1505\n",
      "Epoch 199/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 722.4974 - mse: 722.4974 - val_loss: 1470.5332 - val_mse: 1470.5332\n",
      "Epoch 200/200\n",
      "6634/6634 [==============================] - 0s 6us/step - loss: 722.7566 - mse: 722.7565 - val_loss: 1426.7339 - val_mse: 1426.7339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29e7148e208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atts_model = create_dense(X_train.shape[1], linear=True)\n",
    "\n",
    "atts_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "atts_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=200, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1098.314461591573\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((atts_model.predict(X_test) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es excesivamente malo para ser un modelo tan simple. Ahora voy a definir un modelo para trabajar con las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función simplifica la creación de una arquitectura CNN. Su utilidad es parecida a la anterior, pero ésta me da la opción de utilizar una arquitectura base y hacer transfer learning. También con \"keep_first\" permito que la arquitectura base sea entrenable a excepción de la primera capa. Si no le especificamos ningún modelo base, hay una opción de añadir una capa \"dropout\" que podría ayudar en ciertos casos, y el parámetro \"density\" permite especificar la densidad de la capa que sigue al final de las convolucionales. De esta forma podré probar diferentes opciones usando pocas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(shape, base=None, linear=False, density=144, mode='trainable', dropout=None):\n",
    "    \n",
    "    if base:\n",
    "        \n",
    "        if mode == 'transfer_learning':\n",
    "            for layer in base.layers: \n",
    "                layer.trainable = False\n",
    "                \n",
    "        if mode == 'keep_first':\n",
    "            base.layers[1].trainable = False\n",
    "        \n",
    "        model = base.layers[-1].output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(density, activation='relu')(model)\n",
    "        \n",
    "        if linear:\n",
    "            model = Dense(9, activation='relu')(model)\n",
    "            model = Dense(3, activation='relu')(model)\n",
    "            model = Dense(1, activation='linear')(model)\n",
    "            \n",
    "        model = Model(base.input, model)\n",
    "    \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(18, kernel_size=(3, 3), input_shape=shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "        model.add(Conv2D(36, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(density, activation='relu'))\n",
    "    \n",
    "        if linear:\n",
    "            model.add(Dense(9, activation='relu'))\n",
    "            model.add(Dense(3, activation='relu'))\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescalo las imágenes a la mitad de su resolución original por cuestiones de memoria RAM de mi GPU y después estandarizo los datos para que el modelo trabaje con números entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],shape[1]//2,shape[2]//2,shape[3]))\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se comporta esta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/5\n",
      "6634/6634 [==============================] - 17s 3ms/step - loss: 3691.7026 - mse: 3691.7024 - val_loss: 6231.0491 - val_mse: 6231.0493\n",
      "Epoch 2/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 3026.5012 - mse: 3026.5017 - val_loss: 3827.0973 - val_mse: 3827.0972\n",
      "Epoch 3/5\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 2995.1230 - mse: 2995.1223 - val_loss: 3673.2359 - val_mse: 3673.2361\n",
      "Epoch 4/5\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 2990.3713 - mse: 2990.3711 - val_loss: 3697.3896 - val_mse: 3697.3896\n",
      "Epoch 5/5\n",
      "6634/6634 [==============================] - 13s 2ms/step - loss: 2972.8133 - mse: 2972.8135 - val_loss: 3652.1954 - val_mse: 3652.1951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29f461a1c08>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_model = create_cnn(train_imgs_res.shape[1:], linear=True, dropout=0.2, density=512)\n",
    "\n",
    "imgs_model.compile(loss='mse', optimizer='adam' , metrics=['mse'])\n",
    "\n",
    "imgs_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3129.786404807161\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((imgs_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora voy a probar con VGG16 cargando los pesos de imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],48,48,3)) # Reescalo a 48, 48, 3 para poder usar VGG16\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/5\n",
      "6634/6634 [==============================] - 5s 825us/step - loss: 3854.4900 - mse: 3854.4900 - val_loss: 3671.7904 - val_mse: 3671.7908\n",
      "Epoch 2/5\n",
      "6634/6634 [==============================] - 5s 797us/step - loss: 3035.0784 - mse: 3035.0786 - val_loss: 3651.8110 - val_mse: 3651.8110\n",
      "Epoch 3/5\n",
      "6634/6634 [==============================] - 5s 808us/step - loss: 3013.3430 - mse: 3013.3428 - val_loss: 3613.9933 - val_mse: 3613.9932\n",
      "Epoch 4/5\n",
      "6634/6634 [==============================] - 5s 810us/step - loss: 2993.0550 - mse: 2993.0549 - val_loss: 3606.1533 - val_mse: 3606.1531\n",
      "Epoch 5/5\n",
      "6634/6634 [==============================] - 5s 798us/step - loss: 2986.8756 - mse: 2986.8757 - val_loss: 3605.2599 - val_mse: 3605.2598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29fae968b88>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "\n",
    "vgg16_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=True, density=1024, mode='transfer_learning')\n",
    "\n",
    "vgg16_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "vgg16_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3080.5947193321904\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((vgg16_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí he definido dos funciones que permiten generar mis modelos de una forma sencilla. Ahora toca unificarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero voy a pasar la variable objetivo a categórica para trabajar con las categorías \"barato\", \"asequible\" y \"caro\", ya que seguramente serán más fáciles de predecir que un precio aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_train_cat = []\n",
    "for y in Y_train:\n",
    "    if y < 35: Y_train_cat.append(0)\n",
    "    elif y < 85: Y_train_cat.append(1)\n",
    "    else: Y_train_cat.append(2)\n",
    "        \n",
    "plt.hist(Y_train_cat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_cat = []\n",
    "for y in Y_val:\n",
    "    if y < 35: Y_val_cat.append(0)\n",
    "    elif y < 85: Y_val_cat.append(1)\n",
    "    else: Y_val_cat.append(2)\n",
    "        \n",
    "Y_test_cat = []\n",
    "for y in Y_test:\n",
    "    if y < 35: Y_test_cat.append(0)\n",
    "    elif y < 85: Y_test_cat.append(1)\n",
    "    else: Y_test_cat.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un buen reparto de los valores en categorías, veamos como se comportan nuestros modelos si los unificamos. Comienzo creando mis dos modelos (atributos e imágenes) con las funciones que definí al comienzo del notebook, pero esta vez le indicaré en los parámetros que no quiero un final lineal. Así podré combinarlas y agregar las capas finales que crea convenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(he dejado comentados algunos de los modelos que he estado probando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.4825 - accuracy: 0.4828 - val_loss: 4.6277 - val_accuracy: 0.5271\n",
      "Epoch 2/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.0421 - accuracy: 0.5021 - val_loss: 1.0244 - val_accuracy: 0.5312\n",
      "Epoch 3/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.0368 - accuracy: 0.5044 - val_loss: 1.0247 - val_accuracy: 0.5285\n",
      "Epoch 4/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.0338 - accuracy: 0.5035 - val_loss: 1.0178 - val_accuracy: 0.5285\n",
      "Epoch 5/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1.0272 - accuracy: 0.5083 - val_loss: 0.9878 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.9502 - accuracy: 0.5485 - val_loss: 0.9108 - val_accuracy: 0.5772\n",
      "Epoch 7/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.9000 - accuracy: 0.5686 - val_loss: 0.8795 - val_accuracy: 0.5854\n",
      "Epoch 8/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.8380 - accuracy: 0.5944 - val_loss: 0.8412 - val_accuracy: 0.5935\n",
      "Epoch 9/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.8407 - accuracy: 0.5817 - val_loss: 0.8228 - val_accuracy: 0.5637\n",
      "Epoch 10/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7971 - accuracy: 0.6058 - val_loss: 0.7841 - val_accuracy: 0.6382\n",
      "Epoch 11/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7543 - accuracy: 0.6423 - val_loss: 0.7714 - val_accuracy: 0.6192\n",
      "Epoch 12/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7427 - accuracy: 0.6513 - val_loss: 0.7395 - val_accuracy: 0.6396\n",
      "Epoch 13/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7427 - accuracy: 0.6575 - val_loss: 0.7684 - val_accuracy: 0.6328\n",
      "Epoch 14/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7093 - accuracy: 0.6788 - val_loss: 0.7426 - val_accuracy: 0.6409\n",
      "Epoch 15/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.7001 - accuracy: 0.6825 - val_loss: 0.7096 - val_accuracy: 0.6789\n",
      "Epoch 16/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6866 - accuracy: 0.6929 - val_loss: 1.8681 - val_accuracy: 0.6599\n",
      "Epoch 17/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6790 - accuracy: 0.6938 - val_loss: 0.6874 - val_accuracy: 0.6911\n",
      "Epoch 18/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6777 - accuracy: 0.6952 - val_loss: 0.7110 - val_accuracy: 0.6734\n",
      "Epoch 19/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6508 - accuracy: 0.7113 - val_loss: 0.7268 - val_accuracy: 0.7019\n",
      "Epoch 20/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6372 - accuracy: 0.7225 - val_loss: 0.7883 - val_accuracy: 0.6843\n",
      "Epoch 21/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6268 - accuracy: 0.7226 - val_loss: 0.7929 - val_accuracy: 0.6951\n",
      "Epoch 22/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6334 - accuracy: 0.7195 - val_loss: 0.7607 - val_accuracy: 0.6721\n",
      "Epoch 23/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6382 - accuracy: 0.7226 - val_loss: 0.7522 - val_accuracy: 0.6518\n",
      "Epoch 24/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6438 - accuracy: 0.7186 - val_loss: 1.5029 - val_accuracy: 0.7046\n",
      "Epoch 25/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6252 - accuracy: 0.7238 - val_loss: 0.7372 - val_accuracy: 0.6585\n",
      "Epoch 26/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6020 - accuracy: 0.7385 - val_loss: 1.0654 - val_accuracy: 0.7182\n",
      "Epoch 27/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6026 - accuracy: 0.7382 - val_loss: 6.3976 - val_accuracy: 0.6612\n",
      "Epoch 28/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6314 - accuracy: 0.7199 - val_loss: 11.1698 - val_accuracy: 0.6938\n",
      "Epoch 29/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6465 - accuracy: 0.7251 - val_loss: 1.2112 - val_accuracy: 0.6734\n",
      "Epoch 30/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6039 - accuracy: 0.7345 - val_loss: 1.8986 - val_accuracy: 0.7127\n",
      "Epoch 31/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6025 - accuracy: 0.7342 - val_loss: 1.8096 - val_accuracy: 0.7060\n",
      "Epoch 32/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5825 - accuracy: 0.7490 - val_loss: 1.6586 - val_accuracy: 0.6789\n",
      "Epoch 33/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5710 - accuracy: 0.7511 - val_loss: 1.1935 - val_accuracy: 0.7182\n",
      "Epoch 34/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5862 - accuracy: 0.7446 - val_loss: 1.1051 - val_accuracy: 0.6951\n",
      "Epoch 35/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5547 - accuracy: 0.7609 - val_loss: 1.1298 - val_accuracy: 0.6938\n",
      "Epoch 36/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5736 - accuracy: 0.7474 - val_loss: 0.9827 - val_accuracy: 0.6829\n",
      "Epoch 37/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5591 - accuracy: 0.7541 - val_loss: 3.1615 - val_accuracy: 0.6734\n",
      "Epoch 38/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5693 - accuracy: 0.7450 - val_loss: 1.1689 - val_accuracy: 0.7019\n",
      "Epoch 39/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5527 - accuracy: 0.7605 - val_loss: 2.6521 - val_accuracy: 0.6897\n",
      "Epoch 40/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5370 - accuracy: 0.7644 - val_loss: 1.2800 - val_accuracy: 0.6938\n",
      "Epoch 41/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5303 - accuracy: 0.7743 - val_loss: 1.2752 - val_accuracy: 0.6992\n",
      "Epoch 42/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5529 - accuracy: 0.7560 - val_loss: 0.7302 - val_accuracy: 0.7060\n",
      "Epoch 43/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5321 - accuracy: 0.7689 - val_loss: 2.0890 - val_accuracy: 0.6816\n",
      "Epoch 44/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5468 - accuracy: 0.7620 - val_loss: 0.9210 - val_accuracy: 0.7073\n",
      "Epoch 45/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5234 - accuracy: 0.7710 - val_loss: 0.6716 - val_accuracy: 0.7033\n",
      "Epoch 46/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5337 - accuracy: 0.7679 - val_loss: 3.4774 - val_accuracy: 0.6992\n",
      "Epoch 47/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5405 - accuracy: 0.7602 - val_loss: 2.3227 - val_accuracy: 0.6545\n",
      "Epoch 48/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.6100 - accuracy: 0.7445 - val_loss: 1.8628 - val_accuracy: 0.7046\n",
      "Epoch 49/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5171 - accuracy: 0.7743 - val_loss: 1.5494 - val_accuracy: 0.7100\n",
      "Epoch 50/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 0.5058 - accuracy: 0.7835 - val_loss: 1.5300 - val_accuracy: 0.7033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29fa10ad288>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='transfer_learning')\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], linear=False, density=512)\n",
    "#imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512)\n",
    "imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='keep_first')\n",
    "\n",
    "atts_model = create_dense(X_train.shape[1], linear=False)\n",
    "\n",
    "combined = concatenate([atts_model.output, imgs_model.output])\n",
    "\n",
    "# Añado una nueva capa de procesado y una última salida que me haga la clasificación.\n",
    "z = Dense(9, activation='relu')(combined)\n",
    "z = Dense(3, activation='softmax')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res], Y_train_cat, validation_data=([X_val, val_imgs_res], Y_val_cat),\n",
    "             epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy in test: {sum(np.array([np.argmax(comb) for comb in combined_model.predict([X_test,test_imgs_res])])==np.array(Y_test_cat))/len(Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ver ahora cómo se comportaría esta misma arquitectura frente a un problema de regresión añadiendo una capa más de complejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 4758.3197 - mse: 4758.3198 - val_loss: 78425.0164 - val_mse: 78425.0156\n",
      "Epoch 2/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 2048.2493 - mse: 2048.2495 - val_loss: 4758.2724 - val_mse: 4758.2725\n",
      "Epoch 3/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1498.0831 - mse: 1498.0833 - val_loss: 2373.9849 - val_mse: 2373.9849\n",
      "Epoch 4/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1345.4903 - mse: 1345.4904 - val_loss: 2260.4643 - val_mse: 2260.4644\n",
      "Epoch 5/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1329.2197 - mse: 1329.2195 - val_loss: 1787.4342 - val_mse: 1787.4343\n",
      "Epoch 6/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1322.2091 - mse: 1322.2091 - val_loss: 2644.8494 - val_mse: 2644.8494\n",
      "Epoch 7/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1332.8248 - mse: 1332.8248 - val_loss: 2863.3058 - val_mse: 2863.3059\n",
      "Epoch 8/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1269.8220 - mse: 1269.8221 - val_loss: 1707.4158 - val_mse: 1707.4158\n",
      "Epoch 9/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1253.9754 - mse: 1253.9755 - val_loss: 1665.4645 - val_mse: 1665.4645\n",
      "Epoch 10/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1246.9801 - mse: 1246.9800 - val_loss: 4140.2905 - val_mse: 4140.2905\n",
      "Epoch 11/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1198.2543 - mse: 1198.2544 - val_loss: 1437.4716 - val_mse: 1437.4717\n",
      "Epoch 12/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1114.3764 - mse: 1114.3762 - val_loss: 2039.1057 - val_mse: 2039.1057\n",
      "Epoch 13/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1093.9662 - mse: 1093.9661 - val_loss: 1832.0389 - val_mse: 1832.0388\n",
      "Epoch 14/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1143.3593 - mse: 1143.3593 - val_loss: 2357.6191 - val_mse: 2357.6191\n",
      "Epoch 15/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1155.9996 - mse: 1155.9996 - val_loss: 1581.5578 - val_mse: 1581.5576\n",
      "Epoch 16/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1140.0404 - mse: 1140.0405 - val_loss: 2057.9009 - val_mse: 2057.9009\n",
      "Epoch 17/50\n",
      "6634/6634 [==============================] - 15s 2ms/step - loss: 1111.0048 - mse: 1111.0049 - val_loss: 1627.3795 - val_mse: 1627.3794\n",
      "Epoch 18/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1017.7171 - mse: 1017.7172 - val_loss: 1453.4567 - val_mse: 1453.4567\n",
      "Epoch 19/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1096.1422 - mse: 1096.1425 - val_loss: 2315.7011 - val_mse: 2315.7012\n",
      "Epoch 20/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1063.0060 - mse: 1063.0060 - val_loss: 1733.5742 - val_mse: 1733.5743\n",
      "Epoch 21/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 987.2818 - mse: 987.2820 - val_loss: 1573.6589 - val_mse: 1573.6589\n",
      "Epoch 22/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 985.3531 - mse: 985.3530 - val_loss: 7332.2735 - val_mse: 7332.2739\n",
      "Epoch 23/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1011.1946 - mse: 1011.1946 - val_loss: 1654.2268 - val_mse: 1654.2268\n",
      "Epoch 24/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1076.5790 - mse: 1076.5790 - val_loss: 3777.8972 - val_mse: 3777.8975\n",
      "Epoch 25/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 957.0815 - mse: 957.0815 - val_loss: 1568.5786 - val_mse: 1568.5786\n",
      "Epoch 26/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 961.3939 - mse: 961.3937 - val_loss: 1537.4786 - val_mse: 1537.4785\n",
      "Epoch 27/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 969.0106 - mse: 969.0106 - val_loss: 1893.5892 - val_mse: 1893.5892\n",
      "Epoch 28/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 1017.8011 - mse: 1017.8010 - val_loss: 1686.8621 - val_mse: 1686.8622\n",
      "Epoch 29/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 990.4258 - mse: 990.4257 - val_loss: 1415.4590 - val_mse: 1415.4591\n",
      "Epoch 30/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 923.7182 - mse: 923.7183 - val_loss: 1592.7699 - val_mse: 1592.7698\n",
      "Epoch 31/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 961.9423 - mse: 961.9423 - val_loss: 1295.9181 - val_mse: 1295.9180\n",
      "Epoch 32/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 942.3998 - mse: 942.3998 - val_loss: 1544.9359 - val_mse: 1544.9360\n",
      "Epoch 33/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 946.7158 - mse: 946.7159 - val_loss: 1505.5163 - val_mse: 1505.5162\n",
      "Epoch 34/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 938.0440 - mse: 938.0440 - val_loss: 1380.8745 - val_mse: 1380.8744\n",
      "Epoch 35/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 933.7409 - mse: 933.7410 - val_loss: 1381.8358 - val_mse: 1381.8358\n",
      "Epoch 36/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 890.1663 - mse: 890.1663 - val_loss: 1550.5279 - val_mse: 1550.5278\n",
      "Epoch 37/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 870.6714 - mse: 870.6716 - val_loss: 3769.2616 - val_mse: 3769.2615\n",
      "Epoch 38/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 863.1585 - mse: 863.1581 - val_loss: 1641.3218 - val_mse: 1641.3218\n",
      "Epoch 39/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 934.5009 - mse: 934.5009 - val_loss: 1461.4642 - val_mse: 1461.4641\n",
      "Epoch 40/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 892.6704 - mse: 892.6707 - val_loss: 1432.5342 - val_mse: 1432.5342\n",
      "Epoch 41/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 852.8782 - mse: 852.8781 - val_loss: 1521.4730 - val_mse: 1521.4729\n",
      "Epoch 42/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 850.1150 - mse: 850.1149 - val_loss: 1459.7078 - val_mse: 1459.7079\n",
      "Epoch 43/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 888.6358 - mse: 888.6359 - val_loss: 1495.8240 - val_mse: 1495.8240\n",
      "Epoch 44/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 875.2429 - mse: 875.2429 - val_loss: 1560.8951 - val_mse: 1560.8951\n",
      "Epoch 45/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 841.1810 - mse: 841.1810 - val_loss: 1405.7245 - val_mse: 1405.7245\n",
      "Epoch 46/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 845.4315 - mse: 845.4314 - val_loss: 1453.5726 - val_mse: 1453.5726\n",
      "Epoch 47/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 843.9761 - mse: 843.9761 - val_loss: 1373.3172 - val_mse: 1373.3171\n",
      "Epoch 48/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 880.0596 - mse: 880.0596 - val_loss: 1600.6496 - val_mse: 1600.6495\n",
      "Epoch 49/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 852.7441 - mse: 852.7440 - val_loss: 1643.2316 - val_mse: 1643.2317\n",
      "Epoch 50/50\n",
      "6634/6634 [==============================] - 14s 2ms/step - loss: 947.5065 - mse: 947.5067 - val_loss: 1570.5875 - val_mse: 1570.5874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29fa98cadc8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añado una salida que me haga la regresión.\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "z = Dense(8, activation='relu')(z)\n",
    "z = Dense(1, activation='linear')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res], Y_train, validation_data=([X_val, val_imgs_res], Y_val),\n",
    "             epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1099.408170057614\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((combined_model.predict([X_test, test_imgs_res]) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí éstos son los mejores resultados que he podido obtener, en el siguiente notebook utilizaré los datos de las variables descriptivas para intentar mejorar las predicciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
