{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregando una red neuronal recurrente a nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo cargando los datos, definiendo mis funciones del notebook 3 y dejando ya preparadas las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv',sep=';', decimal='.')\n",
    "val = pd.read_csv('./val.csv',sep=';', decimal='.')\n",
    "test = pd.read_csv('./test.csv',sep=';', decimal='.')\n",
    "\n",
    "train_desc = pd.read_csv('./train_desc.csv',sep=';', decimal='.')\n",
    "val_desc = pd.read_csv('./val_desc.csv',sep=';', decimal='.')\n",
    "test_desc = pd.read_csv('./test_desc.csv',sep=';', decimal='.')\n",
    "\n",
    "images = np.load('images.npy')\n",
    "train_imgs = images[train['Unnamed: 0']]\n",
    "val_imgs = images[val['Unnamed: 0']]\n",
    "test_imgs = images[test['Unnamed: 0']]\n",
    "\n",
    "Y_train = train.Price\n",
    "Y_val = val.Price\n",
    "Y_test = test.Price\n",
    "\n",
    "X_train = train.drop(columns=['Unnamed: 0','Price'])\n",
    "X_val = val.drop(columns=['Unnamed: 0','Price'])\n",
    "X_test = test.drop(columns=['Unnamed: 0','Price'])\n",
    "\n",
    "train_desc = train_desc.drop(columns=['Unnamed: 0'])\n",
    "val_desc = val_desc.drop(columns=['Unnamed: 0'])\n",
    "test_desc = test_desc.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, LSTM, concatenate\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(dim, linear=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, activation='relu'))\n",
    "    while dim > 18:\n",
    "        dim //= 2\n",
    "        model.add(Dense(dim, activation='relu'))\n",
    "    \n",
    "    if linear:\n",
    "        model.add(Dense(3, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(shape, base=None, linear=False, density=144, mode='trainable', dropout=None):\n",
    "    \n",
    "    if base:\n",
    "        \n",
    "        if mode == 'transfer_learning':\n",
    "            for layer in base.layers: \n",
    "                layer.trainable = False\n",
    "                \n",
    "        if mode == 'keep_first':\n",
    "            base.layers[1].trainable = False\n",
    "        \n",
    "        model = base.layers[-1].output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(density, activation='relu')(model)\n",
    "        \n",
    "        if linear:\n",
    "            model = Dense(9, activation='relu')(model)\n",
    "            model = Dense(3, activation='relu')(model)\n",
    "            model = Dense(1, activation='linear')(model)\n",
    "            \n",
    "        model = Model(base.input, model)\n",
    "    \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(18, kernel_size=(3, 3), input_shape=shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "        model.add(Conv2D(36, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(density, activation='relu'))\n",
    "    \n",
    "        if linear:\n",
    "            model.add(Dense(9, activation='relu'))\n",
    "            model.add(Dense(3, activation='relu'))\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescalo las imágenes para VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for length in (train_imgs.shape[0], val_imgs.shape[0], test_imgs.shape[0]):\n",
    "    shapes.append((length,48,48,3))\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A partir de aquí comenzaré a trabajar las variables descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo vectorizando las variables descriptivas para que el modelo pueda trabajar con ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque ha sido improvisado, tuve que apañarme con algo rápido por falta de tiempo pero soy consciente de que se podría haber hecho mucho mejor (el código está muy amontonado y los datos no son tratados todo lo bien que se debería)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Establezco el número máximo de palabras de cada elemento.\n",
    "maxim = 50\n",
    "\n",
    "for col, idcol in zip(train_desc.columns,range(len(train_desc.columns))):\n",
    "    \n",
    "    # Añado la palabra \"none\" a los registros vacíos.\n",
    "    train_desc[col] = train_desc[col].fillna('none')\n",
    "    val_desc[col] = val_desc[col].fillna('none')\n",
    "    test_desc[col] = test_desc[col].fillna('none')\n",
    "    \n",
    "    # Creo un diccionario con Tokenizer que le da un valor a cada palabra\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(train_desc[col])\n",
    "    \n",
    "    # Utilizo dicho diccionario para convertir los textos en arrays numéricos\n",
    "    # Aquí el código es algo sucio pero quería hacer algo rápido por falta de tiempo.\n",
    "    train_desc[col] = train_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    val_desc[col] = val_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    test_desc[col] = test_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    # Recorro cada dataset para fijar los vectores en 50 números (50 palabras).\n",
    "    # Los vectores que no llegan a 50 se rellenan con ceros.\n",
    "    for idx, vect in zip(range(len(train_desc)),train_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            train_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            train_desc[col][idx] = vect[:maxim]\n",
    "            \n",
    "    for idx, vect in zip(range(len(val_desc)),val_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            val_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            val_desc[col][idx] = vect[:maxim]\n",
    "            \n",
    "    for idx, vect in zip(range(len(test_desc)),test_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            test_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            test_desc[col][idx] = vect[:maxim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque transformo mis dataframes en arrays de numpy con un shape adecuado para que la red LSTM pueda procesarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = np.zeros(tuple(list(train_desc.shape)+[maxim]))\n",
    "val_processed = np.zeros(tuple(list(val_desc.shape)+[maxim]))\n",
    "test_processed = np.zeros(tuple(list(test_desc.shape)+[maxim]))\n",
    "\n",
    "for col,idcol in zip(train_desc.columns,range(15)):\n",
    "    for idx in range(len(train_desc)):\n",
    "        for i in range(maxim):\n",
    "            train_processed[idx][idcol][i] = train_desc[col][idx][i]\n",
    "            \n",
    "    for idx in range(len(val_desc)):\n",
    "        for i in range(maxim):\n",
    "            val_processed[idx][idcol][i] = val_desc[col][idx][i]\n",
    "            \n",
    "    for idx in range(len(test_desc)):\n",
    "        for i in range(maxim):\n",
    "            test_processed[idx][idcol][i] = test_desc[col][idx][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo mi modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/50\n",
      "6634/6634 [==============================] - 2s 298us/step - loss: 3238.1098 - mse: 3238.1104 - val_loss: 3314.2494 - val_mse: 3314.2490\n",
      "Epoch 2/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 2650.0342 - mse: 2650.0337 - val_loss: 3164.2696 - val_mse: 3164.2693\n",
      "Epoch 3/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 2427.8215 - mse: 2427.8213 - val_loss: 3030.5322 - val_mse: 3030.5322\n",
      "Epoch 4/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 2282.3230 - mse: 2282.3232 - val_loss: 2809.2496 - val_mse: 2809.2495\n",
      "Epoch 5/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1870.4601 - mse: 1870.4601 - val_loss: 2766.3927 - val_mse: 2766.3923\n",
      "Epoch 6/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1638.8180 - mse: 1638.8181 - val_loss: 3237.3681 - val_mse: 3237.3682\n",
      "Epoch 7/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 1477.1021 - mse: 1477.1022 - val_loss: 3052.2111 - val_mse: 3052.2109\n",
      "Epoch 8/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1258.5280 - mse: 1258.5282 - val_loss: 2773.3333 - val_mse: 2773.3333\n",
      "Epoch 9/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1055.0043 - mse: 1055.0043 - val_loss: 2805.8288 - val_mse: 2805.8289\n",
      "Epoch 10/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 897.5151 - mse: 897.5152 - val_loss: 2805.3486 - val_mse: 2805.3486\n",
      "Epoch 11/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 783.0824 - mse: 783.0823 - val_loss: 2732.4577 - val_mse: 2732.4578\n",
      "Epoch 12/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 676.4418 - mse: 676.4417 - val_loss: 2628.6207 - val_mse: 2628.6208\n",
      "Epoch 13/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 600.5956 - mse: 600.5957 - val_loss: 2579.9817 - val_mse: 2579.9817\n",
      "Epoch 14/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 531.0742 - mse: 531.0743 - val_loss: 2535.0330 - val_mse: 2535.0330\n",
      "Epoch 15/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 485.5264 - mse: 485.5264 - val_loss: 2567.0542 - val_mse: 2567.0542\n",
      "Epoch 16/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 439.7498 - mse: 439.7498 - val_loss: 2628.0134 - val_mse: 2628.0137\n",
      "Epoch 17/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 406.2709 - mse: 406.2709 - val_loss: 2539.2409 - val_mse: 2539.2410\n",
      "Epoch 18/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 360.3726 - mse: 360.3726 - val_loss: 2532.5010 - val_mse: 2532.5007\n",
      "Epoch 19/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 333.1940 - mse: 333.1940 - val_loss: 2610.9870 - val_mse: 2610.9871\n",
      "Epoch 20/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 335.0300 - mse: 335.0300 - val_loss: 2669.7353 - val_mse: 2669.7354\n",
      "Epoch 21/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 315.2120 - mse: 315.2119 - val_loss: 2695.6787 - val_mse: 2695.6787\n",
      "Epoch 22/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 298.5206 - mse: 298.5206 - val_loss: 2844.1480 - val_mse: 2844.1479\n",
      "Epoch 23/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 256.1556 - mse: 256.1557 - val_loss: 2741.1925 - val_mse: 2741.1926\n",
      "Epoch 24/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 240.5084 - mse: 240.5084 - val_loss: 2773.1114 - val_mse: 2773.1113\n",
      "Epoch 25/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 242.7249 - mse: 242.7249 - val_loss: 2748.5455 - val_mse: 2748.5452\n",
      "Epoch 26/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 227.2084 - mse: 227.2084 - val_loss: 2647.3872 - val_mse: 2647.3875\n",
      "Epoch 27/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 202.4476 - mse: 202.4476 - val_loss: 2600.0935 - val_mse: 2600.0938\n",
      "Epoch 28/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 176.6952 - mse: 176.6951 - val_loss: 2629.0609 - val_mse: 2629.0610\n",
      "Epoch 29/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 160.5706 - mse: 160.5705 - val_loss: 2614.4212 - val_mse: 2614.4211\n",
      "Epoch 30/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 162.9068 - mse: 162.9068 - val_loss: 2471.3704 - val_mse: 2471.3704\n",
      "Epoch 31/50\n",
      "6634/6634 [==============================] - 2s 250us/step - loss: 155.9009 - mse: 155.9009 - val_loss: 2638.0354 - val_mse: 2638.0354\n",
      "Epoch 32/50\n",
      "6634/6634 [==============================] - 2s 249us/step - loss: 167.6505 - mse: 167.6505 - val_loss: 2572.7493 - val_mse: 2572.7493\n",
      "Epoch 33/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 164.3994 - mse: 164.3994 - val_loss: 2597.3882 - val_mse: 2597.3882\n",
      "Epoch 34/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 149.0769 - mse: 149.0769 - val_loss: 2561.1015 - val_mse: 2561.1016\n",
      "Epoch 35/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 158.9777 - mse: 158.9777 - val_loss: 2461.4030 - val_mse: 2461.4031\n",
      "Epoch 36/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 133.1648 - mse: 133.1648 - val_loss: 2485.3185 - val_mse: 2485.3184\n",
      "Epoch 37/50\n",
      "6634/6634 [==============================] - 2s 249us/step - loss: 111.7241 - mse: 111.7242 - val_loss: 2584.2644 - val_mse: 2584.2646\n",
      "Epoch 38/50\n",
      "6634/6634 [==============================] - 2s 249us/step - loss: 110.4618 - mse: 110.4618 - val_loss: 2466.3464 - val_mse: 2466.3464\n",
      "Epoch 39/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 127.2851 - mse: 127.2851 - val_loss: 2486.8564 - val_mse: 2486.8564\n",
      "Epoch 40/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 119.6092 - mse: 119.6092 - val_loss: 2522.0773 - val_mse: 2522.0774\n",
      "Epoch 41/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 108.0139 - mse: 108.0139 - val_loss: 2510.0631 - val_mse: 2510.0632\n",
      "Epoch 42/50\n",
      "6634/6634 [==============================] - 2s 249us/step - loss: 123.8563 - mse: 123.8564 - val_loss: 2571.1809 - val_mse: 2571.1809\n",
      "Epoch 43/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 115.4238 - mse: 115.4238 - val_loss: 2452.4721 - val_mse: 2452.4719\n",
      "Epoch 44/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 116.5134 - mse: 116.5134 - val_loss: 2494.3139 - val_mse: 2494.3140\n",
      "Epoch 45/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 112.0623 - mse: 112.0623 - val_loss: 2444.7354 - val_mse: 2444.7356\n",
      "Epoch 46/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 135.4147 - mse: 135.4147 - val_loss: 2446.6057 - val_mse: 2446.6055\n",
      "Epoch 47/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 121.8677 - mse: 121.8677 - val_loss: 2519.8339 - val_mse: 2519.8337\n",
      "Epoch 48/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 128.1786 - mse: 128.1786 - val_loss: 2487.9368 - val_mse: 2487.9370\n",
      "Epoch 49/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 129.8236 - mse: 129.8236 - val_loss: 2544.8711 - val_mse: 2544.8708\n",
      "Epoch 50/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 129.3760 - mse: 129.3760 - val_loss: 2365.3931 - val_mse: 2365.3931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23d9e2a0a48>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(train_processed.shape[1:])))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(train_processed, Y_train, validation_data=(val_processed, Y_val), epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 2454.830600556245\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((model.predict(test_processed)-Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si puedo mejorar mis resultados sumando esta red a las dos que ya tenía en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/200\n",
      "6634/6634 [==============================] - 17s 3ms/step - loss: 4969.7386 - mse: 4969.7388 - val_loss: 77635.8663 - val_mse: 77635.8672\n",
      "Epoch 2/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 2398.8473 - mse: 2398.8474 - val_loss: 4236.9835 - val_mse: 4236.9839\n",
      "Epoch 3/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1980.9149 - mse: 1980.9152 - val_loss: 2470.9642 - val_mse: 2470.9644\n",
      "Epoch 4/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1605.1658 - mse: 1605.1660 - val_loss: 2353.0114 - val_mse: 2353.0115\n",
      "Epoch 5/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1268.5616 - mse: 1268.5615 - val_loss: 17679.4449 - val_mse: 17679.4453\n",
      "Epoch 6/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1153.1931 - mse: 1153.1931 - val_loss: 1900.8646 - val_mse: 1900.8646\n",
      "Epoch 7/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 881.7789 - mse: 881.7789 - val_loss: 2340.6805 - val_mse: 2340.6807\n",
      "Epoch 8/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 744.6790 - mse: 744.6788 - val_loss: 2086.6637 - val_mse: 2086.6638\n",
      "Epoch 9/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 711.0066 - mse: 711.0067 - val_loss: 1782.7481 - val_mse: 1782.7482\n",
      "Epoch 10/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 600.6064 - mse: 600.6064 - val_loss: 1912.3655 - val_mse: 1912.3657\n",
      "Epoch 11/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 533.5937 - mse: 533.5937 - val_loss: 2119.7394 - val_mse: 2119.7393\n",
      "Epoch 12/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 540.9584 - mse: 540.9583 - val_loss: 1803.5789 - val_mse: 1803.5787\n",
      "Epoch 13/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 391.0452 - mse: 391.0453 - val_loss: 1914.7238 - val_mse: 1914.7238\n",
      "Epoch 14/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 335.3449 - mse: 335.3449 - val_loss: 1820.9437 - val_mse: 1820.9436\n",
      "Epoch 15/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 361.1729 - mse: 361.1729 - val_loss: 1653.5760 - val_mse: 1653.5762\n",
      "Epoch 16/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 317.9642 - mse: 317.9643 - val_loss: 1765.0288 - val_mse: 1765.0288\n",
      "Epoch 17/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 313.1180 - mse: 313.1180 - val_loss: 1670.9983 - val_mse: 1670.9985\n",
      "Epoch 18/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 289.2066 - mse: 289.2066 - val_loss: 1547.2911 - val_mse: 1547.2911\n",
      "Epoch 19/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 269.7269 - mse: 269.7269 - val_loss: 1595.3367 - val_mse: 1595.3367\n",
      "Epoch 20/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 257.7253 - mse: 257.7253 - val_loss: 1692.8223 - val_mse: 1692.8225\n",
      "Epoch 21/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 247.3889 - mse: 247.3889 - val_loss: 1930.9594 - val_mse: 1930.9594\n",
      "Epoch 22/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 228.4629 - mse: 228.4628 - val_loss: 2360.2834 - val_mse: 2360.2834\n",
      "Epoch 23/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 199.0469 - mse: 199.0469 - val_loss: 1729.0768 - val_mse: 1729.0769\n",
      "Epoch 24/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 196.2448 - mse: 196.2448 - val_loss: 3746.1341 - val_mse: 3746.1340\n",
      "Epoch 25/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 152.9598 - mse: 152.9598 - val_loss: 15993.9079 - val_mse: 15993.9092\n",
      "Epoch 26/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 157.0174 - mse: 157.0174 - val_loss: 20145.9291 - val_mse: 20145.9277\n",
      "Epoch 27/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 171.6434 - mse: 171.6434 - val_loss: 1619.0660 - val_mse: 1619.0659\n",
      "Epoch 28/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 129.5288 - mse: 129.5288 - val_loss: 1527.3325 - val_mse: 1527.3325\n",
      "Epoch 29/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 157.7113 - mse: 157.7113 - val_loss: 1624.2572 - val_mse: 1624.2571\n",
      "Epoch 30/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 152.7776 - mse: 152.7776 - val_loss: 1726.2276 - val_mse: 1726.2274\n",
      "Epoch 31/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 143.3395 - mse: 143.3395 - val_loss: 1571.9256 - val_mse: 1571.9257\n",
      "Epoch 32/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 129.9788 - mse: 129.9788 - val_loss: 1304.5799 - val_mse: 1304.5800\n",
      "Epoch 33/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 122.4546 - mse: 122.4546 - val_loss: 1585.7217 - val_mse: 1585.7217\n",
      "Epoch 34/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 121.0983 - mse: 121.0983 - val_loss: 1716.9837 - val_mse: 1716.9835\n",
      "Epoch 35/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 114.1598 - mse: 114.1598 - val_loss: 1328.3027 - val_mse: 1328.3027\n",
      "Epoch 36/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 91.8771 - mse: 91.8772 - val_loss: 2033.3927 - val_mse: 2033.3926\n",
      "Epoch 37/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 78.8463 - mse: 78.8463 - val_loss: 1338.7189 - val_mse: 1338.7189\n",
      "Epoch 38/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 132.2099 - mse: 132.2099 - val_loss: 1535.0840 - val_mse: 1535.0840\n",
      "Epoch 39/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 130.9614 - mse: 130.9614 - val_loss: 1375.3597 - val_mse: 1375.3599\n",
      "Epoch 40/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 99.2210 - mse: 99.2210 - val_loss: 1392.3142 - val_mse: 1392.3142\n",
      "Epoch 41/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 84.5435 - mse: 84.5435 - val_loss: 1361.4847 - val_mse: 1361.4846\n",
      "Epoch 42/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 71.8509 - mse: 71.8508 - val_loss: 1508.6136 - val_mse: 1508.6136\n",
      "Epoch 43/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 74.7570 - mse: 74.7570 - val_loss: 1444.6354 - val_mse: 1444.6355\n",
      "Epoch 44/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 72.5075 - mse: 72.5075 - val_loss: 1516.1318 - val_mse: 1516.1320\n",
      "Epoch 45/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 107.3196 - mse: 107.3196 - val_loss: 1372.4777 - val_mse: 1372.4778\n",
      "Epoch 46/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 93.1238 - mse: 93.1238 - val_loss: 1595.6304 - val_mse: 1595.6302\n",
      "Epoch 47/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 72.0970 - mse: 72.0970 - val_loss: 1463.9366 - val_mse: 1463.9366\n",
      "Epoch 48/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 71.6365 - mse: 71.6366 - val_loss: 1948.6330 - val_mse: 1948.6333\n",
      "Epoch 49/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 63.7074 - mse: 63.7074 - val_loss: 1326.2779 - val_mse: 1326.2778\n",
      "Epoch 50/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 55.5163 - mse: 55.5163 - val_loss: 1603.1760 - val_mse: 1603.1760\n",
      "Epoch 51/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 65.1279 - mse: 65.1279 - val_loss: 1472.6352 - val_mse: 1472.6354\n",
      "Epoch 52/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 63.8417 - mse: 63.8417 - val_loss: 1634.6822 - val_mse: 1634.6823\n",
      "Epoch 53/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 59.6866 - mse: 59.6866 - val_loss: 1411.0934 - val_mse: 1411.0935\n",
      "Epoch 54/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 63.0515 - mse: 63.0515 - val_loss: 1429.0961 - val_mse: 1429.0962\n",
      "Epoch 55/200\n",
      "6634/6634 [==============================] - 17s 2ms/step - loss: 74.7906 - mse: 74.7906 - val_loss: 1576.4719 - val_mse: 1576.4719\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 16s 2ms/step - loss: 87.6389 - mse: 87.6389 - val_loss: 1482.6007 - val_mse: 1482.6010\n",
      "Epoch 57/200\n",
      "6634/6634 [==============================] - 17s 2ms/step - loss: 71.8933 - mse: 71.8933 - val_loss: 2850.3140 - val_mse: 2850.3145\n",
      "Epoch 58/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 149.9390 - mse: 149.9390 - val_loss: 4182.8437 - val_mse: 4182.8438\n",
      "Epoch 59/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 189.4941 - mse: 189.4941 - val_loss: 1809.2259 - val_mse: 1809.2258\n",
      "Epoch 60/200\n",
      "6634/6634 [==============================] - 17s 2ms/step - loss: 158.4206 - mse: 158.4206 - val_loss: 1395.8902 - val_mse: 1395.8903\n",
      "Epoch 61/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 143.2293 - mse: 143.2293 - val_loss: 1372.5967 - val_mse: 1372.5967\n",
      "Epoch 62/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 131.0300 - mse: 131.0300 - val_loss: 1541.1743 - val_mse: 1541.1744\n",
      "Epoch 63/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 101.8853 - mse: 101.8852 - val_loss: 1292.9149 - val_mse: 1292.9148\n",
      "Epoch 64/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 90.1917 - mse: 90.1917 - val_loss: 1477.8037 - val_mse: 1477.8038\n",
      "Epoch 65/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 70.7257 - mse: 70.7257 - val_loss: 1362.8163 - val_mse: 1362.8163\n",
      "Epoch 66/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 64.3528 - mse: 64.3527 - val_loss: 1386.2009 - val_mse: 1386.2008\n",
      "Epoch 67/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 48.4515 - mse: 48.4515 - val_loss: 1346.7839 - val_mse: 1346.7841\n",
      "Epoch 68/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 162.4630 - mse: 162.4630 - val_loss: 1957.6200 - val_mse: 1957.6201\n",
      "Epoch 69/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 212.2886 - mse: 212.2887 - val_loss: 1282.8330 - val_mse: 1282.8330\n",
      "Epoch 70/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 144.1293 - mse: 144.1293 - val_loss: 1327.7373 - val_mse: 1327.7374\n",
      "Epoch 71/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 157.0927 - mse: 157.0927 - val_loss: 1265.3279 - val_mse: 1265.3279\n",
      "Epoch 72/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 112.5051 - mse: 112.5051 - val_loss: 1317.3578 - val_mse: 1317.3579\n",
      "Epoch 73/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 82.8677 - mse: 82.8677 - val_loss: 1299.1103 - val_mse: 1299.1102\n",
      "Epoch 74/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 74.9099 - mse: 74.9099 - val_loss: 1302.9198 - val_mse: 1302.9198\n",
      "Epoch 75/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 52.7274 - mse: 52.7274 - val_loss: 1317.8358 - val_mse: 1317.8358\n",
      "Epoch 76/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 41.7879 - mse: 41.7879 - val_loss: 1275.7740 - val_mse: 1275.7739\n",
      "Epoch 77/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 32.3727 - mse: 32.3727 - val_loss: 1313.6089 - val_mse: 1313.6089\n",
      "Epoch 78/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.6758 - mse: 33.6758 - val_loss: 1338.4064 - val_mse: 1338.4064\n",
      "Epoch 79/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 31.8243 - mse: 31.8243 - val_loss: 1315.3132 - val_mse: 1315.3132\n",
      "Epoch 80/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 27.3499 - mse: 27.3499 - val_loss: 1269.5465 - val_mse: 1269.5464\n",
      "Epoch 81/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 25.4771 - mse: 25.4771 - val_loss: 1289.1169 - val_mse: 1289.1168\n",
      "Epoch 82/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 24.0729 - mse: 24.0729 - val_loss: 1243.6305 - val_mse: 1243.6305\n",
      "Epoch 83/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 20.9361 - mse: 20.9361 - val_loss: 1327.4853 - val_mse: 1327.4852\n",
      "Epoch 84/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.4128 - mse: 17.4128 - val_loss: 1292.1206 - val_mse: 1292.1206\n",
      "Epoch 85/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 18.5309 - mse: 18.5309 - val_loss: 1300.6526 - val_mse: 1300.6527\n",
      "Epoch 86/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.9853 - mse: 17.9853 - val_loss: 1268.9175 - val_mse: 1268.9176\n",
      "Epoch 87/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 18.4187 - mse: 18.4187 - val_loss: 1270.8867 - val_mse: 1270.8867\n",
      "Epoch 88/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 18.2089 - mse: 18.2089 - val_loss: 1273.4258 - val_mse: 1273.4259\n",
      "Epoch 89/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 22.4306 - mse: 22.4306 - val_loss: 1305.3168 - val_mse: 1305.3168\n",
      "Epoch 90/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.9305 - mse: 23.9305 - val_loss: 1268.1874 - val_mse: 1268.1874\n",
      "Epoch 91/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.4044 - mse: 21.4044 - val_loss: 1260.2712 - val_mse: 1260.2711\n",
      "Epoch 92/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.2638 - mse: 21.2638 - val_loss: 1323.6043 - val_mse: 1323.6042\n",
      "Epoch 93/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 29.4217 - mse: 29.4217 - val_loss: 1299.1049 - val_mse: 1299.1047\n",
      "Epoch 94/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 44.4421 - mse: 44.4420 - val_loss: 1337.0493 - val_mse: 1337.0493\n",
      "Epoch 95/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 46.3293 - mse: 46.3293 - val_loss: 1261.4460 - val_mse: 1261.4459\n",
      "Epoch 96/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 41.7713 - mse: 41.7713 - val_loss: 1650.4419 - val_mse: 1650.4419\n",
      "Epoch 97/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 49.4007 - mse: 49.4007 - val_loss: 1365.6462 - val_mse: 1365.6461\n",
      "Epoch 98/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 54.6970 - mse: 54.6970 - val_loss: 1437.1286 - val_mse: 1437.1288\n",
      "Epoch 99/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 96.6999 - mse: 96.6998 - val_loss: 1406.3678 - val_mse: 1406.3677\n",
      "Epoch 100/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 119.8973 - mse: 119.8973 - val_loss: 1290.1812 - val_mse: 1290.1812\n",
      "Epoch 101/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 4272.8453 - mse: 4272.8452 - val_loss: 3347.5188 - val_mse: 3347.5190\n",
      "Epoch 102/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1889.9400 - mse: 1889.9401 - val_loss: 2352.6033 - val_mse: 2352.6033\n",
      "Epoch 103/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1005.6777 - mse: 1005.6777 - val_loss: 1809.0595 - val_mse: 1809.0596\n",
      "Epoch 104/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 653.6241 - mse: 653.6241 - val_loss: 1664.4513 - val_mse: 1664.4514\n",
      "Epoch 105/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 453.2483 - mse: 453.2482 - val_loss: 1604.8754 - val_mse: 1604.8754\n",
      "Epoch 106/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 356.3268 - mse: 356.3268 - val_loss: 1587.5947 - val_mse: 1587.5947\n",
      "Epoch 107/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 253.7748 - mse: 253.7748 - val_loss: 1535.8535 - val_mse: 1535.8535\n",
      "Epoch 108/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 210.5736 - mse: 210.5736 - val_loss: 1593.8708 - val_mse: 1593.8707\n",
      "Epoch 109/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 179.4068 - mse: 179.4069 - val_loss: 1483.9530 - val_mse: 1483.9529\n",
      "Epoch 110/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 124.8741 - mse: 124.8741 - val_loss: 1618.8130 - val_mse: 1618.8132\n",
      "Epoch 111/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 131.2398 - mse: 131.2398 - val_loss: 1549.7076 - val_mse: 1549.7075\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 16s 2ms/step - loss: 108.7885 - mse: 108.7885 - val_loss: 1564.0886 - val_mse: 1564.0886\n",
      "Epoch 113/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 82.6055 - mse: 82.6055 - val_loss: 1594.3972 - val_mse: 1594.3972\n",
      "Epoch 114/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 63.8327 - mse: 63.8327 - val_loss: 1594.8450 - val_mse: 1594.8450\n",
      "Epoch 115/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 55.5876 - mse: 55.5876 - val_loss: 1629.0106 - val_mse: 1629.0105\n",
      "Epoch 116/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 56.9714 - mse: 56.9714 - val_loss: 1515.3490 - val_mse: 1515.3489\n",
      "Epoch 117/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 54.1085 - mse: 54.1085 - val_loss: 1538.6227 - val_mse: 1538.6227\n",
      "Epoch 118/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 48.9131 - mse: 48.9131 - val_loss: 1502.4023 - val_mse: 1502.4025\n",
      "Epoch 119/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 49.7025 - mse: 49.7024 - val_loss: 1480.4721 - val_mse: 1480.4720\n",
      "Epoch 120/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 51.3181 - mse: 51.3181 - val_loss: 1560.0097 - val_mse: 1560.0095\n",
      "Epoch 121/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 49.1166 - mse: 49.1166 - val_loss: 1524.4318 - val_mse: 1524.4318\n",
      "Epoch 122/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 44.7100 - mse: 44.7100 - val_loss: 1499.5092 - val_mse: 1499.5092\n",
      "Epoch 123/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 49.3351 - mse: 49.3351 - val_loss: 1515.8963 - val_mse: 1515.8964\n",
      "Epoch 124/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 52.7056 - mse: 52.7056 - val_loss: 1489.6403 - val_mse: 1489.6404\n",
      "Epoch 125/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 39.0985 - mse: 39.0985 - val_loss: 1509.7358 - val_mse: 1509.7357\n",
      "Epoch 126/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 35.4390 - mse: 35.4390 - val_loss: 1498.9737 - val_mse: 1498.9738\n",
      "Epoch 127/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 38.8466 - mse: 38.8465 - val_loss: 1501.2811 - val_mse: 1501.2810\n",
      "Epoch 128/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 48.5281 - mse: 48.5281 - val_loss: 1509.4065 - val_mse: 1509.4065\n",
      "Epoch 129/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 38.4078 - mse: 38.4078 - val_loss: 1442.2143 - val_mse: 1442.2145\n",
      "Epoch 130/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.8248 - mse: 33.8247 - val_loss: 1494.7501 - val_mse: 1494.7501\n",
      "Epoch 131/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 32.2848 - mse: 32.2848 - val_loss: 1534.4595 - val_mse: 1534.4595\n",
      "Epoch 132/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 28.6311 - mse: 28.6311 - val_loss: 1512.5745 - val_mse: 1512.5743\n",
      "Epoch 133/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 31.0967 - mse: 31.0967 - val_loss: 1444.8526 - val_mse: 1444.8524\n",
      "Epoch 134/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 30.6152 - mse: 30.6152 - val_loss: 1480.7663 - val_mse: 1480.7662\n",
      "Epoch 135/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.7004 - mse: 33.7004 - val_loss: 1456.9808 - val_mse: 1456.9807\n",
      "Epoch 136/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 45.0589 - mse: 45.0590 - val_loss: 1482.9720 - val_mse: 1482.9719\n",
      "Epoch 137/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 50.4177 - mse: 50.4177 - val_loss: 1459.9915 - val_mse: 1459.9913\n",
      "Epoch 138/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 62.4886 - mse: 62.4886 - val_loss: 1488.9131 - val_mse: 1488.9131\n",
      "Epoch 139/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 59.0716 - mse: 59.0717 - val_loss: 1512.8701 - val_mse: 1512.8702\n",
      "Epoch 140/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 73.5482 - mse: 73.5482 - val_loss: 1635.9641 - val_mse: 1635.9641\n",
      "Epoch 141/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 93.7915 - mse: 93.7915 - val_loss: 1434.9973 - val_mse: 1434.9973\n",
      "Epoch 142/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 84.7045 - mse: 84.7045 - val_loss: 1500.5497 - val_mse: 1500.5497\n",
      "Epoch 143/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 64.8594 - mse: 64.8594 - val_loss: 1404.4500 - val_mse: 1404.4501\n",
      "Epoch 144/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 53.7012 - mse: 53.7012 - val_loss: 1648.3334 - val_mse: 1648.3334\n",
      "Epoch 145/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 43.8340 - mse: 43.8340 - val_loss: 1485.3113 - val_mse: 1485.3112\n",
      "Epoch 146/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 37.7436 - mse: 37.7436 - val_loss: 1490.3482 - val_mse: 1490.3483\n",
      "Epoch 147/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 35.2356 - mse: 35.2356 - val_loss: 1464.7676 - val_mse: 1464.7676\n",
      "Epoch 148/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 43.2358 - mse: 43.2358 - val_loss: 1453.7428 - val_mse: 1453.7427\n",
      "Epoch 149/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 42.3119 - mse: 42.3119 - val_loss: 1456.6633 - val_mse: 1456.6633\n",
      "Epoch 150/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 28.6041 - mse: 28.6041 - val_loss: 1486.8691 - val_mse: 1486.8690\n",
      "Epoch 151/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 24.7255 - mse: 24.7255 - val_loss: 1485.1043 - val_mse: 1485.1044\n",
      "Epoch 152/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.7505 - mse: 21.7505 - val_loss: 1470.8720 - val_mse: 1470.8721\n",
      "Epoch 153/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.5896 - mse: 21.5896 - val_loss: 1476.6644 - val_mse: 1476.6644\n",
      "Epoch 154/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.9184 - mse: 23.9184 - val_loss: 1438.3852 - val_mse: 1438.3854\n",
      "Epoch 155/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.7070 - mse: 21.7070 - val_loss: 1408.8182 - val_mse: 1408.8182\n",
      "Epoch 156/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.3793 - mse: 23.3793 - val_loss: 1423.8244 - val_mse: 1423.8246\n",
      "Epoch 157/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 24.6039 - mse: 24.6039 - val_loss: 1425.7416 - val_mse: 1425.7416\n",
      "Epoch 158/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 26.2053 - mse: 26.2053 - val_loss: 1443.5471 - val_mse: 1443.5471\n",
      "Epoch 159/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 26.6084 - mse: 26.6084 - val_loss: 1457.6139 - val_mse: 1457.6140\n",
      "Epoch 160/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 32.7612 - mse: 32.7612 - val_loss: 1426.0552 - val_mse: 1426.0552\n",
      "Epoch 161/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.4675 - mse: 33.4675 - val_loss: 1415.0030 - val_mse: 1415.0031\n",
      "Epoch 162/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 42.7254 - mse: 42.7253 - val_loss: 1412.0141 - val_mse: 1412.0140\n",
      "Epoch 163/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 88.5612 - mse: 88.5612 - val_loss: 1562.7595 - val_mse: 1562.7593\n",
      "Epoch 164/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 153.9675 - mse: 153.9675 - val_loss: 1347.9013 - val_mse: 1347.9011\n",
      "Epoch 165/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 206.1070 - mse: 206.1070 - val_loss: 1679.5812 - val_mse: 1679.5813\n",
      "Epoch 166/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 181.0625 - mse: 181.0625 - val_loss: 1592.8686 - val_mse: 1592.8685\n",
      "Epoch 167/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 150.0253 - mse: 150.0253 - val_loss: 1461.8336 - val_mse: 1461.8336\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 16s 2ms/step - loss: 95.5918 - mse: 95.5918 - val_loss: 1503.5309 - val_mse: 1503.5309\n",
      "Epoch 169/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 61.5170 - mse: 61.5170 - val_loss: 1509.5077 - val_mse: 1509.5076\n",
      "Epoch 170/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 47.0103 - mse: 47.0103 - val_loss: 1463.0832 - val_mse: 1463.0834\n",
      "Epoch 171/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.7084 - mse: 33.7084 - val_loss: 1510.7296 - val_mse: 1510.7295\n",
      "Epoch 172/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 25.2991 - mse: 25.2991 - val_loss: 1439.8092 - val_mse: 1439.8091\n",
      "Epoch 173/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 22.1584 - mse: 22.1584 - val_loss: 1456.0676 - val_mse: 1456.0676\n",
      "Epoch 174/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 20.8818 - mse: 20.8818 - val_loss: 1442.2658 - val_mse: 1442.2657\n",
      "Epoch 175/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.9505 - mse: 17.9505 - val_loss: 1410.8920 - val_mse: 1410.8920\n",
      "Epoch 176/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 18.3314 - mse: 18.3314 - val_loss: 1419.2960 - val_mse: 1419.2960\n",
      "Epoch 177/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.9119 - mse: 17.9119 - val_loss: 1388.7787 - val_mse: 1388.7787\n",
      "Epoch 178/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 20.1645 - mse: 20.1645 - val_loss: 1427.5118 - val_mse: 1427.5118\n",
      "Epoch 179/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.7118 - mse: 17.7118 - val_loss: 1464.7329 - val_mse: 1464.7329\n",
      "Epoch 180/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 16.8494 - mse: 16.8494 - val_loss: 1424.5559 - val_mse: 1424.5559\n",
      "Epoch 181/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 16.8868 - mse: 16.8868 - val_loss: 1469.5320 - val_mse: 1469.5320\n",
      "Epoch 182/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 17.2258 - mse: 17.2258 - val_loss: 1456.1230 - val_mse: 1456.1229\n",
      "Epoch 183/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 22.8676 - mse: 22.8676 - val_loss: 1413.8855 - val_mse: 1413.8855\n",
      "Epoch 184/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.3300 - mse: 21.3300 - val_loss: 1381.4461 - val_mse: 1381.4462\n",
      "Epoch 185/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 22.0567 - mse: 22.0567 - val_loss: 1361.2052 - val_mse: 1361.2052\n",
      "Epoch 186/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.6257 - mse: 23.6257 - val_loss: 1523.5956 - val_mse: 1523.5957\n",
      "Epoch 187/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 21.1880 - mse: 21.1880 - val_loss: 1439.8901 - val_mse: 1439.8903\n",
      "Epoch 188/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 20.0854 - mse: 20.0854 - val_loss: 1465.8759 - val_mse: 1465.8760\n",
      "Epoch 189/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 19.0534 - mse: 19.0534 - val_loss: 1371.9764 - val_mse: 1371.9766\n",
      "Epoch 190/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.5901 - mse: 23.5901 - val_loss: 1432.6919 - val_mse: 1432.6919\n",
      "Epoch 191/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 24.3498 - mse: 24.3498 - val_loss: 1353.3469 - val_mse: 1353.3468\n",
      "Epoch 192/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 23.6432 - mse: 23.6432 - val_loss: 1352.5886 - val_mse: 1352.5886\n",
      "Epoch 193/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 26.8018 - mse: 26.8018 - val_loss: 1344.4603 - val_mse: 1344.4603\n",
      "Epoch 194/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 43.1579 - mse: 43.1579 - val_loss: 1370.8911 - val_mse: 1370.8910\n",
      "Epoch 195/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 35.5019 - mse: 35.5019 - val_loss: 1442.6772 - val_mse: 1442.6771\n",
      "Epoch 196/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 35.3250 - mse: 35.3250 - val_loss: 1365.0315 - val_mse: 1365.0315\n",
      "Epoch 197/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 34.3288 - mse: 34.3288 - val_loss: 1390.9912 - val_mse: 1390.9913\n",
      "Epoch 198/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 38.0118 - mse: 38.0118 - val_loss: 1467.0962 - val_mse: 1467.0964\n",
      "Epoch 199/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 38.4043 - mse: 38.4043 - val_loss: 1353.4695 - val_mse: 1353.4695\n",
      "Epoch 200/200\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 45.8874 - mse: 45.8874 - val_loss: 1288.4377 - val_mse: 1288.4377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23da3468588>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='keep_first')\n",
    "\n",
    "atts_model = create_dense(X_train.shape[1], linear=False)\n",
    "\n",
    "desc_model = Sequential()\n",
    "desc_model.add(LSTM(512, input_shape=(train_processed.shape[1:])))\n",
    "desc_model.add(Dense(1024, activation='relu'))\n",
    "desc_model.add(Dense(512, activation='relu'))\n",
    "\n",
    "combined = concatenate([atts_model.output, imgs_model.output, desc_model.output])\n",
    "\n",
    "# Añado una salida que me haga la regresión.\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "z = Dense(8, activation='relu')(z)\n",
    "z = Dense(1, activation='linear')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input, desc_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res, train_processed], Y_train,\n",
    "                   validation_data=([X_val, val_imgs_res, val_processed], Y_val),\n",
    "                   epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1366.6725110306572\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((combined_model.predict([X_test, test_imgs_res, test_processed]) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diría que mejora un poquitín la capacidad de generalización respecto al modelo que solo utiliza imágenes y datos numéricos, viendo que los resultados en validación y test aquí son muy parecidos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
