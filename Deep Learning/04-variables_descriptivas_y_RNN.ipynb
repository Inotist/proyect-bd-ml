{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregando una red neuronal recurrente a nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo cargando los datos, definiendo mis funciones del notebook 3 y dejando ya preparadas las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv',sep=';', decimal='.')\n",
    "val = pd.read_csv('./val.csv',sep=';', decimal='.')\n",
    "test = pd.read_csv('./test.csv',sep=';', decimal='.')\n",
    "\n",
    "train_desc = pd.read_csv('./train_desc.csv',sep=';', decimal='.')\n",
    "val_desc = pd.read_csv('./val_desc.csv',sep=';', decimal='.')\n",
    "test_desc = pd.read_csv('./test_desc.csv',sep=';', decimal='.')\n",
    "\n",
    "images = np.load('images.npy')\n",
    "train_imgs = images[train['Unnamed: 0']]\n",
    "val_imgs = images[val['Unnamed: 0']]\n",
    "test_imgs = images[test['Unnamed: 0']]\n",
    "\n",
    "Y_train = train.Price\n",
    "Y_val = val.Price\n",
    "Y_test = test.Price\n",
    "\n",
    "X_train = train.drop(columns=['Unnamed: 0','Price'])\n",
    "X_val = val.drop(columns=['Unnamed: 0','Price'])\n",
    "X_test = test.drop(columns=['Unnamed: 0','Price'])\n",
    "\n",
    "train_desc = train_desc.drop(columns=['Unnamed: 0'])\n",
    "val_desc = val_desc.drop(columns=['Unnamed: 0'])\n",
    "test_desc = test_desc.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, LSTM, concatenate\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(dim, linear=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, input_dim=dim, activation='relu'))\n",
    "    while dim > 18:\n",
    "        dim //= 2\n",
    "        model.add(Dense(dim, activation='relu'))\n",
    "    \n",
    "    if linear:\n",
    "        model.add(Dense(3, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(shape, base=None, linear=False, density=144, mode='trainable', dropout=None):\n",
    "    \n",
    "    if base:\n",
    "        \n",
    "        if mode == 'transfer_learning':\n",
    "            for layer in base.layers: \n",
    "                layer.trainable = False\n",
    "                \n",
    "        if mode == 'keep_first':\n",
    "            base.layers[1].trainable = False\n",
    "        \n",
    "        model = base.layers[-1].output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(density, activation='relu')(model)\n",
    "        \n",
    "        if linear:\n",
    "            model = Dense(9, activation='relu')(model)\n",
    "            model = Dense(3, activation='relu')(model)\n",
    "            model = Dense(1, activation='linear')(model)\n",
    "            \n",
    "        model = Model(base.input, model)\n",
    "    \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(18, kernel_size=(3, 3), input_shape=shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "        model.add(Conv2D(36, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        if dropout: model.add(Dropout(dropout))\n",
    "    \n",
    "        model.add(Conv2D(72, kernel_size=(3, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(density, activation='relu'))\n",
    "    \n",
    "        if linear:\n",
    "            model.add(Dense(9, activation='relu'))\n",
    "            model.add(Dense(3, activation='relu'))\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescalo las imágenes para VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for length in (train_imgs.shape[0], val_imgs.shape[0], test_imgs.shape[0]):\n",
    "    shapes.append((length,48,48,3))\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A partir de aquí comenzaré a trabajar las variables descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo vectorizando las variables descriptivas para que el modelo pueda trabajar con ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque ha sido improvisado, tuve que apañarme con algo rápido por falta de tiempo pero soy consciente de que se podría haber hecho mucho mejor (el código está muy amontonado y los datos no son tratados todo lo bien que se debería)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Establezco el número máximo de palabras de cada elemento.\n",
    "maxim = 50\n",
    "\n",
    "for col, idcol in zip(train_desc.columns,range(len(train_desc.columns))):\n",
    "    \n",
    "    # Añado la palabra \"none\" a los registros vacíos.\n",
    "    train_desc[col] = train_desc[col].fillna('none')\n",
    "    val_desc[col] = val_desc[col].fillna('none')\n",
    "    test_desc[col] = test_desc[col].fillna('none')\n",
    "    \n",
    "    # Creo un diccionario con Tokenizer que le da un valor a cada palabra\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(train_desc[col])\n",
    "    \n",
    "    # Utilizo dicho diccionario para convertir los textos en arrays numéricos\n",
    "    # Aquí el código es algo sucio pero quería hacer algo rápido por falta de tiempo.\n",
    "    train_desc[col] = train_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    val_desc[col] = val_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    test_desc[col] = test_desc[col].apply(lambda x: list(filter(lambda c: c != None,\n",
    "                                                                  [t.word_index.get(w.lower()) for w in\n",
    "                                                                   filter(lambda f: f != '',\n",
    "                                                                          re.split('[^a-zA-Z]',x))])))\n",
    "    \n",
    "    # Recorro cada dataset para fijar los vectores en 50 números (50 palabras).\n",
    "    # Los vectores que no llegan a 50 se rellenan con ceros.\n",
    "    for idx, vect in zip(range(len(train_desc)),train_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            train_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            train_desc[col][idx] = vect[:maxim]\n",
    "            \n",
    "    for idx, vect in zip(range(len(val_desc)),val_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            val_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            val_desc[col][idx] = vect[:maxim]\n",
    "            \n",
    "    for idx, vect in zip(range(len(test_desc)),test_desc[col]):\n",
    "        if len(vect) < maxim:\n",
    "            test_desc[col][idx] = np.concatenate((np.array(vect), np.zeros(maxim-len(vect)))).tolist()\n",
    "        elif len(vect) > maxim:\n",
    "            test_desc[col][idx] = vect[:maxim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque transformo mis dataframes en arrays de numpy con un shape adecuado para que la red LSTM pueda procesarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = np.zeros(tuple(list(train_desc.shape)+[maxim]))\n",
    "val_processed = np.zeros(tuple(list(val_desc.shape)+[maxim]))\n",
    "test_processed = np.zeros(tuple(list(test_desc.shape)+[maxim]))\n",
    "\n",
    "for col,idcol in zip(train_desc.columns,range(15)):\n",
    "    for idx in range(len(train_desc)):\n",
    "        for i in range(maxim):\n",
    "            train_processed[idx][idcol][i] = train_desc[col][idx][i]\n",
    "            \n",
    "    for idx in range(len(val_desc)):\n",
    "        for i in range(maxim):\n",
    "            val_processed[idx][idcol][i] = val_desc[col][idx][i]\n",
    "            \n",
    "    for idx in range(len(test_desc)):\n",
    "        for i in range(maxim):\n",
    "            test_processed[idx][idcol][i] = test_desc[col][idx][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo mi modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/50\n",
      "6634/6634 [==============================] - 3s 392us/step - loss: 3239.3657 - mse: 3239.3650 - val_loss: 3339.1823 - val_mse: 3339.1821\n",
      "Epoch 2/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 2678.2213 - mse: 2678.2212 - val_loss: 3250.0647 - val_mse: 3250.0649\n",
      "Epoch 3/50\n",
      "6634/6634 [==============================] - 2s 248us/step - loss: 2465.1572 - mse: 2465.1570 - val_loss: 3089.2396 - val_mse: 3089.2395\n",
      "Epoch 4/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 2193.8099 - mse: 2193.8096 - val_loss: 2683.7648 - val_mse: 2683.7649\n",
      "Epoch 5/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 1950.5539 - mse: 1950.5541 - val_loss: 2743.2367 - val_mse: 2743.2371\n",
      "Epoch 6/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1635.2230 - mse: 1635.2233 - val_loss: 3206.7321 - val_mse: 3206.7324\n",
      "Epoch 7/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1565.0276 - mse: 1565.0273 - val_loss: 2753.9675 - val_mse: 2753.9675\n",
      "Epoch 8/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 1249.1812 - mse: 1249.1812 - val_loss: 3104.7678 - val_mse: 3104.7681\n",
      "Epoch 9/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 976.4531 - mse: 976.4531 - val_loss: 2897.2365 - val_mse: 2897.2363\n",
      "Epoch 10/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 889.7451 - mse: 889.7449 - val_loss: 2979.6614 - val_mse: 2979.6616\n",
      "Epoch 11/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 836.5018 - mse: 836.5018 - val_loss: 2776.1900 - val_mse: 2776.1899\n",
      "Epoch 12/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 737.9496 - mse: 737.9497 - val_loss: 3116.9118 - val_mse: 3116.9119\n",
      "Epoch 13/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 604.7742 - mse: 604.7741 - val_loss: 3033.4665 - val_mse: 3033.4666\n",
      "Epoch 14/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 535.4469 - mse: 535.4469 - val_loss: 2678.2593 - val_mse: 2678.2593\n",
      "Epoch 15/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 477.6314 - mse: 477.6314 - val_loss: 2860.9879 - val_mse: 2860.9878\n",
      "Epoch 16/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 471.3157 - mse: 471.3158 - val_loss: 2886.1922 - val_mse: 2886.1921\n",
      "Epoch 17/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 441.8938 - mse: 441.8938 - val_loss: 2850.8838 - val_mse: 2850.8838\n",
      "Epoch 18/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 395.4722 - mse: 395.4721 - val_loss: 2643.1650 - val_mse: 2643.1648\n",
      "Epoch 19/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 343.5419 - mse: 343.5418 - val_loss: 2692.1534 - val_mse: 2692.1536\n",
      "Epoch 20/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 311.5160 - mse: 311.5160 - val_loss: 2673.4630 - val_mse: 2673.4631\n",
      "Epoch 21/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 289.7304 - mse: 289.7305 - val_loss: 3114.1233 - val_mse: 3114.1233\n",
      "Epoch 22/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 279.0242 - mse: 279.0243 - val_loss: 2749.6203 - val_mse: 2749.6204\n",
      "Epoch 23/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 236.2036 - mse: 236.2036 - val_loss: 2727.7849 - val_mse: 2727.7852\n",
      "Epoch 24/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 221.7126 - mse: 221.7127 - val_loss: 2809.4851 - val_mse: 2809.4851\n",
      "Epoch 25/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 234.3478 - mse: 234.3478 - val_loss: 2724.5480 - val_mse: 2724.5479\n",
      "Epoch 26/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 232.9413 - mse: 232.9413 - val_loss: 2739.5829 - val_mse: 2739.5828\n",
      "Epoch 27/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 220.9285 - mse: 220.9285 - val_loss: 2834.3230 - val_mse: 2834.3230\n",
      "Epoch 28/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 209.2153 - mse: 209.2153 - val_loss: 2721.3295 - val_mse: 2721.3293\n",
      "Epoch 29/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 173.8247 - mse: 173.8247 - val_loss: 2744.4473 - val_mse: 2744.4473\n",
      "Epoch 30/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 159.0806 - mse: 159.0806 - val_loss: 2814.3545 - val_mse: 2814.3547\n",
      "Epoch 31/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 180.0751 - mse: 180.0751 - val_loss: 2627.2881 - val_mse: 2627.2878\n",
      "Epoch 32/50\n",
      "6634/6634 [==============================] - 2s 247us/step - loss: 209.9977 - mse: 209.9977 - val_loss: 2622.1549 - val_mse: 2622.1550\n",
      "Epoch 33/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 212.3203 - mse: 212.3203 - val_loss: 2738.6833 - val_mse: 2738.6833\n",
      "Epoch 34/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 184.2039 - mse: 184.2039 - val_loss: 2680.8295 - val_mse: 2680.8293\n",
      "Epoch 35/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 158.1694 - mse: 158.1694 - val_loss: 2826.0086 - val_mse: 2826.0085\n",
      "Epoch 36/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 171.3338 - mse: 171.3338 - val_loss: 2719.7932 - val_mse: 2719.7930\n",
      "Epoch 37/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 187.1673 - mse: 187.1673 - val_loss: 2776.5053 - val_mse: 2776.5054\n",
      "Epoch 38/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 177.6271 - mse: 177.6270 - val_loss: 2655.6936 - val_mse: 2655.6934\n",
      "Epoch 39/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 161.0461 - mse: 161.0461 - val_loss: 2718.5242 - val_mse: 2718.5242\n",
      "Epoch 40/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 139.5714 - mse: 139.5714 - val_loss: 2674.4897 - val_mse: 2674.4897\n",
      "Epoch 41/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 123.7270 - mse: 123.7270 - val_loss: 2678.6074 - val_mse: 2678.6077\n",
      "Epoch 42/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 117.9403 - mse: 117.9403 - val_loss: 2687.1777 - val_mse: 2687.1775\n",
      "Epoch 43/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 108.6888 - mse: 108.6889 - val_loss: 2704.8829 - val_mse: 2704.8828\n",
      "Epoch 44/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 119.7597 - mse: 119.7597 - val_loss: 2731.6145 - val_mse: 2731.6143\n",
      "Epoch 45/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 108.4823 - mse: 108.4823 - val_loss: 2626.5564 - val_mse: 2626.5566\n",
      "Epoch 46/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 111.9274 - mse: 111.9274 - val_loss: 2691.2486 - val_mse: 2691.2485\n",
      "Epoch 47/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 107.1478 - mse: 107.1478 - val_loss: 2622.9360 - val_mse: 2622.9360\n",
      "Epoch 48/50\n",
      "6634/6634 [==============================] - 2s 244us/step - loss: 96.3532 - mse: 96.3532 - val_loss: 2498.2187 - val_mse: 2498.2188\n",
      "Epoch 49/50\n",
      "6634/6634 [==============================] - 2s 246us/step - loss: 99.1940 - mse: 99.1940 - val_loss: 2556.0685 - val_mse: 2556.0684\n",
      "Epoch 50/50\n",
      "6634/6634 [==============================] - 2s 245us/step - loss: 106.2907 - mse: 106.2907 - val_loss: 2594.9132 - val_mse: 2594.9131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d7f7e54508>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(train_processed.shape[1:])))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(train_processed, Y_train, validation_data=(val_processed, Y_val), epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 2561.903957532806\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((model.predict(test_processed)-Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si puedo mejorar mis resultados sumando esta red a las dos que ya tenía en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/100\n",
      "6634/6634 [==============================] - 22s 3ms/step - loss: 59741.7977 - mse: 59741.8008 - val_loss: 12776.0953 - val_mse: 12776.0947\n",
      "Epoch 2/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 2448.1055 - mse: 2448.1057 - val_loss: 4044.2649 - val_mse: 4044.2651\n",
      "Epoch 3/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1970.3166 - mse: 1970.3164 - val_loss: 5357.4706 - val_mse: 5357.4707\n",
      "Epoch 4/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1668.5579 - mse: 1668.5582 - val_loss: 2854.8906 - val_mse: 2854.8909\n",
      "Epoch 5/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1330.2476 - mse: 1330.2476 - val_loss: 16403.2374 - val_mse: 16403.2363\n",
      "Epoch 6/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 1071.1219 - mse: 1071.1218 - val_loss: 8743.6171 - val_mse: 8743.6162\n",
      "Epoch 7/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 895.8557 - mse: 895.8559 - val_loss: 22075.7379 - val_mse: 22075.7383\n",
      "Epoch 8/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 832.0527 - mse: 832.0526 - val_loss: 25185.9669 - val_mse: 25185.9668\n",
      "Epoch 9/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 795.9063 - mse: 795.9062 - val_loss: 21111.7263 - val_mse: 21111.7285\n",
      "Epoch 10/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 640.1376 - mse: 640.1376 - val_loss: 259308.3574 - val_mse: 259308.3750\n",
      "Epoch 11/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 586.1049 - mse: 586.1050 - val_loss: 1817.5034 - val_mse: 1817.5034\n",
      "Epoch 12/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 438.6137 - mse: 438.6137 - val_loss: 4838.6635 - val_mse: 4838.6636\n",
      "Epoch 13/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 446.2274 - mse: 446.2274 - val_loss: 1747.1021 - val_mse: 1747.1022\n",
      "Epoch 14/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 400.9233 - mse: 400.9233 - val_loss: 1851.2614 - val_mse: 1851.2614\n",
      "Epoch 15/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 421.4195 - mse: 421.4195 - val_loss: 1712.0231 - val_mse: 1712.0232\n",
      "Epoch 16/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 369.3713 - mse: 369.3713 - val_loss: 1632.7914 - val_mse: 1632.7915\n",
      "Epoch 17/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 317.8261 - mse: 317.8260 - val_loss: 1652.3395 - val_mse: 1652.3395\n",
      "Epoch 18/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 280.2647 - mse: 280.2646 - val_loss: 1391.9294 - val_mse: 1391.9294\n",
      "Epoch 19/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 238.6586 - mse: 238.6586 - val_loss: 6846.8255 - val_mse: 6846.8252\n",
      "Epoch 20/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 212.5855 - mse: 212.5854 - val_loss: 1872.3926 - val_mse: 1872.3923\n",
      "Epoch 21/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 187.6119 - mse: 187.6119 - val_loss: 1707.5829 - val_mse: 1707.5830\n",
      "Epoch 22/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 182.5649 - mse: 182.5649 - val_loss: 1650.3900 - val_mse: 1650.3900\n",
      "Epoch 23/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 169.0611 - mse: 169.0611 - val_loss: 1569.9387 - val_mse: 1569.9387\n",
      "Epoch 24/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 132.0614 - mse: 132.0614 - val_loss: 1666.9921 - val_mse: 1666.9918\n",
      "Epoch 25/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 140.5581 - mse: 140.5581 - val_loss: 1612.1986 - val_mse: 1612.1985\n",
      "Epoch 26/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 135.7380 - mse: 135.7379 - val_loss: 1500.5673 - val_mse: 1500.5673\n",
      "Epoch 27/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 128.7470 - mse: 128.7469 - val_loss: 1537.5314 - val_mse: 1537.5314\n",
      "Epoch 28/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 126.7147 - mse: 126.7147 - val_loss: 1422.7333 - val_mse: 1422.7333\n",
      "Epoch 29/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 104.2881 - mse: 104.2881 - val_loss: 8498.9921 - val_mse: 8498.9932\n",
      "Epoch 30/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 103.8399 - mse: 103.8399 - val_loss: 12691.8741 - val_mse: 12691.8740\n",
      "Epoch 31/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 123.0183 - mse: 123.0182 - val_loss: 1710.5408 - val_mse: 1710.5408\n",
      "Epoch 32/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 119.5335 - mse: 119.5335 - val_loss: 1722.1254 - val_mse: 1722.1254\n",
      "Epoch 33/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 104.1963 - mse: 104.1963 - val_loss: 1662.6062 - val_mse: 1662.6063\n",
      "Epoch 34/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 97.4744 - mse: 97.4744 - val_loss: 2146.9936 - val_mse: 2146.9937\n",
      "Epoch 35/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 119.9382 - mse: 119.9383 - val_loss: 1843.4836 - val_mse: 1843.4835\n",
      "Epoch 36/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 114.5586 - mse: 114.5586 - val_loss: 1784.6677 - val_mse: 1784.6677\n",
      "Epoch 37/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 85.2055 - mse: 85.2055 - val_loss: 1868.8703 - val_mse: 1868.8705\n",
      "Epoch 38/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 85.7367 - mse: 85.7368 - val_loss: 1951.3090 - val_mse: 1951.3090\n",
      "Epoch 39/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 104.8487 - mse: 104.8486 - val_loss: 2030.9510 - val_mse: 2030.9510\n",
      "Epoch 40/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 91.2938 - mse: 91.2938 - val_loss: 1664.5265 - val_mse: 1664.5264\n",
      "Epoch 41/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 79.6665 - mse: 79.6665 - val_loss: 1912.4867 - val_mse: 1912.4866\n",
      "Epoch 42/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 74.8747 - mse: 74.8747 - val_loss: 1919.1527 - val_mse: 1919.1526\n",
      "Epoch 43/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 69.6946 - mse: 69.6946 - val_loss: 1514.5230 - val_mse: 1514.5231\n",
      "Epoch 44/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 63.4288 - mse: 63.4287 - val_loss: 1425.1469 - val_mse: 1425.1470\n",
      "Epoch 45/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 74.2798 - mse: 74.2798 - val_loss: 1467.1216 - val_mse: 1467.1216\n",
      "Epoch 46/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 77.1269 - mse: 77.1269 - val_loss: 1562.5681 - val_mse: 1562.5681\n",
      "Epoch 47/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 78.8479 - mse: 78.8479 - val_loss: 1595.4881 - val_mse: 1595.4882\n",
      "Epoch 48/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 90.2889 - mse: 90.2889 - val_loss: 1613.3821 - val_mse: 1613.3820\n",
      "Epoch 49/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 81.8657 - mse: 81.8657 - val_loss: 1544.2409 - val_mse: 1544.2410\n",
      "Epoch 50/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 82.7728 - mse: 82.7727 - val_loss: 1711.8358 - val_mse: 1711.8357\n",
      "Epoch 51/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 75.9830 - mse: 75.9830 - val_loss: 1625.1913 - val_mse: 1625.1914\n",
      "Epoch 52/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 74.9349 - mse: 74.9349 - val_loss: 1626.8075 - val_mse: 1626.8076\n",
      "Epoch 53/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 75.3363 - mse: 75.3362 - val_loss: 1510.9366 - val_mse: 1510.9366\n",
      "Epoch 54/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 82.9140 - mse: 82.9140 - val_loss: 1698.0621 - val_mse: 1698.0620\n",
      "Epoch 55/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 72.6147 - mse: 72.6147 - val_loss: 1568.2592 - val_mse: 1568.2593\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634/6634 [==============================] - 16s 2ms/step - loss: 75.8362 - mse: 75.8362 - val_loss: 1637.8717 - val_mse: 1637.8716\n",
      "Epoch 57/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 73.9209 - mse: 73.9209 - val_loss: 1550.2440 - val_mse: 1550.2440\n",
      "Epoch 58/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 67.2593 - mse: 67.2593 - val_loss: 1504.1632 - val_mse: 1504.1631\n",
      "Epoch 59/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 61.5748 - mse: 61.5748 - val_loss: 1662.5516 - val_mse: 1662.5515\n",
      "Epoch 60/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 59.6685 - mse: 59.6685 - val_loss: 1642.6914 - val_mse: 1642.6914\n",
      "Epoch 61/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 71.0123 - mse: 71.0123 - val_loss: 1570.9563 - val_mse: 1570.9563\n",
      "Epoch 62/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 59.9288 - mse: 59.9288 - val_loss: 1590.6197 - val_mse: 1590.6198\n",
      "Epoch 63/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 54.3511 - mse: 54.3511 - val_loss: 1549.7289 - val_mse: 1549.7289\n",
      "Epoch 64/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 62.7458 - mse: 62.7458 - val_loss: 1446.8036 - val_mse: 1446.8038\n",
      "Epoch 65/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 57.2981 - mse: 57.2981 - val_loss: 1354.6096 - val_mse: 1354.6096\n",
      "Epoch 66/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 60.8312 - mse: 60.8312 - val_loss: 1484.3014 - val_mse: 1484.3015\n",
      "Epoch 67/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 55.0988 - mse: 55.0988 - val_loss: 1556.6856 - val_mse: 1556.6857\n",
      "Epoch 68/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 61.7340 - mse: 61.7340 - val_loss: 1387.8206 - val_mse: 1387.8207\n",
      "Epoch 69/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 55.5660 - mse: 55.5660 - val_loss: 1429.4917 - val_mse: 1429.4917\n",
      "Epoch 70/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 50.0399 - mse: 50.0399 - val_loss: 1455.2007 - val_mse: 1455.2007\n",
      "Epoch 71/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 49.6187 - mse: 49.6187 - val_loss: 1699.4809 - val_mse: 1699.4808\n",
      "Epoch 72/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 56.3741 - mse: 56.3741 - val_loss: 1547.7385 - val_mse: 1547.7385\n",
      "Epoch 73/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 71.3527 - mse: 71.3527 - val_loss: 1475.0269 - val_mse: 1475.0267\n",
      "Epoch 74/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 76.1047 - mse: 76.1047 - val_loss: 1593.3523 - val_mse: 1593.3522\n",
      "Epoch 75/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 69.1205 - mse: 69.1206 - val_loss: 1527.6671 - val_mse: 1527.6670\n",
      "Epoch 76/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 66.1200 - mse: 66.1200 - val_loss: 1344.9072 - val_mse: 1344.9072\n",
      "Epoch 77/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 116.4804 - mse: 116.4804 - val_loss: 1399.7069 - val_mse: 1399.7069\n",
      "Epoch 78/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 88.7456 - mse: 88.7456 - val_loss: 1426.9715 - val_mse: 1426.9716\n",
      "Epoch 79/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 70.2726 - mse: 70.2726 - val_loss: 1345.2434 - val_mse: 1345.2434\n",
      "Epoch 80/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 69.5903 - mse: 69.5903 - val_loss: 1837.9222 - val_mse: 1837.9221\n",
      "Epoch 81/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 56.1950 - mse: 56.1950 - val_loss: 1356.7900 - val_mse: 1356.7899\n",
      "Epoch 82/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 44.7536 - mse: 44.7536 - val_loss: 1356.6547 - val_mse: 1356.6548\n",
      "Epoch 83/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 47.3328 - mse: 47.3328 - val_loss: 1366.2274 - val_mse: 1366.2274\n",
      "Epoch 84/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 42.3775 - mse: 42.3775 - val_loss: 1247.5757 - val_mse: 1247.5757\n",
      "Epoch 85/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 35.9048 - mse: 35.9048 - val_loss: 1273.8468 - val_mse: 1273.8469\n",
      "Epoch 86/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 33.3989 - mse: 33.3989 - val_loss: 2667.8335 - val_mse: 2667.8335\n",
      "Epoch 87/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 32.9540 - mse: 32.9541 - val_loss: 1409.3873 - val_mse: 1409.3873\n",
      "Epoch 88/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 36.3565 - mse: 36.3565 - val_loss: 1236.0110 - val_mse: 1236.0110\n",
      "Epoch 89/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 38.0463 - mse: 38.0463 - val_loss: 1568.8211 - val_mse: 1568.8212\n",
      "Epoch 90/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 37.8221 - mse: 37.8222 - val_loss: 1434.7129 - val_mse: 1434.7129\n",
      "Epoch 91/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 40.6446 - mse: 40.6446 - val_loss: 1805.5370 - val_mse: 1805.5369\n",
      "Epoch 92/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 241.2699 - mse: 241.2699 - val_loss: 1442.1060 - val_mse: 1442.1061\n",
      "Epoch 93/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 275.0198 - mse: 275.0198 - val_loss: 1408.2657 - val_mse: 1408.2656\n",
      "Epoch 94/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 282.4556 - mse: 282.4556 - val_loss: 1405.1603 - val_mse: 1405.1603\n",
      "Epoch 95/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 195.6338 - mse: 195.6339 - val_loss: 2450.5558 - val_mse: 2450.5557\n",
      "Epoch 96/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 132.0425 - mse: 132.0425 - val_loss: 1425.2948 - val_mse: 1425.2949\n",
      "Epoch 97/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 84.0970 - mse: 84.0970 - val_loss: 1333.6900 - val_mse: 1333.6898\n",
      "Epoch 98/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 64.6956 - mse: 64.6956 - val_loss: 1382.7785 - val_mse: 1382.7787\n",
      "Epoch 99/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 48.9664 - mse: 48.9664 - val_loss: 1309.9339 - val_mse: 1309.9338\n",
      "Epoch 100/100\n",
      "6634/6634 [==============================] - 16s 2ms/step - loss: 40.8807 - mse: 40.8807 - val_loss: 1255.6363 - val_mse: 1255.6362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d7fa65dec8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "imgs_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=512, mode='keep_first')\n",
    "\n",
    "atts_model = create_dense(X_train.shape[1], linear=False)\n",
    "\n",
    "desc_model = Sequential()\n",
    "desc_model.add(LSTM(512, input_shape=(train_processed.shape[1:])))\n",
    "desc_model.add(Dense(1024, activation='relu'))\n",
    "desc_model.add(Dense(512, activation='relu'))\n",
    "\n",
    "combined = concatenate([atts_model.output, imgs_model.output, desc_model.output])\n",
    "\n",
    "# Añado una salida que me haga la regresión.\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "z = Dense(8, activation='relu')(z)\n",
    "z = Dense(1, activation='linear')(z)\n",
    "\n",
    "combined_model = Model([atts_model.input, imgs_model.input, desc_model.input], z)\n",
    "\n",
    "combined_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "combined_model.fit([X_train, train_imgs_res, train_processed], Y_train,\n",
    "                   validation_data=([X_val, val_imgs_res, val_processed], Y_val),\n",
    "                   epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1277.1653229604826\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((combined_model.predict([X_test, test_imgs_res, test_processed]) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diría que mejora la capacidad de generalización respecto al modelo que solo utiliza imágenes y datos numéricos, viendo que los resultados en validación y test aquí son muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error medio es de unos: 35.74€\n"
     ]
    }
   ],
   "source": [
    "print(f'El error medio es de unos: {np.mean(((combined_model.predict([X_test, test_imgs_res, test_processed]) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))**0.5:.2f}€')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de los precios estimados por el modelo: 61.60\n",
      "Media real de los precios: 64.91973969631236\n"
     ]
    }
   ],
   "source": [
    "print(f'Media de los precios estimados por el modelo: {combined_model.predict([X_test, test_imgs_res, test_processed]).mean():.2f}',\n",
    "      f'Media real de los precios: {Y_test.mean()}', sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
