{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda y construcción del modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./train.csv',sep=';', decimal='.')\n",
    "val = pd.read_csv('./val.csv',sep=';', decimal='.')\n",
    "test = pd.read_csv('./test.csv',sep=';', decimal='.')\n",
    "\n",
    "images = np.load('images.npy')\n",
    "train_imgs = images[train['Unnamed: 0']]\n",
    "val_imgs = images[val['Unnamed: 0']]\n",
    "test_imgs = images[test['Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraigo la variable objetivo y dropeo los índices para empezar a trabajar con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train.Price\n",
    "Y_val = val.Price\n",
    "Y_test = test.Price\n",
    "\n",
    "X_train = train.drop(columns=['Unnamed: 0','Price'])\n",
    "X_val = val.drop(columns=['Unnamed: 0','Price'])\n",
    "X_test = test.drop(columns=['Unnamed: 0','Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, concatenate\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función me permite crear un modelo denso lineal o categórico en base al parámetro \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(dim, linear=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=dim, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    if linear:\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "    else:\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echo un primer vistazo al modelo con los hiperparámetros que mejor suelen funcionar a nivel general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/25\n",
      "6634/6634 [==============================] - 1s 135us/step - loss: 3513.7036 - mse: 3513.7034 - val_loss: 14282.8155 - val_mse: 14282.8154\n",
      "Epoch 2/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 3115.2304 - mse: 3115.2302 - val_loss: 127591.9870 - val_mse: 127592.0000\n",
      "Epoch 3/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 2522.5927 - mse: 2522.5928 - val_loss: 1973814.8015 - val_mse: 1973814.6250\n",
      "Epoch 4/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 3612.2261 - mse: 3612.2256 - val_loss: 1006874.5131 - val_mse: 1006874.5625\n",
      "Epoch 5/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 2267.5266 - mse: 2267.5264 - val_loss: 1641853.8349 - val_mse: 1641853.7500\n",
      "Epoch 6/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1887.2092 - mse: 1887.2092 - val_loss: 1711922.8013 - val_mse: 1711922.8750\n",
      "Epoch 7/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 2209.6883 - mse: 2209.6885 - val_loss: 1303902.1674 - val_mse: 1303902.2500\n",
      "Epoch 8/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 3043.8033 - mse: 3043.8035 - val_loss: 158723.5310 - val_mse: 158723.5156\n",
      "Epoch 9/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1848.1663 - mse: 1848.1663 - val_loss: 15758.7911 - val_mse: 15758.7900\n",
      "Epoch 10/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1581.0842 - mse: 1581.0845 - val_loss: 2015.0380 - val_mse: 2015.0380\n",
      "Epoch 11/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1471.0883 - mse: 1471.0885 - val_loss: 1643.7031 - val_mse: 1643.7031\n",
      "Epoch 12/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1445.0812 - mse: 1445.0809 - val_loss: 1571.5500 - val_mse: 1571.5499\n",
      "Epoch 13/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1430.0282 - mse: 1430.0282 - val_loss: 1578.4489 - val_mse: 1578.4489\n",
      "Epoch 14/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1403.7167 - mse: 1403.7166 - val_loss: 1568.9843 - val_mse: 1568.9843\n",
      "Epoch 15/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1373.4410 - mse: 1373.4408 - val_loss: 1563.5519 - val_mse: 1563.5520\n",
      "Epoch 16/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1361.7299 - mse: 1361.7297 - val_loss: 1560.6968 - val_mse: 1560.6968\n",
      "Epoch 17/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1362.9353 - mse: 1362.9354 - val_loss: 1555.5480 - val_mse: 1555.5480\n",
      "Epoch 18/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1330.8573 - mse: 1330.8573 - val_loss: 1497.1452 - val_mse: 1497.1451\n",
      "Epoch 19/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1323.2196 - mse: 1323.2200 - val_loss: 1533.4861 - val_mse: 1533.4861\n",
      "Epoch 20/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1301.3716 - mse: 1301.3715 - val_loss: 1636.6458 - val_mse: 1636.6460\n",
      "Epoch 21/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1290.4123 - mse: 1290.4122 - val_loss: 1509.5028 - val_mse: 1509.5029\n",
      "Epoch 22/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1282.9708 - mse: 1282.9709 - val_loss: 1494.4385 - val_mse: 1494.4387\n",
      "Epoch 23/25\n",
      "6634/6634 [==============================] - 0s 30us/step - loss: 1233.5272 - mse: 1233.5273 - val_loss: 1476.4442 - val_mse: 1476.4441\n",
      "Epoch 24/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1300.5899 - mse: 1300.5897 - val_loss: 1461.5340 - val_mse: 1461.5341\n",
      "Epoch 25/25\n",
      "6634/6634 [==============================] - 0s 29us/step - loss: 1254.2958 - mse: 1254.2960 - val_loss: 1532.4068 - val_mse: 1532.4069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ffc5ec56c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atts_model = create_dense(X_train.shape[1], linear=True)\n",
    "atts_model.compile(loss='mse', optimizer='adam' , metrics=['mse'])\n",
    "atts_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=25, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 1272.0077008123938\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((atts_model.predict(X_test) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es excesivamente malo para ser un modelo tan simple. Ahora voy a definir un modelo para trabajar con las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función construye un modelo sencillo que puede generar una salida lineal o categórica dependiendo del parámetro \"linear\", y también permite añadir una salida final partiendo de una arquiectura base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(shape, base=None, linear=False, density=128, mode='trainable'):\n",
    "    \n",
    "    if base:\n",
    "        \n",
    "        if mode == 'transfer_learning':\n",
    "            for layer in base.layers: \n",
    "                layer.trainable = False            \n",
    "        \n",
    "        model = base.layers[-1].output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense(density, activation='relu')(model)\n",
    "        model = Dropout(0.3)(model)\n",
    "        \n",
    "        if linear:\n",
    "            model = Dense(1, activation='linear')(model)\n",
    "        else:\n",
    "            model = Dense(12, activation='relu')(model)\n",
    "            \n",
    "        model = Model(base.input, model)\n",
    "    \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=shape))\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(density, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "        if linear:\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "        else:\n",
    "            model.add(Dense(12, activation='relu'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescalo las imágenes a la mitad de su resolución original por cuestiones de memoria RAM de mi GPU y después estandarizo los datos para que el modelo trabaje con números entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],shape[1]//2,shape[2]//2,shape[3]))\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se comporta esta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/3\n",
      "6634/6634 [==============================] - 56s 8ms/step - loss: 3533.8644 - mse: 3533.8643 - val_loss: 3603.5449 - val_mse: 3603.5444\n",
      "Epoch 2/3\n",
      "6634/6634 [==============================] - 54s 8ms/step - loss: 3284.9164 - mse: 3284.9158 - val_loss: 3610.8911 - val_mse: 3610.8914\n",
      "Epoch 3/3\n",
      "6634/6634 [==============================] - 54s 8ms/step - loss: 3271.0443 - mse: 3271.0449 - val_loss: 3732.9472 - val_mse: 3732.9465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ffc57e2488>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_model = create_cnn(train_imgs_res.shape[1:], linear=True)\n",
    "imgs_model.compile(loss='mse', optimizer='adam' , metrics=['mse'])\n",
    "imgs_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3182.0093433880124\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((imgs_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados horribles. Voy a probar con VGG16 cargando los pesos de imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for shape in (train_imgs.shape, val_imgs.shape, test_imgs.shape):\n",
    "    shapes.append((shape[0],48,48,3)) # Reescalo a 48, 48, 3 para poder usar VGG16\n",
    "    \n",
    "train_imgs_res = np.zeros(shapes[0], dtype=int)\n",
    "val_imgs_res = np.zeros(shapes[1], dtype=int)\n",
    "test_imgs_res = np.zeros(shapes[2], dtype=int)        \n",
    "\n",
    "for idx, img in zip(range(shapes[0][0]),train_imgs):\n",
    "    train_imgs_res[idx] = cv2.resize(img, shapes[0][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[1][0]),val_imgs):\n",
    "    val_imgs_res[idx] = cv2.resize(img, shapes[1][1:-1])\n",
    "\n",
    "for idx, img in zip(range(shapes[2][0]),test_imgs):\n",
    "    test_imgs_res[idx] = cv2.resize(img, shapes[2][1:-1])\n",
    "    \n",
    "train_imgs_res = train_imgs_res / 255\n",
    "val_imgs_res = val_imgs_res / 255\n",
    "test_imgs_res = test_imgs_res / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/3\n",
      "6634/6634 [==============================] - 7s 1ms/step - loss: 3564.1297 - mse: 3564.1296 - val_loss: 3646.3452 - val_mse: 3646.3452\n",
      "Epoch 2/3\n",
      "6634/6634 [==============================] - 5s 807us/step - loss: 3028.2040 - mse: 3028.2041 - val_loss: 3649.9255 - val_mse: 3649.9255\n",
      "Epoch 3/3\n",
      "6634/6634 [==============================] - 5s 805us/step - loss: 3003.4130 - mse: 3003.4131 - val_loss: 3626.9171 - val_mse: 3626.9172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ffcf2c5e08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "vgg16_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=True, density=2500, mode='transfer_learning')\n",
    "vgg16_model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "vgg16_model.fit(train_imgs_res, Y_train, validation_data=(val_imgs_res, Y_val), epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE en test: 3111.819489143141\n"
     ]
    }
   ],
   "source": [
    "print(f'MSE en test: {np.mean(((vgg16_model.predict(test_imgs_res) - Y_test.to_numpy().reshape(-1,1))**2).mean(axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí he definido dos funciones que permiten generar modelos lineales y categóricos de una forma sencilla. Ahora voy a trabajar más a fondo con estos dos modelos y a unificarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero voy a pasar la variable objetivo a categórica para trabajar con las categorías \"barato\", \"asequible\" y \"caro\", ya que seguramente serán más fáciles de predecir que un precio aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATAUlEQVR4nO3df6zd9X3f8eer5kfWJCqmNpln3NjpvK0gLcAswpJqI6EDQ9SZaI1ktDVOxuRmgynRqkmkkUaWDo1Ka5mipUxOsWqmDELzo3FTt9QlVFEX8eOSEYNxCTeGBdcI38aEBEVjg733x/nc9mDuj3Ov7zmGfJ4P6eh8z/v7+Z7v+3z95XW+9/s955CqQpLUhx871Q1IkibH0Jekjhj6ktQRQ1+SOmLoS1JHTjvVDSxkzZo1tXHjxlPdhiS9rjz00EN/UVVr55r3mg79jRs3MjU1darbkKTXlST/a755nt6RpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOvKa/kasfPRtv+P1T3YLm8NTN7z3VLWhCPNKXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVk0dBP8oYkDyT5ZpKDSf59q29Kcn+SJ5J8LskZrX5mezzd5m8ceq6PtfrjSa4Y14uSJM1tlCP9F4H3VNXbgQuArUkuAX4NuKWqNgPPAde28dcCz1XV3wRuaeNIch6wHTgf2Ar8ZpJVK/liJEkLWzT0a+CF9vD0divgPcDnW30PcHWb3tYe0+ZfliStfmdVvVhVTwLTwMUr8iokSSMZ6Zx+klVJHgaOAfuBbwPfq6qX2pAjwPo2vR54GqDNfx74yeH6HMsMr2tnkqkkUzMzM0t/RZKkeY0U+lX1clVdAJzL4Oj8Z+Ya1u4zz7z56ieua1dVbamqLWvXrh2lPUnSiJb06Z2q+h7wJ8AlwFlJTmuzzgWOtukjwAaANv8ngOPD9TmWkSRNwCif3lmb5Kw2/deAnwMOAfcCv9CG7QC+3Kb3tse0+V+tqmr17e3TPZuAzcADK/VCJEmLO23xIawD9rRP2vwYcFdVfSXJY8CdSf4D8D+B29r424D/lmSawRH+doCqOpjkLuAx4CXguqp6eWVfjiRpIYuGflUdAC6co36YOT59U1X/G3j/PM91E3DT0tuUJK0Ev5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6smjoJ9mQ5N4kh5IcTPKRVv9Ekj9P8nC7XTW0zMeSTCd5PMkVQ/WtrTad5IbxvCRJ0nxOG2HMS8AvV9U3krwZeCjJ/jbvlqr6T8ODk5wHbAfOB/4G8MdJ/lab/WngHwFHgAeT7K2qx1bihUiSFrdo6FfVM8AzbfoHSQ4B6xdYZBtwZ1W9CDyZZBq4uM2brqrDAEnubGMNfUmakCWd00+yEbgQuL+Vrk9yIMnuJKtbbT3w9NBiR1ptvvqJ69iZZCrJ1MzMzFLakyQtYuTQT/Im4AvAR6vq+8CtwE8DFzD4S+DXZ4fOsXgtUH9loWpXVW2pqi1r164dtT1J0ghGOadPktMZBP5nq+qLAFX17ND8zwBfaQ+PABuGFj8XONqm56tLkiZglE/vBLgNOFRVvzFUXzc07H3Ao216L7A9yZlJNgGbgQeAB4HNSTYlOYPBxd69K/MyJEmjGOVI/13ALwKPJHm41X4FuCbJBQxO0TwF/BJAVR1McheDC7QvAddV1csASa4H7gZWAbur6uAKvhZJ0iJG+fTOnzL3+fh9CyxzE3DTHPV9Cy0nSRovv5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6smjoJ9mQ5N4kh5IcTPKRVj87yf4kT7T71a2eJJ9KMp3kQJKLhp5rRxv/RJId43tZkqS5jHKk/xLwy1X1M8AlwHVJzgNuAO6pqs3APe0xwJXA5nbbCdwKgzcJ4EbgHcDFwI2zbxSSpMlYNPSr6pmq+kab/gFwCFgPbAP2tGF7gKvb9Dbg9hq4DzgryTrgCmB/VR2vqueA/cDWFX01kqQFLemcfpKNwIXA/cBbquoZGLwxAOe0YeuBp4cWO9Jq89VPXMfOJFNJpmZmZpbSniRpESOHfpI3AV8APlpV319o6By1WqD+ykLVrqraUlVb1q5dO2p7kqQRjBT6SU5nEPifraovtvKz7bQN7f5Yqx8BNgwtfi5wdIG6JGlCRvn0ToDbgENV9RtDs/YCs5/A2QF8eaj+gfYpnkuA59vpn7uBy5OsbhdwL281SdKEnDbCmHcBvwg8kuThVvsV4GbgriTXAt8B3t/m7QOuAqaBHwIfAqiq40l+FXiwjftkVR1fkVchSRrJoqFfVX/K3OfjAS6bY3wB183zXLuB3UtpUJK0cvxGriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JFRfnDtdWvjDb9/qluQpNcUj/QlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjP9LfyJU0Gr+9/trz1M3vHcvzLnqkn2R3kmNJHh2qfSLJnyd5uN2uGpr3sSTTSR5PcsVQfWurTSe5YeVfiiRpMaOc3vltYOsc9Vuq6oJ22weQ5DxgO3B+W+Y3k6xKsgr4NHAlcB5wTRsrSZqgRU/vVNXXkmwc8fm2AXdW1YvAk0mmgYvbvOmqOgyQ5M429rEldyxJWraTuZB7fZID7fTP6lZbDzw9NOZIq81XlyRN0HJD/1bgp4ELgGeAX2/1zDG2Fqi/SpKdSaaSTM3MzCyzPUnSXJYV+lX1bFW9XFX/D/gMf3UK5wiwYWjoucDRBepzPfeuqtpSVVvWrl27nPYkSfNYVugnWTf08H3A7Cd79gLbk5yZZBOwGXgAeBDYnGRTkjMYXOzdu/y2JUnLseiF3CR3AJcCa5IcAW4ELk1yAYNTNE8BvwRQVQeT3MXgAu1LwHVV9XJ7nuuBu4FVwO6qOrjir0aStKBRPr1zzRzl2xYYfxNw0xz1fcC+JXUnSVpR/gyDJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyKKhn2R3kmNJHh2qnZ1kf5In2v3qVk+STyWZTnIgyUVDy+xo459IsmM8L0eStJBRjvR/G9h6Qu0G4J6q2gzc0x4DXAlsbredwK0weJMAbgTeAVwM3Dj7RiFJmpxFQ7+qvgYcP6G8DdjTpvcAVw/Vb6+B+4CzkqwDrgD2V9XxqnoO2M+r30gkSWO23HP6b6mqZwDa/Tmtvh54emjckVabry5JmqCVvpCbOWq1QP3VT5DsTDKVZGpmZmZFm5Ok3i039J9tp21o98da/QiwYWjcucDRBeqvUlW7qmpLVW1Zu3btMtuTJM1luaG/F5j9BM4O4MtD9Q+0T/FcAjzfTv/cDVyeZHW7gHt5q0mSJui0xQYkuQO4FFiT5AiDT+HcDNyV5FrgO8D72/B9wFXANPBD4EMAVXU8ya8CD7Zxn6yqEy8OS5LGbNHQr6pr5pl12RxjC7hunufZDexeUneSpBXlN3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6shJhX6Sp5I8kuThJFOtdnaS/UmeaPerWz1JPpVkOsmBJBetxAuQJI1uJY70311VF1TVlvb4BuCeqtoM3NMeA1wJbG63ncCtK7BuSdISjOP0zjZgT5veA1w9VL+9Bu4DzkqybgzrlyTN42RDv4A/SvJQkp2t9paqegag3Z/T6uuBp4eWPdJqr5BkZ5KpJFMzMzMn2Z4kadhpJ7n8u6rqaJJzgP1J/myBsZmjVq8qVO0CdgFs2bLlVfMlSct3Ukf6VXW03R8DvgRcDDw7e9qm3R9rw48AG4YWPxc4ejLrlyQtzbJDP8kbk7x5dhq4HHgU2AvsaMN2AF9u03uBD7RP8VwCPD97GkiSNBknc3rnLcCXksw+z3+vqj9M8iBwV5Jrge8A72/j9wFXAdPAD4EPncS6JUnLsOzQr6rDwNvnqH8XuGyOegHXLXd9kqST5zdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRyYe+km2Jnk8yXSSGya9fknq2URDP8kq4NPAlcB5wDVJzptkD5LUs0kf6V8MTFfV4ar6P8CdwLYJ9yBJ3TptwutbDzw99PgI8I7hAUl2AjvbwxeSPH4S61sD/MVJLD8u9rU09rU09rU0r8m+8msn1ddb55sx6dDPHLV6xYOqXcCuFVlZMlVVW1biuVaSfS2NfS2NfS1Nb31N+vTOEWDD0ONzgaMT7kGSujXp0H8Q2JxkU5IzgO3A3gn3IEndmujpnap6Kcn1wN3AKmB3VR0c4ypX5DTRGNjX0tjX0tjX0nTVV6pq8VGSpB8JfiNXkjpi6EtSR16Xob/YTzkkOTPJ59r8+5NsHJr3sVZ/PMkVE+7r3yR5LMmBJPckeevQvJeTPNxuK3pxe4S+PphkZmj9/2Jo3o4kT7Tbjgn3dctQT99K8r2heePcXruTHEvy6Dzzk+RTre8DSS4amjfO7bVYX/+09XMgydeTvH1o3lNJHmnba2rCfV2a5Pmhf69/NzRvbD/LMkJf/3aop0fbPnV2mzfO7bUhyb1JDiU5mOQjc4wZ3z5WVa+rG4MLwN8G3gacAXwTOO+EMf8K+K9tejvwuTZ9Xht/JrCpPc+qCfb1buDH2/S/nO2rPX7hFG6vDwL/ZY5lzwYOt/vVbXr1pPo6Yfy/ZnDhf6zbqz33PwAuAh6dZ/5VwB8w+N7JJcD9495eI/b1ztn1Mfipk/uH5j0FrDlF2+tS4Csnuw+sdF8njP154KsT2l7rgIva9JuBb83x3+TY9rHX45H+KD/lsA3Y06Y/D1yWJK1+Z1W9WFVPAtPt+SbSV1XdW1U/bA/vY/A9hXE7mZ++uALYX1XHq+o5YD+w9RT1dQ1wxwqte0FV9TXg+AJDtgG318B9wFlJ1jHe7bVoX1X19bZemNz+Ncr2ms9Yf5ZliX1Ncv96pqq+0aZ/ABxi8GsFw8a2j70eQ3+un3I4cYP95Ziqegl4HvjJEZcdZ1/DrmXwTj7rDUmmktyX5OoV6mkpff2T9mfk55PMfoHuNbG92mmwTcBXh8rj2l6jmK/3cW6vpTpx/yrgj5I8lMFPnUza30/yzSR/kOT8VntNbK8kP84gOL8wVJ7I9srg1POFwP0nzBrbPjbpn2FYCYv+lMMCY0ZZdrlGfu4k/wzYAvzDofJPVdXRJG8Dvprkkar69oT6+j3gjqp6McmHGfyV9J4Rlx1nX7O2A5+vqpeHauPaXqM4FfvXyJK8m0Ho/+xQ+V1te50D7E/yZ+1IeBK+Aby1ql5IchXwu8BmXiPbi8Gpnf9RVcN/FYx9eyV5E4M3mo9W1fdPnD3HIiuyj70ej/RH+SmHvxyT5DTgJxj8mTfOn4EY6bmT/BzwceAfV9WLs/WqOtruDwN/wuDdfyJ9VdV3h3r5DPD3Rl12nH0N2c4Jf3qPcXuNYr7eT/nPjCT5u8BvAduq6ruz9aHtdQz4Eit3WnNRVfX9qnqhTe8DTk+yhtfA9moW2r/Gsr2SnM4g8D9bVV+cY8j49rFxXKgY543BXyeHGfy5P3vx5/wTxlzHKy/k3tWmz+eVF3IPs3IXckfp60IGF642n1BfDZzZptcAT7BCF7RG7Gvd0PT7gPvqry4aPdn6W92mz55UX23c32ZwUS2T2F5D69jI/Bcm38srL7I9MO7tNWJfP8XgOtU7T6i/EXjz0PTXga0T7Ouvz/77MQjP77RtN9I+MK6+2vzZA8I3Tmp7tdd+O/CfFxgztn1sxTbuJG8Mrmx/i0GAfrzVPsng6BngDcDvtP8AHgDeNrTsx9tyjwNXTrivPwaeBR5ut72t/k7gkbbTPwJcO+G+/iNwsK3/XuDvDC37z9t2nAY+NMm+2uNPADefsNy4t9cdwDPA/2VwZHUt8GHgw21+GPzPgL7d1r9lQttrsb5+C3huaP+aavW3tW31zfbv/PEJ93X90P51H0NvSnPtA5Pqq435IIMPdwwvN+7t9bMMTskcGPq3umpS+5g/wyBJHXk9ntOXJC2ToS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I68v8Baf8hNU+MIjUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_train_cat = []\n",
    "for y in Y_train:\n",
    "    if y < 35: Y_train_cat.append(0)\n",
    "    elif y < 85: Y_train_cat.append(1)\n",
    "    else: Y_train_cat.append(2)\n",
    "        \n",
    "plt.hist(Y_train_cat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_cat = []\n",
    "for y in Y_val:\n",
    "    if y < 35: Y_val_cat.append(0)\n",
    "    elif y < 85: Y_val_cat.append(1)\n",
    "    else: Y_val_cat.append(2)\n",
    "        \n",
    "Y_test_cat = []\n",
    "for y in Y_test:\n",
    "    if y < 35: Y_test_cat.append(0)\n",
    "    elif y < 85: Y_test_cat.append(1)\n",
    "    else: Y_test_cat.append(2)\n",
    "        \n",
    "Y_train_cat = to_categorical(Y_train_cat)\n",
    "Y_val_cat = to_categorical(Y_val_cat)\n",
    "Y_test_cat = to_categorical(Y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un buen reparto de los valores en categorías, veamos como se comportan nuestros modelos si los unificamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts_model = create_dense(X_train.shape[1], linear=False)\n",
    "\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=train_imgs_res.shape[1:])\n",
    "vgg16_model = create_cnn(train_imgs_res.shape[1:], base=vgg16_model, linear=False, density=1028, mode='transfer_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = concatenate([atts_model.output, vgg16_model.output])\n",
    "z = Dense(3, activation='softmax')(combined)\n",
    "combined = Model([atts_model.input, vgg16_model.input], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6634 samples, validate on 738 samples\n",
      "Epoch 1/40\n",
      "6634/6634 [==============================] - 10s 1ms/step - loss: 4.4707 - accuracy: 0.3862 - val_loss: 4.7953 - val_accuracy: 0.4973\n",
      "Epoch 2/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 1.4803 - accuracy: 0.4644 - val_loss: 2.9479 - val_accuracy: 0.5203\n",
      "Epoch 3/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 1.0741 - accuracy: 0.5051 - val_loss: 2.8437 - val_accuracy: 0.5149\n",
      "Epoch 4/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.9809 - accuracy: 0.5378 - val_loss: 7.5908 - val_accuracy: 0.5136\n",
      "Epoch 5/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 1.0395 - accuracy: 0.5419 - val_loss: 2.2263 - val_accuracy: 0.5366\n",
      "Epoch 6/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.9613 - accuracy: 0.5549 - val_loss: 2.1178 - val_accuracy: 0.5488\n",
      "Epoch 7/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.8957 - accuracy: 0.5809 - val_loss: 5.0717 - val_accuracy: 0.5583\n",
      "Epoch 8/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.8841 - accuracy: 0.5939 - val_loss: 2.4364 - val_accuracy: 0.5678\n",
      "Epoch 9/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.8860 - accuracy: 0.6033 - val_loss: 2.5043 - val_accuracy: 0.5610\n",
      "Epoch 10/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.8480 - accuracy: 0.6121 - val_loss: 3.2472 - val_accuracy: 0.5488\n",
      "Epoch 11/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.8231 - accuracy: 0.6140 - val_loss: 2.9013 - val_accuracy: 0.5569\n",
      "Epoch 12/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.8084 - accuracy: 0.6251 - val_loss: 1.9864 - val_accuracy: 0.5854\n",
      "Epoch 13/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.7915 - accuracy: 0.6311 - val_loss: 1.9534 - val_accuracy: 0.5881\n",
      "Epoch 14/40\n",
      "6634/6634 [==============================] - 4s 630us/step - loss: 0.8228 - accuracy: 0.6317 - val_loss: 3.4166 - val_accuracy: 0.5637\n",
      "Epoch 15/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.8480 - accuracy: 0.6296 - val_loss: 1.7177 - val_accuracy: 0.5772\n",
      "Epoch 16/40\n",
      "6634/6634 [==============================] - 4s 633us/step - loss: 0.7775 - accuracy: 0.6485 - val_loss: 1.6806 - val_accuracy: 0.5745\n",
      "Epoch 17/40\n",
      "6634/6634 [==============================] - 4s 640us/step - loss: 0.7881 - accuracy: 0.6545 - val_loss: 2.2573 - val_accuracy: 0.5745\n",
      "Epoch 18/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.7852 - accuracy: 0.6532 - val_loss: 2.6537 - val_accuracy: 0.5664\n",
      "Epoch 19/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.7400 - accuracy: 0.6670 - val_loss: 2.5157 - val_accuracy: 0.5718\n",
      "Epoch 20/40\n",
      "6634/6634 [==============================] - 4s 635us/step - loss: 0.7491 - accuracy: 0.6752 - val_loss: 2.0233 - val_accuracy: 0.5894\n",
      "Epoch 21/40\n",
      "6634/6634 [==============================] - 4s 637us/step - loss: 0.7668 - accuracy: 0.6723 - val_loss: 4.5263 - val_accuracy: 0.5840\n",
      "Epoch 22/40\n",
      "6634/6634 [==============================] - 4s 648us/step - loss: 0.7087 - accuracy: 0.6925 - val_loss: 4.5236 - val_accuracy: 0.5935\n",
      "Epoch 23/40\n",
      "6634/6634 [==============================] - 4s 642us/step - loss: 0.6828 - accuracy: 0.6975 - val_loss: 3.6138 - val_accuracy: 0.5921\n",
      "Epoch 24/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.6528 - accuracy: 0.7163 - val_loss: 2.9230 - val_accuracy: 0.6152\n",
      "Epoch 25/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.6250 - accuracy: 0.7326 - val_loss: 1.9149 - val_accuracy: 0.6070\n",
      "Epoch 26/40\n",
      "6634/6634 [==============================] - 4s 632us/step - loss: 0.6135 - accuracy: 0.7425 - val_loss: 1.7983 - val_accuracy: 0.6070\n",
      "Epoch 27/40\n",
      "6634/6634 [==============================] - 4s 644us/step - loss: 0.6208 - accuracy: 0.7327 - val_loss: 2.1016 - val_accuracy: 0.5908\n",
      "Epoch 28/40\n",
      "6634/6634 [==============================] - 4s 634us/step - loss: 0.6221 - accuracy: 0.7547 - val_loss: 5.2550 - val_accuracy: 0.5813\n",
      "Epoch 29/40\n",
      "6634/6634 [==============================] - 4s 633us/step - loss: 0.6305 - accuracy: 0.7522 - val_loss: 2.9083 - val_accuracy: 0.6220\n",
      "Epoch 30/40\n",
      "6634/6634 [==============================] - 4s 634us/step - loss: 0.5801 - accuracy: 0.7534 - val_loss: 2.9219 - val_accuracy: 0.6003\n",
      "Epoch 31/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.5414 - accuracy: 0.7798 - val_loss: 2.4026 - val_accuracy: 0.6125\n",
      "Epoch 32/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.5373 - accuracy: 0.7841 - val_loss: 2.0542 - val_accuracy: 0.6220\n",
      "Epoch 33/40\n",
      "6634/6634 [==============================] - 4s 643us/step - loss: 0.5041 - accuracy: 0.8057 - val_loss: 1.8761 - val_accuracy: 0.6220\n",
      "Epoch 34/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.5149 - accuracy: 0.7875 - val_loss: 1.8028 - val_accuracy: 0.5718\n",
      "Epoch 35/40\n",
      "6634/6634 [==============================] - 4s 648us/step - loss: 0.4961 - accuracy: 0.8010 - val_loss: 1.4772 - val_accuracy: 0.6382\n",
      "Epoch 36/40\n",
      "6634/6634 [==============================] - 4s 637us/step - loss: 0.4760 - accuracy: 0.8090 - val_loss: 1.5802 - val_accuracy: 0.6165\n",
      "Epoch 37/40\n",
      "6634/6634 [==============================] - 4s 628us/step - loss: 0.4528 - accuracy: 0.8235 - val_loss: 1.6536 - val_accuracy: 0.6247\n",
      "Epoch 38/40\n",
      "6634/6634 [==============================] - 4s 629us/step - loss: 0.4352 - accuracy: 0.8384 - val_loss: 1.3653 - val_accuracy: 0.6287\n",
      "Epoch 39/40\n",
      "6634/6634 [==============================] - 4s 641us/step - loss: 0.4263 - accuracy: 0.8426 - val_loss: 1.6549 - val_accuracy: 0.6043\n",
      "Epoch 40/40\n",
      "6634/6634 [==============================] - 4s 645us/step - loss: 0.4074 - accuracy: 0.8496 - val_loss: 1.6156 - val_accuracy: 0.6247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ffff5b5a48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "combined.fit([X_train, train_imgs_res], Y_train_cat, validation_data=([X_val, val_imgs_res], Y_val_cat),\n",
    "             epochs=40, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.6274403470715835\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy in test: {sum(np.array([np.argmax(comb) for comb in combined.predict([X_test,test_imgs_res])])==np.array([np.argmax(real) for real in Y_test_cat]))/len(Y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
