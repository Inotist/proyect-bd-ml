{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica del módulo NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargo todos los tweetts y los concateno en un único dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783396985093193728</td>\n",
       "      <td>/missyscheng/status/783396985093193728</td>\n",
       "      <td>False</td>\n",
       "      <td>#DataScience Basics: #DataMining vs. #Statisti...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783381842024103936</td>\n",
       "      <td>/EXASOLAG/status/783381842024103936</td>\n",
       "      <td>False</td>\n",
       "      <td>How to Become a #Data Scientist – Part 1: http...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783433625723252736</td>\n",
       "      <td>/TarasNovak/status/783433625723252736</td>\n",
       "      <td>False</td>\n",
       "      <td>@jesterxl @kdnuggets or just go with @tableau :)</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783428740453982208</td>\n",
       "      <td>/kdnuggets/status/783428740453982208</td>\n",
       "      <td>False</td>\n",
       "      <td>#Boston U. Online MS in Applied #Business #Ana...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1h1 hour ago</td>\n",
       "      <td>787052623291641856</td>\n",
       "      <td>/kdnuggets/status/787052623291641856</td>\n",
       "      <td>False</td>\n",
       "      <td>#ICYMI Still Searching for ROI in #BigData Ana...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          date                  id  \\\n",
       "0           0         Oct 4  783396985093193728   \n",
       "1           1         Oct 4  783381842024103936   \n",
       "2           2         Oct 4  783433625723252736   \n",
       "3           3         Oct 4  783428740453982208   \n",
       "4           4  1h1 hour ago  787052623291641856   \n",
       "\n",
       "                                     link  retweet  \\\n",
       "0  /missyscheng/status/783396985093193728    False   \n",
       "1     /EXASOLAG/status/783381842024103936    False   \n",
       "2   /TarasNovak/status/783433625723252736    False   \n",
       "3    /kdnuggets/status/783428740453982208    False   \n",
       "4    /kdnuggets/status/787052623291641856    False   \n",
       "\n",
       "                                                text   author  \n",
       "0  #DataScience Basics: #DataMining vs. #Statisti...  various  \n",
       "1  How to Become a #Data Scientist – Part 1: http...  various  \n",
       "2   @jesterxl @kdnuggets or just go with @tableau :)  various  \n",
       "3  #Boston U. Online MS in Applied #Business #Ana...  various  \n",
       "4  #ICYMI Still Searching for ROI in #BigData Ana...  various  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob('twitter_corpus/*.csv')\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "corpus = pd.concat(li, axis=0, ignore_index=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generación de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a convertir todo el texto de los tweets en un único documento plano de texto. Para ello hago una limpieza permitiendo únicamente caracteres alfanuméricos, signos de puntuación y espacios y luego los unifico con un salto de línea por cada tweet diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = \"\\n\".join(re.sub('[^a-zA-Z0-9!?\\',.:\" ]+', '', tweet) for tweet in corpus.sample(25000).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero un diccionario de caracteres y dos conversores que transformen respectivamente cada caracter en su número correspondiente y a la inversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(full_corpus)))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero los chunks de texto que se van a utilizar para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples: 2810923\n"
     ]
    }
   ],
   "source": [
    "# El modelo se fijará en tramos de 40 caracteres para predecir el que sigue\n",
    "seq_length = 40\n",
    "\n",
    "# Los chunks se generan caracter a caracter para tener un mayor número de ejemplos y no perder la linealidad de las palabras.\n",
    "step = 1\n",
    "\n",
    "# Lista para guardar los chunks\n",
    "sentences = []\n",
    "\n",
    "# Lista para guardar los caracteres que el modelo debe tomar como predicciones correctas para cada chunk.\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(full_corpus)-seq_length, step):\n",
    "    sentences.append(full_corpus[i:i+seq_length])\n",
    "    next_chars.append(full_corpus[i+seq_length])\n",
    "    \n",
    "print('num training examples: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero un tensor de chunks con cada carácter representado en un vector one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot-encoded vectors\n",
    "X = np.zeros((len(sentences), seq_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo el modelo y lo entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2670376 samples, validate on 140547 samples\n",
      "Epoch 1/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 2.4838 - accuracy: 0.3385 - val_loss: 2.0790 - val_accuracy: 0.4295\n",
      "Epoch 2/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.8554 - accuracy: 0.4902 - val_loss: 1.8624 - val_accuracy: 0.4872\n",
      "Epoch 3/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.7311 - accuracy: 0.5225 - val_loss: 1.8156 - val_accuracy: 0.4947\n",
      "Epoch 4/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.6744 - accuracy: 0.5372 - val_loss: 1.7459 - val_accuracy: 0.5166\n",
      "Epoch 5/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.6405 - accuracy: 0.5458 - val_loss: 1.7344 - val_accuracy: 0.5142\n",
      "Epoch 6/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.6172 - accuracy: 0.5517 - val_loss: 1.7022 - val_accuracy: 0.5286\n",
      "Epoch 7/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.6001 - accuracy: 0.5560 - val_loss: 1.6873 - val_accuracy: 0.5321\n",
      "Epoch 8/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.5872 - accuracy: 0.5592 - val_loss: 1.6875 - val_accuracy: 0.5326\n",
      "Epoch 9/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.5766 - accuracy: 0.5618 - val_loss: 1.6682 - val_accuracy: 0.5378\n",
      "Epoch 10/100\n",
      "2670376/2670376 [==============================] - 150s 56us/step - loss: 1.5683 - accuracy: 0.5639 - val_loss: 1.6562 - val_accuracy: 0.5406\n",
      "Epoch 11/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5612 - accuracy: 0.5658 - val_loss: 1.6712 - val_accuracy: 0.5373\n",
      "Epoch 12/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5553 - accuracy: 0.5672 - val_loss: 1.6618 - val_accuracy: 0.5407\n",
      "Epoch 13/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5498 - accuracy: 0.5686 - val_loss: 1.6537 - val_accuracy: 0.5409\n",
      "Epoch 14/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5454 - accuracy: 0.5697 - val_loss: 1.6526 - val_accuracy: 0.5437\n",
      "Epoch 15/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5414 - accuracy: 0.5705 - val_loss: 1.6604 - val_accuracy: 0.5403\n",
      "Epoch 16/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5382 - accuracy: 0.5714 - val_loss: 1.6413 - val_accuracy: 0.5437\n",
      "Epoch 17/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5346 - accuracy: 0.5722 - val_loss: 1.6320 - val_accuracy: 0.5496\n",
      "Epoch 18/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5318 - accuracy: 0.5731 - val_loss: 1.6444 - val_accuracy: 0.5471\n",
      "Epoch 19/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5295 - accuracy: 0.5734 - val_loss: 1.6342 - val_accuracy: 0.5484\n",
      "Epoch 20/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5270 - accuracy: 0.5742 - val_loss: 1.6356 - val_accuracy: 0.5443\n",
      "Epoch 21/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5245 - accuracy: 0.5747 - val_loss: 1.6395 - val_accuracy: 0.5475\n",
      "Epoch 22/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5226 - accuracy: 0.5751 - val_loss: 1.6284 - val_accuracy: 0.5508\n",
      "Epoch 23/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5204 - accuracy: 0.5759 - val_loss: 1.6277 - val_accuracy: 0.5484\n",
      "Epoch 24/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5189 - accuracy: 0.5760 - val_loss: 1.6239 - val_accuracy: 0.5508\n",
      "Epoch 25/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5172 - accuracy: 0.5762 - val_loss: 1.6241 - val_accuracy: 0.5493\n",
      "Epoch 26/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5157 - accuracy: 0.5767 - val_loss: 1.6205 - val_accuracy: 0.5519\n",
      "Epoch 27/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5148 - accuracy: 0.5769 - val_loss: 1.6278 - val_accuracy: 0.5495\n",
      "Epoch 28/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5129 - accuracy: 0.5774 - val_loss: 1.6149 - val_accuracy: 0.5525\n",
      "Epoch 29/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5116 - accuracy: 0.5775 - val_loss: 1.6264 - val_accuracy: 0.5505\n",
      "Epoch 30/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5103 - accuracy: 0.5781 - val_loss: 1.6125 - val_accuracy: 0.5517\n",
      "Epoch 31/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5094 - accuracy: 0.5782 - val_loss: 1.6187 - val_accuracy: 0.5520\n",
      "Epoch 32/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5084 - accuracy: 0.5785 - val_loss: 1.6171 - val_accuracy: 0.5505\n",
      "Epoch 33/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5075 - accuracy: 0.5788 - val_loss: 1.6170 - val_accuracy: 0.5541\n",
      "Epoch 34/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5071 - accuracy: 0.5787 - val_loss: 1.6099 - val_accuracy: 0.5522\n",
      "Epoch 35/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5058 - accuracy: 0.5792 - val_loss: 1.6149 - val_accuracy: 0.5525\n",
      "Epoch 36/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5050 - accuracy: 0.5794 - val_loss: 1.6111 - val_accuracy: 0.5541\n",
      "Epoch 37/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5041 - accuracy: 0.5797 - val_loss: 1.6112 - val_accuracy: 0.5533\n",
      "Epoch 38/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5030 - accuracy: 0.5799 - val_loss: 1.6141 - val_accuracy: 0.5522\n",
      "Epoch 39/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5020 - accuracy: 0.5799 - val_loss: 1.6159 - val_accuracy: 0.5546\n",
      "Epoch 40/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5012 - accuracy: 0.5802 - val_loss: 1.6226 - val_accuracy: 0.5522\n",
      "Epoch 41/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5005 - accuracy: 0.5804 - val_loss: 1.6111 - val_accuracy: 0.5539\n",
      "Epoch 42/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5001 - accuracy: 0.5806 - val_loss: 1.6070 - val_accuracy: 0.5538\n",
      "Epoch 43/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4991 - accuracy: 0.5809 - val_loss: 1.6121 - val_accuracy: 0.5541\n",
      "Epoch 44/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4986 - accuracy: 0.5809 - val_loss: 1.6084 - val_accuracy: 0.5573\n",
      "Epoch 45/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4982 - accuracy: 0.5810 - val_loss: 1.6168 - val_accuracy: 0.5491\n",
      "Epoch 46/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4975 - accuracy: 0.5813 - val_loss: 1.6065 - val_accuracy: 0.5552\n",
      "Epoch 47/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4966 - accuracy: 0.5812 - val_loss: 1.6029 - val_accuracy: 0.5567\n",
      "Epoch 48/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4961 - accuracy: 0.5815 - val_loss: 1.6080 - val_accuracy: 0.5538\n",
      "Epoch 49/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4953 - accuracy: 0.5819 - val_loss: 1.6107 - val_accuracy: 0.5510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4950 - accuracy: 0.5817 - val_loss: 1.6128 - val_accuracy: 0.5553\n",
      "Epoch 51/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4947 - accuracy: 0.5817 - val_loss: 1.5981 - val_accuracy: 0.5591\n",
      "Epoch 52/100\n",
      "2670376/2670376 [==============================] - 153s 57us/step - loss: 1.4938 - accuracy: 0.5820 - val_loss: 1.6077 - val_accuracy: 0.5549\n",
      "Epoch 53/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4937 - accuracy: 0.5822 - val_loss: 1.5983 - val_accuracy: 0.5567\n",
      "Epoch 54/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4934 - accuracy: 0.5823 - val_loss: 1.6089 - val_accuracy: 0.5527\n",
      "Epoch 55/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4934 - accuracy: 0.5821 - val_loss: 1.6130 - val_accuracy: 0.5524\n",
      "Epoch 56/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4926 - accuracy: 0.5824 - val_loss: 1.6021 - val_accuracy: 0.5589\n",
      "Epoch 57/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4917 - accuracy: 0.5825 - val_loss: 1.6060 - val_accuracy: 0.5536\n",
      "Epoch 58/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4911 - accuracy: 0.5829 - val_loss: 1.6064 - val_accuracy: 0.5531\n",
      "Epoch 59/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4910 - accuracy: 0.5827 - val_loss: 1.6137 - val_accuracy: 0.5535\n",
      "Epoch 60/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4912 - accuracy: 0.5828 - val_loss: 1.6072 - val_accuracy: 0.5533\n",
      "Epoch 61/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4904 - accuracy: 0.5829 - val_loss: 1.6073 - val_accuracy: 0.5508\n",
      "Epoch 62/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4900 - accuracy: 0.5830 - val_loss: 1.5930 - val_accuracy: 0.5605\n",
      "Epoch 63/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4897 - accuracy: 0.5831 - val_loss: 1.5973 - val_accuracy: 0.5582\n",
      "Epoch 64/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4898 - accuracy: 0.5831 - val_loss: 1.6050 - val_accuracy: 0.5558\n",
      "Epoch 65/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4888 - accuracy: 0.5835 - val_loss: 1.5982 - val_accuracy: 0.5590\n",
      "Epoch 66/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4880 - accuracy: 0.5835 - val_loss: 1.5962 - val_accuracy: 0.5589\n",
      "Epoch 67/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4883 - accuracy: 0.5835 - val_loss: 1.5979 - val_accuracy: 0.5570\n",
      "Epoch 68/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4877 - accuracy: 0.5838 - val_loss: 1.6043 - val_accuracy: 0.5568\n",
      "Epoch 69/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4871 - accuracy: 0.5837 - val_loss: 1.6014 - val_accuracy: 0.5551\n",
      "Epoch 70/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4866 - accuracy: 0.5836 - val_loss: 1.5973 - val_accuracy: 0.5567\n",
      "Epoch 71/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4863 - accuracy: 0.5839 - val_loss: 1.6001 - val_accuracy: 0.5586\n",
      "Epoch 72/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4864 - accuracy: 0.5838 - val_loss: 1.5989 - val_accuracy: 0.5584\n",
      "Epoch 73/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4862 - accuracy: 0.5840 - val_loss: 1.5900 - val_accuracy: 0.5607\n",
      "Epoch 74/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4857 - accuracy: 0.5840 - val_loss: 1.5993 - val_accuracy: 0.5557\n",
      "Epoch 75/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4850 - accuracy: 0.5843 - val_loss: 1.5996 - val_accuracy: 0.5602\n",
      "Epoch 76/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4850 - accuracy: 0.5842 - val_loss: 1.5984 - val_accuracy: 0.5592\n",
      "Epoch 77/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4854 - accuracy: 0.5840 - val_loss: 1.5894 - val_accuracy: 0.5608\n",
      "Epoch 78/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4845 - accuracy: 0.5845 - val_loss: 1.5980 - val_accuracy: 0.5562\n",
      "Epoch 79/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4843 - accuracy: 0.5845 - val_loss: 1.6022 - val_accuracy: 0.5561\n",
      "Epoch 80/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4836 - accuracy: 0.5845 - val_loss: 1.5958 - val_accuracy: 0.5562\n",
      "Epoch 81/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4843 - accuracy: 0.5842 - val_loss: 1.5850 - val_accuracy: 0.5611\n",
      "Epoch 82/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4841 - accuracy: 0.5844 - val_loss: 1.5874 - val_accuracy: 0.5601\n",
      "Epoch 83/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4836 - accuracy: 0.5845 - val_loss: 1.5896 - val_accuracy: 0.5590\n",
      "Epoch 84/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4835 - accuracy: 0.5846 - val_loss: 1.6024 - val_accuracy: 0.5577\n",
      "Epoch 85/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.4840 - accuracy: 0.5845 - val_loss: 1.5952 - val_accuracy: 0.5576\n",
      "Epoch 86/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.4827 - accuracy: 0.5846 - val_loss: 1.5923 - val_accuracy: 0.5602\n",
      "Epoch 87/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4828 - accuracy: 0.5847 - val_loss: 1.6001 - val_accuracy: 0.5568\n",
      "Epoch 88/100\n",
      "2670376/2670376 [==============================] - 154s 58us/step - loss: 1.4823 - accuracy: 0.5847 - val_loss: 1.5829 - val_accuracy: 0.5620\n",
      "Epoch 89/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4821 - accuracy: 0.5849 - val_loss: 1.5914 - val_accuracy: 0.5604\n",
      "Epoch 90/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4818 - accuracy: 0.5849 - val_loss: 1.5969 - val_accuracy: 0.5585\n",
      "Epoch 91/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4821 - accuracy: 0.5849 - val_loss: 1.5991 - val_accuracy: 0.5589\n",
      "Epoch 92/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4825 - accuracy: 0.5849 - val_loss: 1.5987 - val_accuracy: 0.5563\n",
      "Epoch 93/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4821 - accuracy: 0.5850 - val_loss: 1.5868 - val_accuracy: 0.5593\n",
      "Epoch 94/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4819 - accuracy: 0.5850 - val_loss: 1.5908 - val_accuracy: 0.5617\n",
      "Epoch 95/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4829 - accuracy: 0.5849 - val_loss: 1.5797 - val_accuracy: 0.5636\n",
      "Epoch 96/100\n",
      "2670376/2670376 [==============================] - 153s 57us/step - loss: 1.4816 - accuracy: 0.5850 - val_loss: 1.5837 - val_accuracy: 0.5620\n",
      "Epoch 97/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4803 - accuracy: 0.5852 - val_loss: 1.5963 - val_accuracy: 0.5576\n",
      "Epoch 98/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4798 - accuracy: 0.5855 - val_loss: 1.6044 - val_accuracy: 0.5567\n",
      "Epoch 99/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4805 - accuracy: 0.5852 - val_loss: 1.5883 - val_accuracy: 0.5615\n",
      "Epoch 100/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4801 - accuracy: 0.5855 - val_loss: 1.5923 - val_accuracy: 0.5605\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=10000, epochs=100, shuffle=True).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testear el modelo, lo mejor es ponerlo a prueba. Ya que no nos interesa tanto el accuracy como el hecho de que sea capaz de generar un texto legible y coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"I've seen a red unicorn fighting with a giant purple dinosaur. I was so scared that I suddenly woke up.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que vectoriza el input que se le va a dar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    x = np.zeros((1, len(text), len(chars)))\n",
    "\n",
    "    for t, char in enumerate(text):\n",
    "        x[0, t, char_to_index[char]] = 1.\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que predice 200 veces cada carácter en base al último chunk del texto (introducido + generado), generando un texto final que sea el input + 200 nuevos caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_completion(text):\n",
    "    text = re.sub('[^a-zA-Z0-9!?\\',.:\" ]+', '', text)\n",
    "    for _ in range(200):\n",
    "        x = prepare_input(text[-seq_length:])\n",
    "        pred = model.predict(x, verbose=0)[0]\n",
    "        next_index = max(range(len(pred)), key=pred.__getitem__)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        text += next_char\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pongo a prueba el modelo con mi frase de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've seen a red unicorn fighting with a giant purple dinosaur. I was so scared that I suddenly woke up. http:twitpic.comerik  http:twitpic.com1b5nw  The proper proper story is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the w\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the people wants to know if they can still still step the most complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the w\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('But the people wants to know if they can still'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thats how we will overcome the challenges we face: http:buff.ly1Setfa  http:twitpic.com1b5nw  The proper proper story is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the wor\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('That’s how we will overcome the challenges we face:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG no way! Now I need to go there! I've never been and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still a\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"OMG no way! Now I need to go there! I've never been\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's very interesting! I wonder if I could do the same as you did. Thank you!\n",
      "The president we can allow a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"It's very interesting! I wonder if I could do the same as you did.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados denotan que al modelo le ha faltado más entrenamiento para alcanzar la complejidad suficiente. Es interesante ver que aun así es capaz de generar palabras escritas perfectamente, respetando todos los signos de puntuación y hasta saltos de línea (como vemos en el último ejemplo). Para tratarse de un modelo tan sencillo la verdad es que me esperaba algo mucho peor.\n",
    "\n",
    "Da la sensación de que el modelo ha encontrado patrones de caracteres que se repiten mucho (\"is a complete of the world\", \"and the massive still\") y se está limitando a repetir esos patrones una y otra vez porque ha encontrado que, seguramente, así tiene más probabilidades de acertar.\n",
    "\n",
    "Limitaciones:<br>\n",
    "Al principio quise probar juntando el contenido de todos los tweets para saber si el bot podría ser capaz de adquirir un nuevo \"estilo\", mezclando el de todos los autores, o si podía escribir de una manera similar a estos en función del tipo de texto que se le diese como input, pero la carga de datos al vectorizar los caracteres era tan grande que no tenía suficiente memoria RAM y tuve que recortar el número de tweets a 25000 (aunque también podría haber optado por hacer chunks más pequeños). El entrenamiento durante 100 épocas también llevó mucho tiempo y aun así hemos visto que no es suficiente. Este modelo no sirve para el propósito que teníamos, ya que aunque construya frases legibles, no son nada coherentes y tan solo está repitiendo la misma secuencia una y otra vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez voy a utilizar únicamente los tweets de Donald Trump y voy a entrenar al modelo durante el doble de épocas. Al final compararé los resultados con los obtenidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oct 7</td>\n",
       "      <td>784609194234306560</td>\n",
       "      <td>/realDonaldTrump/status/784609194234306560</td>\n",
       "      <td>False</td>\n",
       "      <td>Here is my statement.pic.twitter.com/WAZiGoQqMQ</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oct 10</td>\n",
       "      <td>785608815962099712</td>\n",
       "      <td>/realDonaldTrump/status/785608815962099712</td>\n",
       "      <td>False</td>\n",
       "      <td>Is this really America? Terrible!pic.twitter.c...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Oct 8</td>\n",
       "      <td>784840992734064640</td>\n",
       "      <td>/realDonaldTrump/status/784840992734064641</td>\n",
       "      <td>False</td>\n",
       "      <td>The media and establishment want me out of the...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oct 11</td>\n",
       "      <td>785979396620324864</td>\n",
       "      <td>/realDonaldTrump/status/785979396620324865</td>\n",
       "      <td>False</td>\n",
       "      <td>Wow, @CNN Town Hall questions were given to Cr...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oct 10</td>\n",
       "      <td>785561269571026944</td>\n",
       "      <td>/realDonaldTrump/status/785561269571026946</td>\n",
       "      <td>False</td>\n",
       "      <td>Debate polls look great - thank you!\\n#MAGA #A...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    date                  id  \\\n",
       "0           0   Oct 7  784609194234306560   \n",
       "1           1  Oct 10  785608815962099712   \n",
       "2           2   Oct 8  784840992734064640   \n",
       "3           3  Oct 11  785979396620324864   \n",
       "4           4  Oct 10  785561269571026944   \n",
       "\n",
       "                                         link  retweet  \\\n",
       "0  /realDonaldTrump/status/784609194234306560    False   \n",
       "1  /realDonaldTrump/status/785608815962099712    False   \n",
       "2  /realDonaldTrump/status/784840992734064641    False   \n",
       "3  /realDonaldTrump/status/785979396620324865    False   \n",
       "4  /realDonaldTrump/status/785561269571026946    False   \n",
       "\n",
       "                                                text       author  \n",
       "0    Here is my statement.pic.twitter.com/WAZiGoQqMQ  DonaldTrump  \n",
       "1  Is this really America? Terrible!pic.twitter.c...  DonaldTrump  \n",
       "2  The media and establishment want me out of the...  DonaldTrump  \n",
       "3  Wow, @CNN Town Hall questions were given to Cr...  DonaldTrump  \n",
       "4  Debate polls look great - thank you!\\n#MAGA #A...  DonaldTrump  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob('twitter_corpus/DonaldTrump*.csv')\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "corpus = pd.concat(li, axis=0, ignore_index=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = \"\\n\".join(re.sub('[^a-zA-Z0-9!?\\',.:\" ]+', '', tweet) for tweet in corpus.sample(25000).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(full_corpus)))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples: 2914475\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(full_corpus)-seq_length, step):\n",
    "    sentences.append(full_corpus[i:i+seq_length])\n",
    "    next_chars.append(full_corpus[i+seq_length])\n",
    "    \n",
    "print('num training examples: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), seq_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2768751 samples, validate on 145724 samples\n",
      "Epoch 1/200\n",
      "2768751/2768751 [==============================] - 161s 58us/step - loss: 2.4189 - accuracy: 0.3489 - val_loss: 1.9357 - val_accuracy: 0.4614\n",
      "Epoch 2/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.7594 - accuracy: 0.5105 - val_loss: 1.6653 - val_accuracy: 0.5349\n",
      "Epoch 3/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.6119 - accuracy: 0.5494 - val_loss: 1.5774 - val_accuracy: 0.5585\n",
      "Epoch 4/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.5443 - accuracy: 0.5670 - val_loss: 1.5385 - val_accuracy: 0.5686\n",
      "Epoch 5/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.5052 - accuracy: 0.5769 - val_loss: 1.5074 - val_accuracy: 0.5769\n",
      "Epoch 6/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4790 - accuracy: 0.5832 - val_loss: 1.4842 - val_accuracy: 0.5827\n",
      "Epoch 7/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4603 - accuracy: 0.5882 - val_loss: 1.4745 - val_accuracy: 0.5848\n",
      "Epoch 8/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4463 - accuracy: 0.5915 - val_loss: 1.4608 - val_accuracy: 0.5876\n",
      "Epoch 9/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4349 - accuracy: 0.5944 - val_loss: 1.4589 - val_accuracy: 0.5893\n",
      "Epoch 10/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4260 - accuracy: 0.5967 - val_loss: 1.4498 - val_accuracy: 0.5916\n",
      "Epoch 11/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4187 - accuracy: 0.5986 - val_loss: 1.4425 - val_accuracy: 0.5937\n",
      "Epoch 12/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4123 - accuracy: 0.6002 - val_loss: 1.4382 - val_accuracy: 0.5950\n",
      "Epoch 13/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4066 - accuracy: 0.6015 - val_loss: 1.4321 - val_accuracy: 0.5962\n",
      "Epoch 14/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.4018 - accuracy: 0.6028 - val_loss: 1.4311 - val_accuracy: 0.5968\n",
      "Epoch 15/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3972 - accuracy: 0.6041 - val_loss: 1.4311 - val_accuracy: 0.5977\n",
      "Epoch 16/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3937 - accuracy: 0.6051 - val_loss: 1.4260 - val_accuracy: 0.5991\n",
      "Epoch 17/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3902 - accuracy: 0.6058 - val_loss: 1.4199 - val_accuracy: 0.6004\n",
      "Epoch 18/200\n",
      "2768751/2768751 [==============================] - 157s 57us/step - loss: 1.3870 - accuracy: 0.6067 - val_loss: 1.4224 - val_accuracy: 0.5995\n",
      "Epoch 19/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3843 - accuracy: 0.6073 - val_loss: 1.4200 - val_accuracy: 0.6006\n",
      "Epoch 20/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3818 - accuracy: 0.6078 - val_loss: 1.4183 - val_accuracy: 0.6005\n",
      "Epoch 21/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3793 - accuracy: 0.6085 - val_loss: 1.4161 - val_accuracy: 0.6013\n",
      "Epoch 22/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3773 - accuracy: 0.6091 - val_loss: 1.4147 - val_accuracy: 0.6016\n",
      "Epoch 23/200\n",
      "2768751/2768751 [==============================] - 156s 57us/step - loss: 1.3753 - accuracy: 0.6097 - val_loss: 1.4142 - val_accuracy: 0.6011\n",
      "Epoch 24/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3733 - accuracy: 0.6101 - val_loss: 1.4160 - val_accuracy: 0.6010\n",
      "Epoch 25/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3719 - accuracy: 0.6106 - val_loss: 1.4104 - val_accuracy: 0.6018\n",
      "Epoch 26/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3704 - accuracy: 0.6107 - val_loss: 1.4126 - val_accuracy: 0.6020\n",
      "Epoch 27/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3688 - accuracy: 0.6111 - val_loss: 1.4095 - val_accuracy: 0.6036\n",
      "Epoch 28/200\n",
      "2768751/2768751 [==============================] - 157s 57us/step - loss: 1.3674 - accuracy: 0.6117 - val_loss: 1.4122 - val_accuracy: 0.6024\n",
      "Epoch 29/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3663 - accuracy: 0.6116 - val_loss: 1.4087 - val_accuracy: 0.6037\n",
      "Epoch 30/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3648 - accuracy: 0.6123 - val_loss: 1.4096 - val_accuracy: 0.6029\n",
      "Epoch 31/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3639 - accuracy: 0.6124 - val_loss: 1.4053 - val_accuracy: 0.6040\n",
      "Epoch 32/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3628 - accuracy: 0.6125 - val_loss: 1.4046 - val_accuracy: 0.6045\n",
      "Epoch 33/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3624 - accuracy: 0.6126 - val_loss: 1.4060 - val_accuracy: 0.6049\n",
      "Epoch 34/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3613 - accuracy: 0.6128 - val_loss: 1.4015 - val_accuracy: 0.6045\n",
      "Epoch 35/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3601 - accuracy: 0.6133 - val_loss: 1.4079 - val_accuracy: 0.6015\n",
      "Epoch 36/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3591 - accuracy: 0.6134 - val_loss: 1.4018 - val_accuracy: 0.6051\n",
      "Epoch 37/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3587 - accuracy: 0.6138 - val_loss: 1.4008 - val_accuracy: 0.6048\n",
      "Epoch 38/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3575 - accuracy: 0.6141 - val_loss: 1.4019 - val_accuracy: 0.6042\n",
      "Epoch 39/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3571 - accuracy: 0.6140 - val_loss: 1.4002 - val_accuracy: 0.6059\n",
      "Epoch 40/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3562 - accuracy: 0.6142 - val_loss: 1.4001 - val_accuracy: 0.6059\n",
      "Epoch 41/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3554 - accuracy: 0.6145 - val_loss: 1.3994 - val_accuracy: 0.6064\n",
      "Epoch 42/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3550 - accuracy: 0.6145 - val_loss: 1.3989 - val_accuracy: 0.6061\n",
      "Epoch 43/200\n",
      "2768751/2768751 [==============================] - 160s 58us/step - loss: 1.3545 - accuracy: 0.6146 - val_loss: 1.4003 - val_accuracy: 0.6054\n",
      "Epoch 44/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3534 - accuracy: 0.6148 - val_loss: 1.4018 - val_accuracy: 0.6055\n",
      "Epoch 45/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3534 - accuracy: 0.6149 - val_loss: 1.4020 - val_accuracy: 0.6052\n",
      "Epoch 46/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3525 - accuracy: 0.6151 - val_loss: 1.3966 - val_accuracy: 0.6066\n",
      "Epoch 47/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3520 - accuracy: 0.6153 - val_loss: 1.3977 - val_accuracy: 0.6064\n",
      "Epoch 48/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3514 - accuracy: 0.6154 - val_loss: 1.3963 - val_accuracy: 0.6072\n",
      "Epoch 49/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3508 - accuracy: 0.6156 - val_loss: 1.3971 - val_accuracy: 0.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3502 - accuracy: 0.6158 - val_loss: 1.3966 - val_accuracy: 0.6071\n",
      "Epoch 51/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3504 - accuracy: 0.6156 - val_loss: 1.3987 - val_accuracy: 0.6056\n",
      "Epoch 52/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3495 - accuracy: 0.6157 - val_loss: 1.3970 - val_accuracy: 0.6057\n",
      "Epoch 53/200\n",
      "2768751/2768751 [==============================] - 159s 58us/step - loss: 1.3488 - accuracy: 0.6161 - val_loss: 1.3932 - val_accuracy: 0.6072\n",
      "Epoch 54/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3486 - accuracy: 0.6162 - val_loss: 1.3950 - val_accuracy: 0.6072\n",
      "Epoch 55/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3481 - accuracy: 0.6162 - val_loss: 1.3959 - val_accuracy: 0.6070\n",
      "Epoch 56/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3480 - accuracy: 0.6162 - val_loss: 1.3928 - val_accuracy: 0.6083\n",
      "Epoch 57/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3475 - accuracy: 0.6163 - val_loss: 1.3930 - val_accuracy: 0.6081\n",
      "Epoch 58/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3473 - accuracy: 0.6164 - val_loss: 1.3968 - val_accuracy: 0.6066\n",
      "Epoch 59/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3466 - accuracy: 0.6165 - val_loss: 1.3940 - val_accuracy: 0.6076\n",
      "Epoch 60/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3466 - accuracy: 0.6167 - val_loss: 1.3933 - val_accuracy: 0.6084\n",
      "Epoch 61/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3466 - accuracy: 0.6166 - val_loss: 1.3912 - val_accuracy: 0.6076\n",
      "Epoch 62/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3459 - accuracy: 0.6167 - val_loss: 1.3956 - val_accuracy: 0.6056\n",
      "Epoch 63/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3455 - accuracy: 0.6168 - val_loss: 1.3930 - val_accuracy: 0.6072\n",
      "Epoch 64/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3452 - accuracy: 0.6168 - val_loss: 1.3942 - val_accuracy: 0.6057\n",
      "Epoch 65/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3449 - accuracy: 0.6170 - val_loss: 1.3927 - val_accuracy: 0.6074\n",
      "Epoch 66/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3444 - accuracy: 0.6169 - val_loss: 1.3971 - val_accuracy: 0.6060\n",
      "Epoch 67/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3442 - accuracy: 0.6170 - val_loss: 1.3924 - val_accuracy: 0.6078\n",
      "Epoch 68/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3439 - accuracy: 0.6172 - val_loss: 1.3932 - val_accuracy: 0.6075\n",
      "Epoch 69/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3436 - accuracy: 0.6171 - val_loss: 1.3951 - val_accuracy: 0.6065\n",
      "Epoch 70/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3434 - accuracy: 0.6173 - val_loss: 1.3911 - val_accuracy: 0.6089\n",
      "Epoch 71/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3432 - accuracy: 0.6174 - val_loss: 1.3945 - val_accuracy: 0.6072\n",
      "Epoch 72/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3428 - accuracy: 0.6174 - val_loss: 1.3932 - val_accuracy: 0.6067\n",
      "Epoch 73/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3434 - accuracy: 0.6172 - val_loss: 1.3942 - val_accuracy: 0.6073\n",
      "Epoch 74/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3423 - accuracy: 0.6177 - val_loss: 1.3912 - val_accuracy: 0.6077\n",
      "Epoch 75/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3426 - accuracy: 0.6173 - val_loss: 1.3902 - val_accuracy: 0.6088\n",
      "Epoch 76/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3418 - accuracy: 0.6178 - val_loss: 1.3912 - val_accuracy: 0.6095\n",
      "Epoch 77/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3417 - accuracy: 0.6178 - val_loss: 1.3964 - val_accuracy: 0.6055\n",
      "Epoch 78/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3419 - accuracy: 0.6176 - val_loss: 1.3898 - val_accuracy: 0.6095\n",
      "Epoch 79/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3413 - accuracy: 0.6176 - val_loss: 1.3906 - val_accuracy: 0.6073\n",
      "Epoch 80/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3410 - accuracy: 0.6179 - val_loss: 1.3939 - val_accuracy: 0.6066\n",
      "Epoch 81/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3416 - accuracy: 0.6177 - val_loss: 1.3894 - val_accuracy: 0.6095\n",
      "Epoch 82/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3407 - accuracy: 0.6180 - val_loss: 1.3915 - val_accuracy: 0.6069\n",
      "Epoch 83/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3407 - accuracy: 0.6179 - val_loss: 1.3915 - val_accuracy: 0.6070\n",
      "Epoch 84/200\n",
      "2768751/2768751 [==============================] - 157s 57us/step - loss: 1.3413 - accuracy: 0.6178 - val_loss: 1.3922 - val_accuracy: 0.6081\n",
      "Epoch 85/200\n",
      "2768751/2768751 [==============================] - 157s 57us/step - loss: 1.3402 - accuracy: 0.6180 - val_loss: 1.3931 - val_accuracy: 0.6075\n",
      "Epoch 86/200\n",
      "2768751/2768751 [==============================] - 159s 58us/step - loss: 1.3399 - accuracy: 0.6183 - val_loss: 1.3894 - val_accuracy: 0.6082\n",
      "Epoch 87/200\n",
      "2768751/2768751 [==============================] - 160s 58us/step - loss: 1.3407 - accuracy: 0.6180 - val_loss: 1.3890 - val_accuracy: 0.6079\n",
      "Epoch 88/200\n",
      "2768751/2768751 [==============================] - 161s 58us/step - loss: 1.3398 - accuracy: 0.6184 - val_loss: 1.3916 - val_accuracy: 0.6080\n",
      "Epoch 89/200\n",
      "2768751/2768751 [==============================] - 159s 58us/step - loss: 1.3393 - accuracy: 0.6184 - val_loss: 1.3903 - val_accuracy: 0.6074\n",
      "Epoch 90/200\n",
      "2768751/2768751 [==============================] - 159s 58us/step - loss: 1.3391 - accuracy: 0.6182 - val_loss: 1.3887 - val_accuracy: 0.6081\n",
      "Epoch 91/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3399 - accuracy: 0.6181 - val_loss: 1.3892 - val_accuracy: 0.6086\n",
      "Epoch 92/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3386 - accuracy: 0.6183 - val_loss: 1.3934 - val_accuracy: 0.6067\n",
      "Epoch 93/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3384 - accuracy: 0.6186 - val_loss: 1.3872 - val_accuracy: 0.6092\n",
      "Epoch 94/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3382 - accuracy: 0.6188 - val_loss: 1.3892 - val_accuracy: 0.6093\n",
      "Epoch 95/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3379 - accuracy: 0.6188 - val_loss: 1.3920 - val_accuracy: 0.6071\n",
      "Epoch 96/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3384 - accuracy: 0.6186 - val_loss: 1.3904 - val_accuracy: 0.6082\n",
      "Epoch 97/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3377 - accuracy: 0.6187 - val_loss: 1.3874 - val_accuracy: 0.6086\n",
      "Epoch 98/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3385 - accuracy: 0.6186 - val_loss: 1.3902 - val_accuracy: 0.6080\n",
      "Epoch 99/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3374 - accuracy: 0.6187 - val_loss: 1.3886 - val_accuracy: 0.6082\n",
      "Epoch 100/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3386 - accuracy: 0.6185 - val_loss: 1.3900 - val_accuracy: 0.6092\n",
      "Epoch 101/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3393 - accuracy: 0.6184 - val_loss: 1.3996 - val_accuracy: 0.6065\n",
      "Epoch 102/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3391 - accuracy: 0.6185 - val_loss: 1.3919 - val_accuracy: 0.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3372 - accuracy: 0.6191 - val_loss: 1.3890 - val_accuracy: 0.6082\n",
      "Epoch 104/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3366 - accuracy: 0.6191 - val_loss: 1.3880 - val_accuracy: 0.6088\n",
      "Epoch 105/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3370 - accuracy: 0.6189 - val_loss: 1.3877 - val_accuracy: 0.6084\n",
      "Epoch 106/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3371 - accuracy: 0.6189 - val_loss: 1.3884 - val_accuracy: 0.6085\n",
      "Epoch 107/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3362 - accuracy: 0.6192 - val_loss: 1.3871 - val_accuracy: 0.6089\n",
      "Epoch 108/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3362 - accuracy: 0.6192 - val_loss: 1.3887 - val_accuracy: 0.6087\n",
      "Epoch 109/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3364 - accuracy: 0.6192 - val_loss: 1.3918 - val_accuracy: 0.6079\n",
      "Epoch 110/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3364 - accuracy: 0.6193 - val_loss: 1.3881 - val_accuracy: 0.6080\n",
      "Epoch 111/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3360 - accuracy: 0.6193 - val_loss: 1.3857 - val_accuracy: 0.6095\n",
      "Epoch 112/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3355 - accuracy: 0.6192 - val_loss: 1.3882 - val_accuracy: 0.6081\n",
      "Epoch 113/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3363 - accuracy: 0.6190 - val_loss: 1.4018 - val_accuracy: 0.6055\n",
      "Epoch 114/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3358 - accuracy: 0.6192 - val_loss: 1.3872 - val_accuracy: 0.6091\n",
      "Epoch 115/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3356 - accuracy: 0.6195 - val_loss: 1.3889 - val_accuracy: 0.6086\n",
      "Epoch 116/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3365 - accuracy: 0.6192 - val_loss: 1.3864 - val_accuracy: 0.6092\n",
      "Epoch 117/200\n",
      "2768751/2768751 [==============================] - 156s 57us/step - loss: 1.3351 - accuracy: 0.6194 - val_loss: 1.3850 - val_accuracy: 0.6095\n",
      "Epoch 118/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3349 - accuracy: 0.6194 - val_loss: 1.3866 - val_accuracy: 0.6090\n",
      "Epoch 119/200\n",
      "2768751/2768751 [==============================] - 154s 56us/step - loss: 1.3346 - accuracy: 0.6195 - val_loss: 1.3870 - val_accuracy: 0.6094\n",
      "Epoch 120/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3347 - accuracy: 0.6195 - val_loss: 1.3861 - val_accuracy: 0.6092\n",
      "Epoch 121/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3348 - accuracy: 0.6195 - val_loss: 1.3861 - val_accuracy: 0.6087\n",
      "Epoch 122/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3346 - accuracy: 0.6197 - val_loss: 1.3873 - val_accuracy: 0.6083\n",
      "Epoch 123/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3362 - accuracy: 0.6192 - val_loss: 1.3847 - val_accuracy: 0.6087\n",
      "Epoch 124/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3346 - accuracy: 0.6195 - val_loss: 1.3887 - val_accuracy: 0.6088\n",
      "Epoch 125/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3357 - accuracy: 0.6193 - val_loss: 1.3885 - val_accuracy: 0.6084\n",
      "Epoch 126/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3347 - accuracy: 0.6196 - val_loss: 1.3901 - val_accuracy: 0.6071\n",
      "Epoch 127/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3353 - accuracy: 0.6196 - val_loss: 1.3853 - val_accuracy: 0.6100\n",
      "Epoch 128/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3349 - accuracy: 0.6194 - val_loss: 1.3848 - val_accuracy: 0.6109\n",
      "Epoch 129/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3349 - accuracy: 0.6197 - val_loss: 1.3861 - val_accuracy: 0.6093\n",
      "Epoch 130/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3345 - accuracy: 0.6197 - val_loss: 1.3842 - val_accuracy: 0.6094\n",
      "Epoch 131/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3342 - accuracy: 0.6197 - val_loss: 1.3886 - val_accuracy: 0.6085\n",
      "Epoch 132/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3345 - accuracy: 0.6196 - val_loss: 1.3881 - val_accuracy: 0.6091\n",
      "Epoch 133/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3337 - accuracy: 0.6199 - val_loss: 1.3860 - val_accuracy: 0.6097\n",
      "Epoch 134/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3332 - accuracy: 0.6201 - val_loss: 1.3884 - val_accuracy: 0.6086\n",
      "Epoch 135/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3333 - accuracy: 0.6199 - val_loss: 1.3872 - val_accuracy: 0.6089\n",
      "Epoch 136/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3332 - accuracy: 0.6197 - val_loss: 1.3868 - val_accuracy: 0.6086\n",
      "Epoch 137/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3332 - accuracy: 0.6198 - val_loss: 1.3872 - val_accuracy: 0.6097\n",
      "Epoch 138/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3340 - accuracy: 0.6199 - val_loss: 1.3878 - val_accuracy: 0.6094\n",
      "Epoch 139/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3331 - accuracy: 0.6204 - val_loss: 1.3879 - val_accuracy: 0.6085\n",
      "Epoch 140/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3330 - accuracy: 0.6202 - val_loss: 1.3853 - val_accuracy: 0.6100\n",
      "Epoch 141/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3328 - accuracy: 0.6200 - val_loss: 1.3860 - val_accuracy: 0.6098\n",
      "Epoch 142/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3328 - accuracy: 0.6200 - val_loss: 1.3861 - val_accuracy: 0.6093\n",
      "Epoch 143/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3330 - accuracy: 0.6202 - val_loss: 1.3870 - val_accuracy: 0.6102\n",
      "Epoch 144/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3348 - accuracy: 0.6196 - val_loss: 1.3867 - val_accuracy: 0.6094\n",
      "Epoch 145/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3335 - accuracy: 0.6200 - val_loss: 1.3863 - val_accuracy: 0.6082\n",
      "Epoch 146/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3328 - accuracy: 0.6199 - val_loss: 1.3876 - val_accuracy: 0.6088\n",
      "Epoch 147/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3325 - accuracy: 0.6201 - val_loss: 1.3865 - val_accuracy: 0.6100\n",
      "Epoch 148/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3321 - accuracy: 0.6203 - val_loss: 1.3874 - val_accuracy: 0.6101\n",
      "Epoch 149/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3324 - accuracy: 0.6202 - val_loss: 1.3852 - val_accuracy: 0.6105\n",
      "Epoch 150/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3331 - accuracy: 0.6200 - val_loss: 1.3845 - val_accuracy: 0.6093\n",
      "Epoch 151/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3328 - accuracy: 0.6199 - val_loss: 1.3852 - val_accuracy: 0.6097\n",
      "Epoch 152/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3321 - accuracy: 0.6203 - val_loss: 1.3849 - val_accuracy: 0.6102\n",
      "Epoch 153/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3323 - accuracy: 0.6200 - val_loss: 1.3887 - val_accuracy: 0.6081\n",
      "Epoch 154/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3324 - accuracy: 0.6200 - val_loss: 1.3848 - val_accuracy: 0.6097\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3321 - accuracy: 0.6200 - val_loss: 1.3840 - val_accuracy: 0.6099\n",
      "Epoch 156/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3322 - accuracy: 0.6201 - val_loss: 1.3832 - val_accuracy: 0.6102\n",
      "Epoch 157/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3315 - accuracy: 0.6201 - val_loss: 1.3852 - val_accuracy: 0.6104\n",
      "Epoch 158/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3327 - accuracy: 0.6202 - val_loss: 1.3854 - val_accuracy: 0.6098\n",
      "Epoch 159/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3337 - accuracy: 0.6198 - val_loss: 1.3841 - val_accuracy: 0.6093\n",
      "Epoch 160/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3319 - accuracy: 0.6203 - val_loss: 1.3882 - val_accuracy: 0.6088\n",
      "Epoch 161/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3322 - accuracy: 0.6201 - val_loss: 1.3869 - val_accuracy: 0.6085\n",
      "Epoch 162/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3320 - accuracy: 0.6198 - val_loss: 1.3839 - val_accuracy: 0.6095\n",
      "Epoch 163/200\n",
      "2768751/2768751 [==============================] - 155s 56us/step - loss: 1.3315 - accuracy: 0.6203 - val_loss: 1.3853 - val_accuracy: 0.6098\n",
      "Epoch 164/200\n",
      "2768751/2768751 [==============================] - 156s 56us/step - loss: 1.3321 - accuracy: 0.6203 - val_loss: 1.3842 - val_accuracy: 0.6109\n",
      "Epoch 165/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3315 - accuracy: 0.6203 - val_loss: 1.3849 - val_accuracy: 0.6099\n",
      "Epoch 166/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3313 - accuracy: 0.6204 - val_loss: 1.3848 - val_accuracy: 0.6098\n",
      "Epoch 167/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3306 - accuracy: 0.6205 - val_loss: 1.3850 - val_accuracy: 0.6101\n",
      "Epoch 168/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3304 - accuracy: 0.6205 - val_loss: 1.3830 - val_accuracy: 0.6109\n",
      "Epoch 169/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3304 - accuracy: 0.6206 - val_loss: 1.3880 - val_accuracy: 0.6099\n",
      "Epoch 170/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3307 - accuracy: 0.6203 - val_loss: 1.3879 - val_accuracy: 0.6088\n",
      "Epoch 171/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3315 - accuracy: 0.6203 - val_loss: 1.3856 - val_accuracy: 0.6095\n",
      "Epoch 172/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3310 - accuracy: 0.6203 - val_loss: 1.3852 - val_accuracy: 0.6096\n",
      "Epoch 173/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3318 - accuracy: 0.6203 - val_loss: 1.3875 - val_accuracy: 0.6088\n",
      "Epoch 174/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3316 - accuracy: 0.6202 - val_loss: 1.3842 - val_accuracy: 0.6096\n",
      "Epoch 175/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3319 - accuracy: 0.6203 - val_loss: 1.3873 - val_accuracy: 0.6076\n",
      "Epoch 176/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3315 - accuracy: 0.6202 - val_loss: 1.3874 - val_accuracy: 0.6083\n",
      "Epoch 177/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3307 - accuracy: 0.6205 - val_loss: 1.3866 - val_accuracy: 0.6081\n",
      "Epoch 178/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3319 - accuracy: 0.6200 - val_loss: 1.3913 - val_accuracy: 0.6068\n",
      "Epoch 179/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3314 - accuracy: 0.6204 - val_loss: 1.3855 - val_accuracy: 0.6084\n",
      "Epoch 180/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3302 - accuracy: 0.6208 - val_loss: 1.3868 - val_accuracy: 0.6084\n",
      "Epoch 181/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3307 - accuracy: 0.6204 - val_loss: 1.3866 - val_accuracy: 0.6097\n",
      "Epoch 182/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3299 - accuracy: 0.6207 - val_loss: 1.3847 - val_accuracy: 0.6092\n",
      "Epoch 183/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3317 - accuracy: 0.6202 - val_loss: 1.3938 - val_accuracy: 0.6063\n",
      "Epoch 184/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3306 - accuracy: 0.6205 - val_loss: 1.3828 - val_accuracy: 0.6097\n",
      "Epoch 185/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3296 - accuracy: 0.6208 - val_loss: 1.3838 - val_accuracy: 0.6105\n",
      "Epoch 186/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3298 - accuracy: 0.6206 - val_loss: 1.3850 - val_accuracy: 0.6092\n",
      "Epoch 187/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3306 - accuracy: 0.6204 - val_loss: 1.3827 - val_accuracy: 0.6102\n",
      "Epoch 188/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3297 - accuracy: 0.6207 - val_loss: 1.3859 - val_accuracy: 0.6103\n",
      "Epoch 189/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3309 - accuracy: 0.6205 - val_loss: 1.3825 - val_accuracy: 0.6095\n",
      "Epoch 190/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3298 - accuracy: 0.6207 - val_loss: 1.3824 - val_accuracy: 0.6101\n",
      "Epoch 191/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3296 - accuracy: 0.6205 - val_loss: 1.3828 - val_accuracy: 0.6101\n",
      "Epoch 192/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3292 - accuracy: 0.6210 - val_loss: 1.3837 - val_accuracy: 0.6098\n",
      "Epoch 193/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3290 - accuracy: 0.6208 - val_loss: 1.3840 - val_accuracy: 0.6092\n",
      "Epoch 194/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3313 - accuracy: 0.6204 - val_loss: 1.3861 - val_accuracy: 0.6083\n",
      "Epoch 195/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3323 - accuracy: 0.6203 - val_loss: 1.3835 - val_accuracy: 0.6112\n",
      "Epoch 196/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3302 - accuracy: 0.6206 - val_loss: 1.3860 - val_accuracy: 0.6086\n",
      "Epoch 197/200\n",
      "2768751/2768751 [==============================] - 158s 57us/step - loss: 1.3308 - accuracy: 0.6204 - val_loss: 1.3828 - val_accuracy: 0.6091\n",
      "Epoch 198/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3306 - accuracy: 0.6207 - val_loss: 1.3853 - val_accuracy: 0.6095\n",
      "Epoch 199/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3302 - accuracy: 0.6207 - val_loss: 1.3894 - val_accuracy: 0.6084\n",
      "Epoch 200/200\n",
      "2768751/2768751 [==============================] - 159s 57us/step - loss: 1.3290 - accuracy: 0.6208 - val_loss: 1.3837 - val_accuracy: 0.6097\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=10000, epochs=200, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've seen a red unicorn fighting with a giant purple dinosaur. I was so scared that I suddenly woke up. They are so many thing about the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the standar\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the people wants to know if they can still be a great new politicians and the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the stand\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('But the people wants to know if they can still'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thats how we will overcome the challenges we face: http:www.trump.comman the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the standard that \n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('That’s how we will overcome the challenges we face:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG no way! Now I need to go there! I've never been a great policy and the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the standard that I w\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"OMG no way! Now I need to go there! I've never been\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's very interesting! I wonder if I could do the same as you did. The world is a great policy and the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the stan\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"It's very interesting! I wonder if I could do the same as you did.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos extraer más o menos las mismas conclusiones que antes. Tiene pinta de que el texto que nos está generando el modelo son tramos de frases que Trump seguramente no para de repetir una y otra vez de forma machacante. Harían falta muchísimas más épocas y tal vez probar distintas arquitecturas o diferentes optimizadores. ¿Qué pasará si le pasamos como input el comienzo de alguno de los tweets de Donald Trump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be in Missouri today with Melania for the president in the standard that I was a great state of the state of the president in the standard that I was a great state of the state of the president in the standard that I was a great state \n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(corpus.text[127][:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podría ser interesante entrenar un modelo que en lugar de fijarse en chunks de caracteres se fijase en chunks de tokens junto a sus POS tags (cada token podría ser un vector que representase tanto a la palabra como a su correspondiente POS tag), de ese modo el modelo no solo podría predecir qué palabra sigue a las anteriores, sino también qué tipo de palabra. Y esta idea se podría llevar incluso más allá con la inclusión de NERs para que el modelo no solo conozca la sintaxis, sino incluso \"de qué está hablando\".\n",
    "\n",
    "También se me ocurre que podría utilizarse un modelo entrenado de esta manera y aplicarle una serie de reglas que permitiesen modelar su \"personalidad\" y su vocabulario \"silenciando\" o \"limitando\" ciertas palabras de los resultados que son devueltos, ya que el modelo al final devuelve cada posible resultado con el \"peso\" que le corresponde.\n",
    "\n",
    "Aun así, nos seguiríamos encontrando con la limitación de que necesitaríamos una inmensísima cantidad de texto para tener un diccionario lo bastante completo y una representación lo suficientemente fidedigna de cada una de las palabras. No solo eso, sino que la capacidad de cómputo debería ser enormísima y aun así posiblemente el entrenamiento llevaría bastante tiempo.\n",
    "\n",
    "Tal vez para algo más sencillo, como es imitar a una sola personalidad en tweeter, sea suficiente con un modelo que se base en la predicción de caracteres y que aprenda de ese usuario en concreto. Aun así, lo veo muy lejos del alcance de un individuo usando su propio PC, al menos en un periodo de tiempo que no sea excesivamente largo, ya que podemos ver cómo el coste entre la época 100 y la 200 a penas disminuye, y cada vez disminuye más despacio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
