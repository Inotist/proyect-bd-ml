{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica del módulo NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargo todos los tweetts y los concateno en un único dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783396985093193728</td>\n",
       "      <td>/missyscheng/status/783396985093193728</td>\n",
       "      <td>False</td>\n",
       "      <td>#DataScience Basics: #DataMining vs. #Statisti...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783381842024103936</td>\n",
       "      <td>/EXASOLAG/status/783381842024103936</td>\n",
       "      <td>False</td>\n",
       "      <td>How to Become a #Data Scientist – Part 1: http...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783433625723252736</td>\n",
       "      <td>/TarasNovak/status/783433625723252736</td>\n",
       "      <td>False</td>\n",
       "      <td>@jesterxl @kdnuggets or just go with @tableau :)</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oct 4</td>\n",
       "      <td>783428740453982208</td>\n",
       "      <td>/kdnuggets/status/783428740453982208</td>\n",
       "      <td>False</td>\n",
       "      <td>#Boston U. Online MS in Applied #Business #Ana...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1h1 hour ago</td>\n",
       "      <td>787052623291641856</td>\n",
       "      <td>/kdnuggets/status/787052623291641856</td>\n",
       "      <td>False</td>\n",
       "      <td>#ICYMI Still Searching for ROI in #BigData Ana...</td>\n",
       "      <td>various</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          date                  id  \\\n",
       "0           0         Oct 4  783396985093193728   \n",
       "1           1         Oct 4  783381842024103936   \n",
       "2           2         Oct 4  783433625723252736   \n",
       "3           3         Oct 4  783428740453982208   \n",
       "4           4  1h1 hour ago  787052623291641856   \n",
       "\n",
       "                                     link  retweet  \\\n",
       "0  /missyscheng/status/783396985093193728    False   \n",
       "1     /EXASOLAG/status/783381842024103936    False   \n",
       "2   /TarasNovak/status/783433625723252736    False   \n",
       "3    /kdnuggets/status/783428740453982208    False   \n",
       "4    /kdnuggets/status/787052623291641856    False   \n",
       "\n",
       "                                                text   author  \n",
       "0  #DataScience Basics: #DataMining vs. #Statisti...  various  \n",
       "1  How to Become a #Data Scientist – Part 1: http...  various  \n",
       "2   @jesterxl @kdnuggets or just go with @tableau :)  various  \n",
       "3  #Boston U. Online MS in Applied #Business #Ana...  various  \n",
       "4  #ICYMI Still Searching for ROI in #BigData Ana...  various  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob('twitter_corpus/*.csv')\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "corpus = pd.concat(li, axis=0, ignore_index=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generación de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a convertir todo el texto de los tweets en un único documento plano de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = \"\\n\".join(re.sub('[^a-zA-Z0-9!?\\',.:\" ]+', '', tweet) for tweet in corpus.sample(25000).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero un diccionario de caracteres y dos conversores que transformen respectivamente cada caracter en su número correspondiente y a la inversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(full_corpus)))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero los chunks de texto que se van a utilizar para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples: 2810923\n"
     ]
    }
   ],
   "source": [
    "# El modelo se fijará en tramos de 40 caracteres para predecir el que sigue\n",
    "seq_length = 40\n",
    "\n",
    "# Los chunks se generan caracter a caracter para tener un mayor número de ejemplos y no perder la linealidad de las palabras.\n",
    "step = 1\n",
    "\n",
    "# Lista para guardar los chunks\n",
    "sentences = []\n",
    "\n",
    "# Lista para guardar los caracteres que el modelo debe tomar como predicciones correctas para cada chunk.\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(full_corpus)-seq_length, step):\n",
    "    sentences.append(full_corpus[i:i+seq_length])\n",
    "    next_chars.append(full_corpus[i+seq_length])\n",
    "    \n",
    "print('num training examples: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero un tensor de chunks con cada carácter representado en un vector one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot-encoded vectors\n",
    "X = np.zeros((len(sentences), seq_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo el modelo y lo entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2670376 samples, validate on 140547 samples\n",
      "Epoch 1/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 2.4838 - accuracy: 0.3385 - val_loss: 2.0790 - val_accuracy: 0.4295\n",
      "Epoch 2/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.8554 - accuracy: 0.4902 - val_loss: 1.8624 - val_accuracy: 0.4872\n",
      "Epoch 3/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.7311 - accuracy: 0.5225 - val_loss: 1.8156 - val_accuracy: 0.4947\n",
      "Epoch 4/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.6744 - accuracy: 0.5372 - val_loss: 1.7459 - val_accuracy: 0.5166\n",
      "Epoch 5/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.6405 - accuracy: 0.5458 - val_loss: 1.7344 - val_accuracy: 0.5142\n",
      "Epoch 6/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.6172 - accuracy: 0.5517 - val_loss: 1.7022 - val_accuracy: 0.5286\n",
      "Epoch 7/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.6001 - accuracy: 0.5560 - val_loss: 1.6873 - val_accuracy: 0.5321\n",
      "Epoch 8/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.5872 - accuracy: 0.5592 - val_loss: 1.6875 - val_accuracy: 0.5326\n",
      "Epoch 9/100\n",
      "2670376/2670376 [==============================] - 149s 56us/step - loss: 1.5766 - accuracy: 0.5618 - val_loss: 1.6682 - val_accuracy: 0.5378\n",
      "Epoch 10/100\n",
      "2670376/2670376 [==============================] - 150s 56us/step - loss: 1.5683 - accuracy: 0.5639 - val_loss: 1.6562 - val_accuracy: 0.5406\n",
      "Epoch 11/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5612 - accuracy: 0.5658 - val_loss: 1.6712 - val_accuracy: 0.5373\n",
      "Epoch 12/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5553 - accuracy: 0.5672 - val_loss: 1.6618 - val_accuracy: 0.5407\n",
      "Epoch 13/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5498 - accuracy: 0.5686 - val_loss: 1.6537 - val_accuracy: 0.5409\n",
      "Epoch 14/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5454 - accuracy: 0.5697 - val_loss: 1.6526 - val_accuracy: 0.5437\n",
      "Epoch 15/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5414 - accuracy: 0.5705 - val_loss: 1.6604 - val_accuracy: 0.5403\n",
      "Epoch 16/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5382 - accuracy: 0.5714 - val_loss: 1.6413 - val_accuracy: 0.5437\n",
      "Epoch 17/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.5346 - accuracy: 0.5722 - val_loss: 1.6320 - val_accuracy: 0.5496\n",
      "Epoch 18/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5318 - accuracy: 0.5731 - val_loss: 1.6444 - val_accuracy: 0.5471\n",
      "Epoch 19/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5295 - accuracy: 0.5734 - val_loss: 1.6342 - val_accuracy: 0.5484\n",
      "Epoch 20/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5270 - accuracy: 0.5742 - val_loss: 1.6356 - val_accuracy: 0.5443\n",
      "Epoch 21/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5245 - accuracy: 0.5747 - val_loss: 1.6395 - val_accuracy: 0.5475\n",
      "Epoch 22/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5226 - accuracy: 0.5751 - val_loss: 1.6284 - val_accuracy: 0.5508\n",
      "Epoch 23/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5204 - accuracy: 0.5759 - val_loss: 1.6277 - val_accuracy: 0.5484\n",
      "Epoch 24/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5189 - accuracy: 0.5760 - val_loss: 1.6239 - val_accuracy: 0.5508\n",
      "Epoch 25/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5172 - accuracy: 0.5762 - val_loss: 1.6241 - val_accuracy: 0.5493\n",
      "Epoch 26/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5157 - accuracy: 0.5767 - val_loss: 1.6205 - val_accuracy: 0.5519\n",
      "Epoch 27/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5148 - accuracy: 0.5769 - val_loss: 1.6278 - val_accuracy: 0.5495\n",
      "Epoch 28/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5129 - accuracy: 0.5774 - val_loss: 1.6149 - val_accuracy: 0.5525\n",
      "Epoch 29/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5116 - accuracy: 0.5775 - val_loss: 1.6264 - val_accuracy: 0.5505\n",
      "Epoch 30/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5103 - accuracy: 0.5781 - val_loss: 1.6125 - val_accuracy: 0.5517\n",
      "Epoch 31/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5094 - accuracy: 0.5782 - val_loss: 1.6187 - val_accuracy: 0.5520\n",
      "Epoch 32/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5084 - accuracy: 0.5785 - val_loss: 1.6171 - val_accuracy: 0.5505\n",
      "Epoch 33/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.5075 - accuracy: 0.5788 - val_loss: 1.6170 - val_accuracy: 0.5541\n",
      "Epoch 34/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5071 - accuracy: 0.5787 - val_loss: 1.6099 - val_accuracy: 0.5522\n",
      "Epoch 35/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5058 - accuracy: 0.5792 - val_loss: 1.6149 - val_accuracy: 0.5525\n",
      "Epoch 36/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5050 - accuracy: 0.5794 - val_loss: 1.6111 - val_accuracy: 0.5541\n",
      "Epoch 37/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5041 - accuracy: 0.5797 - val_loss: 1.6112 - val_accuracy: 0.5533\n",
      "Epoch 38/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5030 - accuracy: 0.5799 - val_loss: 1.6141 - val_accuracy: 0.5522\n",
      "Epoch 39/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5020 - accuracy: 0.5799 - val_loss: 1.6159 - val_accuracy: 0.5546\n",
      "Epoch 40/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5012 - accuracy: 0.5802 - val_loss: 1.6226 - val_accuracy: 0.5522\n",
      "Epoch 41/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5005 - accuracy: 0.5804 - val_loss: 1.6111 - val_accuracy: 0.5539\n",
      "Epoch 42/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.5001 - accuracy: 0.5806 - val_loss: 1.6070 - val_accuracy: 0.5538\n",
      "Epoch 43/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4991 - accuracy: 0.5809 - val_loss: 1.6121 - val_accuracy: 0.5541\n",
      "Epoch 44/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4986 - accuracy: 0.5809 - val_loss: 1.6084 - val_accuracy: 0.5573\n",
      "Epoch 45/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4982 - accuracy: 0.5810 - val_loss: 1.6168 - val_accuracy: 0.5491\n",
      "Epoch 46/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4975 - accuracy: 0.5813 - val_loss: 1.6065 - val_accuracy: 0.5552\n",
      "Epoch 47/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4966 - accuracy: 0.5812 - val_loss: 1.6029 - val_accuracy: 0.5567\n",
      "Epoch 48/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4961 - accuracy: 0.5815 - val_loss: 1.6080 - val_accuracy: 0.5538\n",
      "Epoch 49/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4953 - accuracy: 0.5819 - val_loss: 1.6107 - val_accuracy: 0.5510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4950 - accuracy: 0.5817 - val_loss: 1.6128 - val_accuracy: 0.5553\n",
      "Epoch 51/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4947 - accuracy: 0.5817 - val_loss: 1.5981 - val_accuracy: 0.5591\n",
      "Epoch 52/100\n",
      "2670376/2670376 [==============================] - 153s 57us/step - loss: 1.4938 - accuracy: 0.5820 - val_loss: 1.6077 - val_accuracy: 0.5549\n",
      "Epoch 53/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4937 - accuracy: 0.5822 - val_loss: 1.5983 - val_accuracy: 0.5567\n",
      "Epoch 54/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4934 - accuracy: 0.5823 - val_loss: 1.6089 - val_accuracy: 0.5527\n",
      "Epoch 55/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4934 - accuracy: 0.5821 - val_loss: 1.6130 - val_accuracy: 0.5524\n",
      "Epoch 56/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4926 - accuracy: 0.5824 - val_loss: 1.6021 - val_accuracy: 0.5589\n",
      "Epoch 57/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4917 - accuracy: 0.5825 - val_loss: 1.6060 - val_accuracy: 0.5536\n",
      "Epoch 58/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4911 - accuracy: 0.5829 - val_loss: 1.6064 - val_accuracy: 0.5531\n",
      "Epoch 59/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4910 - accuracy: 0.5827 - val_loss: 1.6137 - val_accuracy: 0.5535\n",
      "Epoch 60/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4912 - accuracy: 0.5828 - val_loss: 1.6072 - val_accuracy: 0.5533\n",
      "Epoch 61/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4904 - accuracy: 0.5829 - val_loss: 1.6073 - val_accuracy: 0.5508\n",
      "Epoch 62/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4900 - accuracy: 0.5830 - val_loss: 1.5930 - val_accuracy: 0.5605\n",
      "Epoch 63/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4897 - accuracy: 0.5831 - val_loss: 1.5973 - val_accuracy: 0.5582\n",
      "Epoch 64/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4898 - accuracy: 0.5831 - val_loss: 1.6050 - val_accuracy: 0.5558\n",
      "Epoch 65/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4888 - accuracy: 0.5835 - val_loss: 1.5982 - val_accuracy: 0.5590\n",
      "Epoch 66/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4880 - accuracy: 0.5835 - val_loss: 1.5962 - val_accuracy: 0.5589\n",
      "Epoch 67/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4883 - accuracy: 0.5835 - val_loss: 1.5979 - val_accuracy: 0.5570\n",
      "Epoch 68/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4877 - accuracy: 0.5838 - val_loss: 1.6043 - val_accuracy: 0.5568\n",
      "Epoch 69/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4871 - accuracy: 0.5837 - val_loss: 1.6014 - val_accuracy: 0.5551\n",
      "Epoch 70/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4866 - accuracy: 0.5836 - val_loss: 1.5973 - val_accuracy: 0.5567\n",
      "Epoch 71/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4863 - accuracy: 0.5839 - val_loss: 1.6001 - val_accuracy: 0.5586\n",
      "Epoch 72/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4864 - accuracy: 0.5838 - val_loss: 1.5989 - val_accuracy: 0.5584\n",
      "Epoch 73/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4862 - accuracy: 0.5840 - val_loss: 1.5900 - val_accuracy: 0.5607\n",
      "Epoch 74/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4857 - accuracy: 0.5840 - val_loss: 1.5993 - val_accuracy: 0.5557\n",
      "Epoch 75/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4850 - accuracy: 0.5843 - val_loss: 1.5996 - val_accuracy: 0.5602\n",
      "Epoch 76/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4850 - accuracy: 0.5842 - val_loss: 1.5984 - val_accuracy: 0.5592\n",
      "Epoch 77/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4854 - accuracy: 0.5840 - val_loss: 1.5894 - val_accuracy: 0.5608\n",
      "Epoch 78/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4845 - accuracy: 0.5845 - val_loss: 1.5980 - val_accuracy: 0.5562\n",
      "Epoch 79/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4843 - accuracy: 0.5845 - val_loss: 1.6022 - val_accuracy: 0.5561\n",
      "Epoch 80/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4836 - accuracy: 0.5845 - val_loss: 1.5958 - val_accuracy: 0.5562\n",
      "Epoch 81/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4843 - accuracy: 0.5842 - val_loss: 1.5850 - val_accuracy: 0.5611\n",
      "Epoch 82/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4841 - accuracy: 0.5844 - val_loss: 1.5874 - val_accuracy: 0.5601\n",
      "Epoch 83/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4836 - accuracy: 0.5845 - val_loss: 1.5896 - val_accuracy: 0.5590\n",
      "Epoch 84/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4835 - accuracy: 0.5846 - val_loss: 1.6024 - val_accuracy: 0.5577\n",
      "Epoch 85/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.4840 - accuracy: 0.5845 - val_loss: 1.5952 - val_accuracy: 0.5576\n",
      "Epoch 86/100\n",
      "2670376/2670376 [==============================] - 151s 57us/step - loss: 1.4827 - accuracy: 0.5846 - val_loss: 1.5923 - val_accuracy: 0.5602\n",
      "Epoch 87/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4828 - accuracy: 0.5847 - val_loss: 1.6001 - val_accuracy: 0.5568\n",
      "Epoch 88/100\n",
      "2670376/2670376 [==============================] - 154s 58us/step - loss: 1.4823 - accuracy: 0.5847 - val_loss: 1.5829 - val_accuracy: 0.5620\n",
      "Epoch 89/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4821 - accuracy: 0.5849 - val_loss: 1.5914 - val_accuracy: 0.5604\n",
      "Epoch 90/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4818 - accuracy: 0.5849 - val_loss: 1.5969 - val_accuracy: 0.5585\n",
      "Epoch 91/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4821 - accuracy: 0.5849 - val_loss: 1.5991 - val_accuracy: 0.5589\n",
      "Epoch 92/100\n",
      "2670376/2670376 [==============================] - 157s 59us/step - loss: 1.4825 - accuracy: 0.5849 - val_loss: 1.5987 - val_accuracy: 0.5563\n",
      "Epoch 93/100\n",
      "2670376/2670376 [==============================] - 156s 59us/step - loss: 1.4821 - accuracy: 0.5850 - val_loss: 1.5868 - val_accuracy: 0.5593\n",
      "Epoch 94/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4819 - accuracy: 0.5850 - val_loss: 1.5908 - val_accuracy: 0.5617\n",
      "Epoch 95/100\n",
      "2670376/2670376 [==============================] - 152s 57us/step - loss: 1.4829 - accuracy: 0.5849 - val_loss: 1.5797 - val_accuracy: 0.5636\n",
      "Epoch 96/100\n",
      "2670376/2670376 [==============================] - 153s 57us/step - loss: 1.4816 - accuracy: 0.5850 - val_loss: 1.5837 - val_accuracy: 0.5620\n",
      "Epoch 97/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4803 - accuracy: 0.5852 - val_loss: 1.5963 - val_accuracy: 0.5576\n",
      "Epoch 98/100\n",
      "2670376/2670376 [==============================] - 155s 58us/step - loss: 1.4798 - accuracy: 0.5855 - val_loss: 1.6044 - val_accuracy: 0.5567\n",
      "Epoch 99/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4805 - accuracy: 0.5852 - val_loss: 1.5883 - val_accuracy: 0.5615\n",
      "Epoch 100/100\n",
      "2670376/2670376 [==============================] - 156s 58us/step - loss: 1.4801 - accuracy: 0.5855 - val_loss: 1.5923 - val_accuracy: 0.5605\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=10000, epochs=100, shuffle=True).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testear el modelo, lo mejor es ponerlo a prueba. Ya que no nos interesa tanto el accuracy como el hecho de que sea capaz de generar un texto legible y coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"I've seen a red unicorn fighting with a giant purple dinosaur. I was so scared that I suddenly woke up.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que vectoriza el input que se le va a dar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    x = np.zeros((1, len(text), len(chars)))\n",
    "\n",
    "    for t, char in enumerate(text):\n",
    "        x[0, t, char_to_index[char]] = 1.\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que predice 200 veces cada carácter en base al último chunk del texto (introducido + generado), generando un texto final que sea el input + 200 nuevos caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_completion(text):\n",
    "    text = re.sub('[^a-zA-Z0-9!?\\',.:\" ]+', '', text)\n",
    "    for _ in range(200):\n",
    "        x = prepare_input(text[-seq_length:])\n",
    "        pred = model.predict(x, verbose=0)[0]\n",
    "        next_index = max(range(len(pred)), key=pred.__getitem__)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        text += next_char\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pongo a prueba el modelo con mi frase de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've seen a red unicorn fighting with a giant purple dinosaur. I was so scared that I suddenly woke up. http:twitpic.comerik  http:twitpic.com1b5nw  The proper proper story is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the w\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the people wants to know if they can still still step the most complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the w\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('But the people wants to know if they can still'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thats how we will overcome the challenges we face: http:buff.ly1Setfa  http:twitpic.com1b5nw  The proper proper story is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the wor\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion('That’s how we will overcome the challenges we face:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG no way! Now I need to go there! I've never been and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still and the massive still a\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"OMG no way! Now I need to go there! I've never been\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's very interesting! I wonder if I could do the same as you did. Thank you!\n",
      "The president we can allow a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is a complete of the world is\n"
     ]
    }
   ],
   "source": [
    "print(predict_completion(\"It's very interesting! I wonder if I could do the same as you did.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
